{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u49lPLHdbtcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include <memory>\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"include/model.h\"\n",
        "#include \"common/common_test.h\"\n",
        "#include \"include/lite_session.h\"\n",
        "#include \"include/context.h\"\n",
        "#include \"include/errorcode.h\"\n",
        "#include \"utils/log_adapter.h\"\n",
        "#include \"tools/converter/model_parser.h\"\n",
        "#include \"tools/converter/anf_transform.h\"\n",
        "#include \"tools/optimizer/fusion/constant_folding_pass.h\"\n",
        "#include \"src/common/anf_exporter/anf_exporter.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "class ConstantFoldingFusionTest : public mindspore::Common {\n",
        " public:\n",
        "  ConstantFoldingFusionTest() = default;\n",
        "};\n",
        "using MetaGraphTptr = std::shared_ptr<schema::MetaGraphT>;\n",
        "using CNodeTptr = std::unique_ptr<schema::CNodeT>;\n",
        "\n",
        "namespace {\n",
        "\n",
        "MetaGraphTptr BuildGraph(schema::PrimitiveType op_type, void *op_node) {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // biasadd node\n",
        "  auto example_node = std::make_unique<schema::CNodeT>();\n",
        "  example_node->inputIndex = {0, 1};\n",
        "  example_node->outputIndex = {2};\n",
        "  example_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  example_node->primitive->value.type = op_type;\n",
        "  example_node->primitive->value.value = op_node;\n",
        "  example_node->name = \"example\";\n",
        "  meta_graph->nodes.emplace_back(std::move(example_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0, 1};\n",
        "  meta_graph->outputIndex = {2};\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // input 1: data2\n",
        "  auto input1 = std::make_unique<schema::TensorT>();\n",
        "  input1->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input1->format = schema::Format_NHWC;\n",
        "  input1->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input1->dims = {1, 2, 2, 3};\n",
        "  input1->offset = -1;\n",
        "  input1->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto input1_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input1_data[i] = i;\n",
        "  }\n",
        "  memcpy(input1->data.data(), input1_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input1_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input1));\n",
        "\n",
        "  // final add output\n",
        "  auto add_output = std::make_unique<schema::TensorT>();\n",
        "  add_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  add_output->format = schema::Format_NHWC;\n",
        "  add_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  add_output->dims = {1, 2, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(add_output));\n",
        "  // final output\n",
        "  return meta_graph;\n",
        "}\n",
        "\n",
        "MetaGraphTptr BuildGraphForOneInput(schema::PrimitiveType op_type, void *op_node) {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // biasadd node\n",
        "  auto example_node = std::make_unique<schema::CNodeT>();\n",
        "  example_node->inputIndex = {0};\n",
        "  example_node->outputIndex = {1};\n",
        "  example_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  example_node->primitive->value.type = op_type;\n",
        "  example_node->primitive->value.value = op_node;\n",
        "  example_node->name = \"example\";\n",
        "  meta_graph->nodes.emplace_back(std::move(example_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0};\n",
        "  meta_graph->outputIndex = {1};\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i + 1;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // final add output\n",
        "  auto add_output = std::make_unique<schema::TensorT>();\n",
        "  add_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  add_output->format = schema::Format_NHWC;\n",
        "  add_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  add_output->dims = {1, 2, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(add_output));\n",
        "\n",
        "  // final output\n",
        "  return meta_graph;\n",
        "}\n",
        "\n",
        "MetaGraphTptr BuildMixGraph() {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // add node\n",
        "  auto add_node = std::make_unique<schema::CNodeT>();\n",
        "  add_node->inputIndex = {0, 1};\n",
        "  add_node->outputIndex = {2};\n",
        "  add_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  add_node->primitive->value.type = schema::PrimitiveType_Add;\n",
        "  add_node->primitive->value.value = new schema::AddT;\n",
        "  add_node->name = \"add\";\n",
        "  meta_graph->nodes.emplace_back(std::move(add_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0, 1, 2};\n",
        "  meta_graph->outputIndex = {4};\n",
        "\n",
        "  auto mul_node = std::make_unique<schema::CNodeT>();\n",
        "  mul_node->inputIndex = {2, 3};\n",
        "  mul_node->outputIndex = {4};\n",
        "  mul_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  mul_node->primitive->value.type = schema::PrimitiveType_Mul;\n",
        "  mul_node->primitive->value.value = new schema::MulT;\n",
        "  mul_node->name = \"mul\";\n",
        "  meta_graph->nodes.emplace_back(std::move(mul_node));\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // input 1: data2\n",
        "  auto input1 = std::make_unique<schema::TensorT>();\n",
        "  input1->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input1->format = schema::Format_NHWC;\n",
        "  input1->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input1->dims = {1, 2, 2, 3};\n",
        "  input1->offset = -1;\n",
        "  input1->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto input1_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input1_data[i] = i;\n",
        "  }\n",
        "  memcpy(input1->data.data(), input1_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input1_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input1));\n",
        "\n",
        "  // addoutput\n",
        "  auto add_output = std::make_unique<schema::TensorT>();\n",
        "  add_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  add_output->format = schema::Format_NHWC;\n",
        "  add_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  add_output->dims = {1, 2, 2, 3};\n",
        "  add_output->offset = -1;\n",
        "  add_output->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto add_output_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  memcpy(add_output->data.data(), add_output_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] add_output_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(add_output));\n",
        "\n",
        "  // input 2: data3\n",
        "  auto input2 = std::make_unique<schema::TensorT>();\n",
        "  input2->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input2->format = schema::Format_NHWC;\n",
        "  input2->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input2->dims = {1, 2, 2, 3};\n",
        "  input2->offset = -1;\n",
        "  input2->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto input2_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input2_data[i] = 10;\n",
        "  }\n",
        "  memcpy(input2->data.data(), input2_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input2_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input2));\n",
        "\n",
        "  // final mul output\n",
        "  auto mul_output = std::make_unique<schema::TensorT>();\n",
        "  mul_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  mul_output->format = schema::Format_NHWC;\n",
        "  mul_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  mul_output->dims = {1, 2, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(mul_output));\n",
        "  // final output\n",
        "  return meta_graph;\n",
        "}\n",
        "}\n",
        "//  namespace\n",
        "TEST_F(ConstantFoldingFusionTest, TestADDConstantFold) {\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Add, new schema::AddT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestMixedConstantFold) {\n",
        "  auto meta_graph = BuildMixGraph();\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestSubConstantFold) {\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Sub, new schema::SubT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestMulConstantFold) {\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Mul, new schema::MulT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestTransposeConstantFold) {\n",
        "  auto transposeT = new schema::TransposeT;\n",
        "  transposeT->perm = {3, 0, 1, 2};\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Transpose, transposeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestTileConstantFold) {\n",
        "  auto tileT = new schema::TileT;\n",
        "  tileT->multiples = {1, 2, 2, 2};\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Tile, tileT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestStridedSliceConstantFold) {\n",
        "  auto stridedSliceT = new schema::StridedSliceT;\n",
        "  stridedSliceT->begin = {1};\n",
        "  stridedSliceT->end = {3};\n",
        "  stridedSliceT->stride = {1};\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_StridedSlice, stridedSliceT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestStackConstantFold) {\n",
        "  auto stackT = new schema::StackT;\n",
        "  stackT->axis = 1;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Stack, stackT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestSliceConstantFold) {\n",
        "  auto sliceT = new schema::SliceT;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Slice, sliceT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestShapeConstantFold) {\n",
        "  auto shapeT = new schema::ShapeT;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Shape, shapeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestRsqrtConstantFold) {\n",
        "  auto rsqrtT = new schema::RsqrtT;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Rsqrt, rsqrtT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestReshapeConstantFold) {\n",
        "  auto reshapeT = new schema::ReshapeT;\n",
        "  reshapeT->shape = {2, 6};\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Reshape, reshapeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestRangeConstantFold) {\n",
        "  auto rangeT = new schema::RangeT;\n",
        "  rangeT->limit = 10;\n",
        "  rangeT->start = 1;\n",
        "  rangeT->delta = 1;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Range, rangeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "TEST_F(ConstantFoldingFusionTest, TestMatmulConstantFold) {\n",
        "  auto matmulT = new schema::MatMulT;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_MatMul, matmulT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestExpandDimsConstantFold) {\n",
        "  auto expandDimsT = new schema::ExpandDimsT;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_ExpandDims, expandDimsT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestConcatDimsConstantFold) {\n",
        "  auto concatT = new schema::ConcatT;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Concat, concatT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestCastDimsConstantFold) {\n",
        "  auto castT = new schema::CastT;\n",
        "  castT->srcT = kNumberTypeUInt8;\n",
        "  castT->dstT = kNumberTypeFloat32;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Cast, castT);\n",
        "  auto input_tensor = meta_graph->allTensors.at(0).get();\n",
        "  input_tensor->dataType = kNumberTypeUInt8;\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTuk4JZFb-bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/fusion/constant_folding_pass.h\"\n",
        "#include <memory>\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include \"src/kernel_factory.h\"\n",
        "#include \"src/common/anf_exporter/anf_exporter.h\"\n",
        "#include \"src/scheduler.h\"\n",
        "#include \"include/context.h\"\n",
        "#include \"src/lite_session.h\"\n",
        "#include \"src/ir/primitive_t_value.h\"\n",
        "#include \"src/populate_parameter.h\"\n",
        "\n",
        "using mindspore::lite::KernelFactory;\n",
        "using mindspore::lite::tensor::Tensor;\n",
        "using mindspore::lite::PrimitiveTValue;\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "const std::set<schema::PrimitiveType> constant__fold_table{\n",
        "    schema::PrimitiveType_Add, schema::PrimitiveType_Sub, schema::PrimitiveType_Mul, schema::PrimitiveType_MatMul,\n",
        "    schema::PrimitiveType_Tile, schema::PrimitiveType_StridedSlice, schema::PrimitiveType_Slice,\n",
        "    schema::PrimitiveType_Cast, schema::PrimitiveType_Concat\n",
        "};\n",
        "const std::vector<Tensor *> GetCNodeInputTensors(const CNodePtr &CNode) {\n",
        "  MS_ASSERT(CNode != nullptr);\n",
        "  auto tmp_meta_graph = std::make_unique<schema::MetaGraphT>();\n",
        "  auto tmp_fb_node = std::make_unique<schema::CNodeT>();\n",
        "  lite::AnfExporter anfExporter;\n",
        "  anfExporter.SetOpInputNode(CNode, tmp_meta_graph.get(), tmp_fb_node.get());\n",
        "  std::vector<Tensor *> input_tensors;\n",
        "  for (auto input_index:tmp_fb_node->inputIndex) {\n",
        "    auto tensorT = tmp_meta_graph->allTensors.at(input_index).get();\n",
        "    auto tensor_shape = tensorT->dims;\n",
        "    auto lite_tensor =\n",
        "        new(std::nothrow)Tensor(TypeId(tensorT->dataType), tensor_shape, tensorT->format, tensorT->nodeType);\n",
        "    auto lite_tensor_size = tensorT->data.size() * sizeof(uint8_t);\n",
        "    auto tensor_data = new(std::nothrow)char[lite_tensor_size / sizeof(char)];\n",
        "    auto ret = memcpy_s(tensor_data, lite_tensor_size, tensorT->data.data(), lite_tensor_size);\n",
        "    if (ret != EOK) {\n",
        "      MS_LOG(EXCEPTION) << \"memcpy error: \" << ret;\n",
        "    }\n",
        "    lite_tensor->SetData(tensor_data);\n",
        "    input_tensors.emplace_back(lite_tensor);\n",
        "  }\n",
        "  return input_tensors;\n",
        "}\n",
        "schema::Primitive *PackPrimitiveT(const CNodePtr &cnode) {\n",
        "  auto primitiveT_value =\n",
        "      GetValueNode<std::shared_ptr<PrimitiveTValue>>(cnode->input(0));\n",
        "  if (primitiveT_value == nullptr) {\n",
        "    MS_LOG(ERROR) << \"PrimitiveT_value is nullptr\";\n",
        "    return nullptr;\n",
        "  }\n",
        "\n",
        "  auto *lite_primitive = primitiveT_value->GetPrimitiveT();\n",
        "  if (lite_primitive == nullptr) {\n",
        "    MS_LOG(ERROR) << \"Primitive in primitiveT_value is nullptr\";\n",
        "    return nullptr;\n",
        "  }\n",
        "\n",
        "  flatbuffers::FlatBufferBuilder builder(1024);\n",
        "  auto offset = schema::Primitive::Pack(builder, lite_primitive);\n",
        "  builder.Finish(offset);\n",
        "  auto buf = builder.GetBufferPointer();\n",
        "  auto primitive = flatbuffers::GetRoot<schema::Primitive>(buf);\n",
        "  return const_cast<schema::Primitive *>(primitive);\n",
        "}\n",
        "const ParameterPtr CreateNewParamter(const FuncGraphPtr &func_graph, Tensor *tensor) {\n",
        "  auto parameter = func_graph->add_parameter();\n",
        "  std::vector<int> shape;\n",
        "  for (auto &dim : tensor->shape()) {\n",
        "    shape.push_back(dim);\n",
        "  }\n",
        "  auto type_id = static_cast<TypeId>(tensor->data_type());\n",
        "  auto type_ptr = TypeIdToType(type_id);\n",
        "  auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(type_ptr, shape);\n",
        "  parameter->set_abstract(abstract_tensor);\n",
        "\n",
        "  ParamValueLitePtr param_value = std::make_shared<ParamValueLite>();\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  param_value->set_tensor_shape(shape);\n",
        "  param_value->set_tensor_type(type_id);\n",
        "  if (tensor->Data() != nullptr) {\n",
        "    auto size = tensor->ElementsNum();\n",
        "    auto tensor_data = new(std::nothrow)float[size];\n",
        "    auto ret = memcpy_s(tensor_data, size * sizeof(float), tensor->Data(), size * sizeof(float));\n",
        "    if (ret != EOK) {\n",
        "      MS_LOG(EXCEPTION) << \"memcpy error: \" << ret;\n",
        "    }\n",
        "    MS_ASSERT(tensor_data != nullptr);\n",
        "    param_value->set_tensor_addr(tensor_data);\n",
        "    param_value->set_tensor_size(size * sizeof(float) / sizeof(uint8_t));\n",
        "  }\n",
        "  parameter->set_default_param(param_value);\n",
        "  return parameter;\n",
        "}\n",
        "kernel::LiteKernel *GetLiteKernel(std::vector<Tensor *> inputs, std::vector<Tensor *> &outputs,\n",
        "                                  lite::Primitive *primitive) {\n",
        "  MS_ASSERT(nullptr != lite_primitive);\n",
        "  auto data_type = inputs.front()->data_type();\n",
        "  kernel::KernelKey desc{kernel::KERNEL_ARCH::kCPU, data_type, primitive->Type()};\n",
        "  lite::Context context;\n",
        "  auto parameter = kernel::PopulateParameter(primitive);\n",
        "  if (parameter == nullptr) {\n",
        "    MS_LOG(ERROR)\n",
        "            << \"PopulateParameter return nullptr, type: \" << schema::EnumNamePrimitiveType(primitive->Type());\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto creator = lite::KernelRegistry::GetInstance()->GetCreator(desc);\n",
        "  if (creator != nullptr) {\n",
        "    auto lite_kernel = creator(inputs, outputs, parameter, &context, desc, primitive);\n",
        "    return lite_kernel;\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "}  //  namespace\n",
        "\n",
        "const AnfNodePtr ConstFoldPass::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                        const EquivPtr &) const {\n",
        "  CheckIfFuncGraphIsNull(func_graph);\n",
        "  CheckIfAnfNodeIsNull(node);\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return node;\n",
        "  }\n",
        "  auto any_node = node->cast<CNodePtr>();\n",
        "  CheckIfCNodeIsNull(any_node);\n",
        "  std::vector<ParameterPtr> outputs;\n",
        "  for (size_t i = 1; i < any_node->inputs().size(); i++) {\n",
        "    auto input_node = any_node->input(i);\n",
        "    if (input_node->isa<CNode>() && CheckIsAllInputsParam(input_node)\n",
        "        && constant__fold_table.find(GetCNodeType(input_node)) != constant__fold_table.end()) {\n",
        "      MS_LOG(INFO) << \"Begin fold node:\" << input_node->fullname_with_scope();\n",
        "      auto input_cnode = input_node->cast<CNodePtr>();\n",
        "      auto input_tensors = GetCNodeInputTensors(input_cnode);\n",
        "      if (input_tensors.empty()) {\n",
        "        MS_LOG(EXCEPTION) << \"cnode inputs tensor empty\";\n",
        "      }\n",
        "      auto output_nums = GetOutputTensorNum(input_cnode);\n",
        "      std::vector<Tensor *> output_tensors{output_nums, new Tensor()};\n",
        "      auto scheam_primitive = PackPrimitiveT(input_cnode);\n",
        "      auto lite_primitive = lite::Primitive::CreatePrimitive(scheam_primitive);\n",
        "      lite_primitive->InferShape(input_tensors, output_tensors);\n",
        "      auto lite_kernel = GetLiteKernel(input_tensors, output_tensors, lite_primitive);\n",
        "      if (lite_kernel == nullptr) {\n",
        "        MS_LOG(ERROR) << \"constant_folding schedule node lite kernel nullptr\";\n",
        "        return any_node;\n",
        "      }\n",
        "      auto ret = lite_kernel->Run();\n",
        "      if (0 != ret) {\n",
        "        MS_LOG(EXCEPTION) << \"run kernel failed, name: \" << lite_kernel->Name();\n",
        "      }\n",
        "      auto new_parameter = CreateNewParamter(func_graph, output_tensors.front());\n",
        "      any_node->set_input(i, new_parameter);\n",
        "    }\n",
        "  }\n",
        "  return any_node;\n",
        "}\n",
        "}  // namespace mindspore::opt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnEXaDbncFSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_LITE_SRC_PASS_FUSION_ADD_CONSTANT_FOLDING_PASS_H_\n",
        "#define MINDSPORE_LITE_SRC_PASS_FUSION_ADD_CONSTANT_FOLDING_PASS_H_\n",
        "\n",
        "#include \"tools/optimizer/common/optimizer.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace opt {\n",
        "class ConstFoldPass : public PatternProcessPass {\n",
        " public:\n",
        "  explicit ConstFoldPass(bool multigraph = true) : PatternProcessPass(\"constfold_pass\", multigraph) {}\n",
        "  ~ConstFoldPass() override = default;\n",
        "  const AnfNodePtr Process(const FuncGraphPtr &, const AnfNodePtr &, const EquivPtr &) const override;\n",
        "};\n",
        "}  // namespace opt\n",
        "}  // namespace mindspore\n",
        "#endif  // MINDSPORE_LITE_SRC_PASS_FUSION_ADD_CONSTANT_FOLDING_PASS_H_\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    }
  ]
}