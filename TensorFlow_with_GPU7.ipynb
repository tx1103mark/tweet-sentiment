{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TensorFlow_with_GPU7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# pytorch with TPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htU6cfJsJd-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dacc0407-b5ef-41c1-c370-6cdd40774019"
      },
      "source": [
        "!export XLA_USE_BF16=1\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5svsnB73nMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_PREDICT_BATCHNORM_CONVERT_SCALE_PASS_H\n",
        "#define MINDSPORE_PREDICT_BATCHNORM_CONVERT_SCALE_PASS_H\n",
        "\n",
        "#include <unordered_map>\n",
        "#include <memory>\n",
        "#include <string>\n",
        "#include \"tools/converter/legacy_optimizer/fusion/fusion_pass.h\"\n",
        "#include \"tools/common/graph_util.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "struct BNWeightTensors {\n",
        "  TensorT *meanTensor = nullptr;\n",
        "  TensorT *varianceTensor = nullptr;\n",
        "  TensorT *scaleTensor = nullptr;\n",
        "  TensorT *biasTensor = nullptr;\n",
        "};\n",
        "class BatchNormConvertScalePass : public FusionPass {\n",
        " public:\n",
        "  BatchNormConvertScalePass() = default;\n",
        "\n",
        "  ~BatchNormConvertScalePass() override;\n",
        "\n",
        "  STATUS DefinePattern() override;\n",
        "\n",
        "  STATUS DoFusion(MetaGraphT *graph, const std::string &patternName,\n",
        "                  std::unordered_map<std::string, std::shared_ptr<Path>> &matchedPath) override;\n",
        "\n",
        "  STATUS Run(MetaGraphT *graph) override;\n",
        "\n",
        " protected:\n",
        "  STATUS GetTransParam(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath);\n",
        "\n",
        "  // Get and check BNNode weight tensor\n",
        "  STATUS GetBnWeightTensors(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath, BNWeightTensors &bnWeightTensors);\n",
        "\n",
        "  STATUS GetBnEpsilon(MetaGraphT *graph, float &eps);\n",
        "\n",
        "  STATUS FindNodes(MetaGraphT *graph, const std::unordered_map<std::string, std::shared_ptr<Path>> &matchedPath);\n",
        "\n",
        "  STATUS GenNewScaleTensor(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath);\n",
        "\n",
        "  STATUS ConvertBNToScale(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath);\n",
        "\n",
        "  CNodeT *inputNode = nullptr;\n",
        "  CNodeT *bnNode = nullptr;\n",
        "\n",
        "  std::string inputOpName = \"Input\";\n",
        "  std::string bnOpName = \"BatchNorm\";\n",
        "  std::string bnPatternName = \"BnToScaleFusion\";\n",
        "  int bnChannel = 0;\n",
        "  TensorT *bnMeanTensor = nullptr;\n",
        "  float *transScale = nullptr;\n",
        "  float *transBias = nullptr;\n",
        "  std::unique_ptr<TensorT> newScaleWeightTensor = nullptr;\n",
        "  std::unique_ptr<TensorT> newScaleBiasTensor = nullptr;\n",
        "\n",
        "  OpDefCopyer ScaleOpCopyer = [](CNodeT *inOpDef) -> std::unique_ptr<CNodeT> {\n",
        "    std::unique_ptr<CNodeT> newOpDef(new(std::nothrow) CNodeT);\n",
        "    if (newOpDef == nullptr) {\n",
        "      MS_LOG(ERROR) << \"new OpDefT failed\";\n",
        "      return nullptr;\n",
        "    }\n",
        "    newOpDef->name = inOpDef->name;\n",
        "    newOpDef->quantType = inOpDef->quantType;\n",
        "    newOpDef->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "    newOpDef->primitive->value.type = schema::PrimitiveType_Scale;\n",
        "    auto scaleParam = new(std::nothrow) ScaleT;\n",
        "    if (scaleParam == nullptr) {\n",
        "      MS_LOG(ERROR) << \"new scaleParam failed\";\n",
        "      return nullptr;\n",
        "    }\n",
        "    auto inParam = inOpDef->primitive->value.AsScale();\n",
        "    MS_ASSERT(inParam != nullptr);\n",
        "    scaleParam->axis = inParam->axis;\n",
        "    newOpDef->primitive->value.value = scaleParam;\n",
        "    return std::move(newOpDef);\n",
        "  };\n",
        "};\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n",
        "\n",
        "#endif  // MINDSPORE_PREDICT_BATCHNORM_CONVERT_SCALE_PASS_H\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvJsKRJAT25-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/converter/legacy_optimizer/fusion/batchnorm_convert_scale_pass.h\"\n",
        "#include <cfloat>\n",
        "#include <memory>\n",
        "#include <string>\n",
        "#include <unordered_map>\n",
        "#include <utility>\n",
        "#include <vector>\n",
        "#include \"utils/log_adapter.h\"\n",
        "#include \"tools/common/graph_util.h\"\n",
        "#include \"tools/common/tensor_util.h\"\n",
        "#include \"include/errorcode.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"src/common/op_utils.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "#define CAFFE_BATCHNORM_OP_WEIGHT_NUM 2\n",
        "#define TF_BATCHNORM_OP_WEIGHT_NUM 4\n",
        "#define CAFFE_BATCHNORM_MEAN_INDEX 0\n",
        "#define CAFFE_BATCHNORM_VARIANCE_INDEX 1\n",
        "#define TF_BATCHNORM_SCALE_INDEX 0\n",
        "#define TF_BATCHNORM_BIAS_INDEX 1\n",
        "#define TF_BATCHNORM_MEAN_INDEX 2\n",
        "#define TF_BATCHNORM_VARIANCE_INDEX 3\n",
        "namespace {\n",
        "constexpr const float EPS = 1e-8;\n",
        "constexpr const float EPS_DEFAULT_FLOAT = 1e-5;\n",
        "constexpr const float POW_NUM = 0.5;\n",
        "constexpr const int32_t NCHW_DIM_C = 1;\n",
        "}\n",
        "STATUS BatchNormConvertScalePass::Run(MetaGraphT *graph) { return FusionPass::Run(graph); }\n",
        "\n",
        "STATUS BatchNormConvertScalePass::DefinePattern() {\n",
        "  // with preNode\n",
        "  {\n",
        "    auto inputOp = std::make_shared<PatternOp>();\n",
        "    inputOp->id = inputOpName;\n",
        "    inputOp->types = {schema::PrimitiveType_NONE};\n",
        "    inputOp->isPlaceHold = true;\n",
        "\n",
        "    auto bnOp = std::make_shared<PatternOp>();\n",
        "    bnOp->id = bnOpName;\n",
        "    bnOp->types = {schema::PrimitiveType_FusedBatchNorm, schema::PrimitiveType_BatchNorm};\n",
        "    bnOp->left = inputOp;\n",
        "\n",
        "    std::unique_ptr<FusionPattern> fusionPattern(new(std::nothrow) FusionPattern(bnPatternName));\n",
        "    if (fusionPattern == nullptr) {\n",
        "      MS_LOG(ERROR) << \"new fusionPattern failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    fusionPattern->AddPatternOp(inputOp);\n",
        "    fusionPattern->AddPatternOp(bnOp);\n",
        "    fusionPattern->Finish();\n",
        "\n",
        "    this->patterns.emplace_back(fusionPattern.release());\n",
        "  }\n",
        "\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS BatchNormConvertScalePass::DoFusion(MetaGraphT *graph, const std::string &patternName,\n",
        "                                           std::unordered_map<std::string, std::shared_ptr<Path>> &matchedPath) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  if (patternName != bnPatternName) {\n",
        "    MS_LOG(ERROR) << \"BatchNormConvertScale-Fusion match failed\";\n",
        "    return RET_PARAM_INVALID;\n",
        "  }\n",
        "  auto status = FindNodes(graph, matchedPath);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"FindNodes failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  auto bnPath = matchedPath.at(bnOpName);\n",
        "  status = GetTransParam(graph, bnPath);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"GetTransParam failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "\n",
        "  status = GenNewScaleTensor(graph, bnPath);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"GenNewScaleTensor failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "\n",
        "  status = ConvertBNToScale(graph, bnPath);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"GenNewScaleTensor failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS BatchNormConvertScalePass::ConvertBNToScale(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath) {\n",
        "  auto scaleNode = std::unique_ptr<CNodeT>(new(std::nothrow) CNodeT);\n",
        "  if (scaleNode == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new TransNode failed\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  scaleNode->name = bnNode->name;\n",
        "  scaleNode->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  if (scaleNode->primitive == nullptr) {\n",
        "    MS_LOG(ERROR) << \"op->primitive is null\";\n",
        "    return RET_NULL_PTR;\n",
        "  }\n",
        "  scaleNode->primitive->value.type = schema::PrimitiveType_Scale;\n",
        "  std::unique_ptr<ScaleT> scaleParam(new ScaleT());\n",
        "  if (scaleParam == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new transposeParam failed\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  scaleParam->axis = NCHW_DIM_C;\n",
        "  scaleNode->primitive->value.value = scaleParam.release();\n",
        "  auto scaleIter = graph->nodes.begin() + bnPath->nodeIdx;\n",
        "  STATUS errorCode = RET_OK;\n",
        "  scaleIter =\n",
        "      InsertNode(graph, scaleIter, kBefore, 0, std::move(scaleNode), &errorCode, ScaleOpCopyer, false);\n",
        "  if (errorCode != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"InsertNode failed: %d\";  // errorCode);\n",
        "    return errorCode;\n",
        "  }\n",
        "  auto &newScaleNode = *(scaleIter - 1);\n",
        "  graph->allTensors.emplace_back(std::move(newScaleWeightTensor));\n",
        "  auto weightTensorIdx = graph->allTensors.size() - 1;\n",
        "  graph->allTensors.emplace_back(std::move(newScaleBiasTensor));\n",
        "  auto biasTensorIdx = graph->allTensors.size() - 1;\n",
        "  newScaleNode->inputIndex.push_back(weightTensorIdx);\n",
        "  newScaleNode->inputIndex.push_back(biasTensorIdx);\n",
        "  // delete bn node\n",
        "  auto status = IsolateOneWayNode(graph, bnPath->nodeIdx + 1, true);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"IsolateOneWayNode \" << bnNode->name.c_str() << \" failed, error: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS BatchNormConvertScalePass::GenNewScaleTensor(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  GetTransParam(graph, bnPath);\n",
        "  newScaleWeightTensor = std::unique_ptr<TensorT>(new(std::nothrow) TensorT);\n",
        "  if (newScaleWeightTensor == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new weightTensor failed\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  newScaleWeightTensor->dataType = bnMeanTensor->dataType;\n",
        "  newScaleWeightTensor->format = bnMeanTensor->format;\n",
        "  newScaleWeightTensor->refCount = schema::NodeType_ValueNode;\n",
        "  newScaleWeightTensor->dims = bnMeanTensor->dims;\n",
        "  auto weightShapeSize = GetShapeSize(*bnMeanTensor);\n",
        "  newScaleWeightTensor->data.resize(weightShapeSize * sizeof(float));\n",
        "  auto ret = memcpy_s(newScaleWeightTensor->data.data(), weightShapeSize * sizeof(float), transScale,\n",
        "                      weightShapeSize * sizeof(float));\n",
        "  if (ret != RET_OK) {\n",
        "    delete transScale;\n",
        "    MS_LOG(ERROR) << \"memcpy error: \" << ret;\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "\n",
        "  newScaleBiasTensor = std::unique_ptr<TensorT>(new(std::nothrow) TensorT);\n",
        "  if (newScaleBiasTensor == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new weightTensor failed\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  newScaleBiasTensor->dataType = bnMeanTensor->dataType;\n",
        "  newScaleBiasTensor->format = bnMeanTensor->format;\n",
        "\n",
        "  newScaleBiasTensor->refCount = schema::NodeType_ValueNode;\n",
        "  newScaleBiasTensor->dims = bnMeanTensor->dims;\n",
        "  weightShapeSize = GetShapeSize(*bnMeanTensor);\n",
        "  newScaleBiasTensor->data.resize(weightShapeSize * sizeof(float));\n",
        "  ret = memcpy_s(newScaleBiasTensor->data.data(), weightShapeSize * sizeof(float), transBias,\n",
        "                 weightShapeSize * sizeof(float));\n",
        "  if (ret != RET_OK) {\n",
        "    delete transBias;\n",
        "    MS_LOG(ERROR) << \"memcpy error: \" << ret;\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "STATUS BatchNormConvertScalePass::FindNodes(MetaGraphT *graph,\n",
        "                                            const std::unordered_map<std::string, std::shared_ptr<Path>> &matchedPath) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  auto inputPath = matchedPath.at(inputOpName);\n",
        "  auto bnPath = matchedPath.at(bnOpName);\n",
        "  MS_ASSERT(inputPath != nullptr);\n",
        "  MS_ASSERT(bnPath != nullptr);\n",
        "  if (inputPath->subGraphIdx != bnPath->subGraphIdx) {\n",
        "    MS_LOG(ERROR) << \"matched nodes should from same subGraph\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  MS_ASSERT(graph->nodes.size() > inputPath->nodeIdx);\n",
        "  MS_ASSERT(graph->nodes.size() > bnPath->nodeIdx);\n",
        "  inputNode = graph->nodes.at(inputPath->nodeIdx).get();\n",
        "  bnNode = graph->nodes.at(bnPath->nodeIdx).get();\n",
        "  MS_ASSERT(inputNode != nullptr);\n",
        "  MS_ASSERT(bnNode != nullptr);\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS BatchNormConvertScalePass::GetTransParam(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  MS_ASSERT(bnPath != nullptr);\n",
        "\n",
        "  BNWeightTensors bnWeightTensors;\n",
        "\n",
        "  auto status = GetBnWeightTensors(graph, bnPath, bnWeightTensors);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"GetBnWeightTensors error\";\n",
        "    return status;\n",
        "  }\n",
        "  auto *meanTensor = bnWeightTensors.meanTensor;\n",
        "  auto *varianceTensor = bnWeightTensors.varianceTensor;\n",
        "  auto *scaleTensor = bnWeightTensors.scaleTensor;\n",
        "  auto *biasTensor = bnWeightTensors.biasTensor;\n",
        "\n",
        "  auto *meanData = reinterpret_cast<float *>(meanTensor->data.data());\n",
        "  auto *varianceData = reinterpret_cast<float *>(varianceTensor->data.data());\n",
        "\n",
        "  float eps = EPS_DEFAULT_FLOAT;\n",
        "  status = GetBnEpsilon(graph, eps);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"GetBnEpsilon failed\";\n",
        "    return status;\n",
        "  }\n",
        "  this->transScale = new(std::nothrow) float[bnChannel];\n",
        "  this->transBias = new(std::nothrow) float[bnChannel];\n",
        "  // cal transScale, tf : scale/sqrt(variance + eps); caffe : 1/sqrt(variance + eps)\n",
        "  if (memcpy_s(transScale, bnChannel * sizeof(float), varianceData, bnChannel * sizeof(float)) != 0) {\n",
        "    MS_LOG(ERROR) << \"memcpy_s transScale error\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  // 1/sqrt(variance + eps)\n",
        "  for (int32_t i = 0; i < bnChannel; i++) {\n",
        "    float tmp = transScale[i] + eps;\n",
        "    tmp = pow(tmp, POW_NUM);\n",
        "    transScale[i] = 1 / tmp;\n",
        "  }\n",
        "\n",
        "  if (scaleTensor != nullptr) {\n",
        "    auto *scaleData = reinterpret_cast<float *>(scaleTensor->data.data());\n",
        "    // scale/sqrt(variance + eps)\n",
        "    for (int32_t i = 0; i < bnChannel; i++) {\n",
        "      transScale[i] *= scaleData[i];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // cal transBias, tf : -scale*mean/sqrt(variance + eps) + bias; caffe : -mean/sqrt(variance + eps)\n",
        "  //-mean/sqrt(variance + eps)\n",
        "  for (int32_t i = 0; i < bnChannel; i++) {\n",
        "    transBias[i] = -meanData[i] * transScale[i];\n",
        "  }\n",
        "\n",
        "  if (biasTensor != nullptr) {\n",
        "    auto *biasData = reinterpret_cast<float *>(biasTensor->data.data());\n",
        "    //-scale*mean/sqrt(variance + eps) + bias\n",
        "    for (int32_t i = 0; i < bnChannel; i++) {\n",
        "      transBias[i] += biasData[i];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "// BatchNorm weight Tensor definition:\n",
        "// caffe\n",
        "//   estimated_mean  --0\n",
        "//   estimated_variance  --1\n",
        "// tensorflow\n",
        "//   scale    -- 0\n",
        "//   bias        --1\n",
        "//   estimated_mean  --2\n",
        "//   estimated_variance  --3\n",
        "STATUS BatchNormConvertScalePass::GetBnWeightTensors(MetaGraphT *graph, const std::shared_ptr<Path> &bnPath,\n",
        "                                                     BNWeightTensors &bnWeightTensors) {\n",
        "  if (graph == nullptr || bnPath == nullptr) {\n",
        "    MS_LOG(ERROR) << \"null pointer dereferencing.\";\n",
        "    return RET_NULL_PTR;\n",
        "  }\n",
        "  MS_ASSERT(graph->allTensors.size() > bnNode->inputIndex.at(1));\n",
        "  auto bnWeightTensorIdxes = bnNode->inputIndex;\n",
        "  bnWeightTensorIdxes.erase(bnWeightTensorIdxes.begin());\n",
        "  if (bnWeightTensorIdxes.size() == CAFFE_BATCHNORM_OP_WEIGHT_NUM) {\n",
        "    bnWeightTensors.meanTensor = graph->allTensors.at(bnWeightTensorIdxes[CAFFE_BATCHNORM_MEAN_INDEX]).get();\n",
        "    bnWeightTensors.varianceTensor = graph->allTensors.at(bnWeightTensorIdxes[CAFFE_BATCHNORM_VARIANCE_INDEX]).get();\n",
        "  } else if (bnWeightTensorIdxes.size() == TF_BATCHNORM_OP_WEIGHT_NUM) {\n",
        "    bnWeightTensors.scaleTensor = graph->allTensors.at(bnWeightTensorIdxes[TF_BATCHNORM_SCALE_INDEX]).get();\n",
        "    bnWeightTensors.biasTensor = graph->allTensors.at(bnWeightTensorIdxes[TF_BATCHNORM_BIAS_INDEX]).get();\n",
        "    bnWeightTensors.meanTensor = graph->allTensors.at(bnWeightTensorIdxes[TF_BATCHNORM_MEAN_INDEX]).get();\n",
        "    bnWeightTensors.varianceTensor = graph->allTensors.at(bnWeightTensorIdxes[TF_BATCHNORM_VARIANCE_INDEX]).get();\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"BatchNorm should has 2 or 4 weight tensors, current number of weight tensors: \"\n",
        "                  << bnWeightTensorIdxes.size();\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "\n",
        "  if (bnWeightTensors.meanTensor == nullptr) {\n",
        "    MS_LOG(ERROR) << \"BatchNorm's mean tensor is nullptr\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "\n",
        "  if (bnWeightTensors.varianceTensor == nullptr) {\n",
        "    MS_LOG(ERROR) << \"BatchNorm's variance tensor is nullptr\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  bnChannel = bnWeightTensors.meanTensor->data.size() * sizeof(uint8_t) / sizeof(float);\n",
        "  if (bnChannel <= 0) {\n",
        "    MS_LOG(ERROR) << \"BatchNorm's channel less or equal 0\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  bnMeanTensor = bnWeightTensors.meanTensor;\n",
        "  if (bnChannel != bnWeightTensors.varianceTensor->data.size() * sizeof(uint8_t) / sizeof(float)) {\n",
        "    MS_LOG(ERROR) << \"conv kernel num expected to be equal to variance size\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "\n",
        "  if (bnWeightTensors.scaleTensor != nullptr) {\n",
        "    if (bnChannel != bnWeightTensors.scaleTensor->data.size() * sizeof(uint8_t) / sizeof(float)) {\n",
        "      MS_LOG(ERROR) << \"conv kernel num  expected to be equal to scale size\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (bnWeightTensors.biasTensor != nullptr) {\n",
        "    if (bnChannel != bnWeightTensors.biasTensor->data.size() * sizeof(uint8_t) / sizeof(float)) {\n",
        "      MS_LOG(ERROR) << \"conv kernel num expected to be equal to bias size\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "STATUS BatchNormConvertScalePass::GetBnEpsilon(MetaGraphT *graph, float &eps) {\n",
        "  if (graph == nullptr) {\n",
        "    MS_LOG(ERROR) << \"null pointer dereferencing.\";\n",
        "    return RET_NULL_PTR;\n",
        "  }\n",
        "  if (bnNode == nullptr) {\n",
        "    MS_LOG(ERROR) << \"null pointer dereferencing.\";\n",
        "    return RET_NULL_PTR;\n",
        "  }\n",
        "  if (bnNode->primitive->value.type == schema::PrimitiveType_FusedBatchNorm) {\n",
        "    eps = bnNode->primitive->value.AsFusedBatchNorm()->epsilon;\n",
        "  } else if (bnNode->primitive->value.type == schema::PrimitiveType_BatchNorm) {\n",
        "    eps = bnNode->primitive->value.AsBatchNorm()->epsilon;\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"match pattern has error, not BatchNorm node\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "\n",
        "  if (eps < EPS) {\n",
        "    eps = EPS_DEFAULT_FLOAT;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "BatchNormConvertScalePass::~BatchNormConvertScalePass() {\n",
        "  if (this->transScale != nullptr) {\n",
        "    delete (this->transScale);\n",
        "  }\n",
        "  if (this->transBias != nullptr) {\n",
        "    delete (this->transBias);\n",
        "  }\n",
        "}\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS8FDBIx3tmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *conv_activation_fusion.h\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/optimizer/fusion/transform_batchnorm_to_scale.h\"\n",
        "#include <memory>\n",
        "#include \"src/param_value_lite.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"src/ir/primitive_t_value.h\"\n",
        "#include \"utils/utils.h\"\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include \"securec/include/securec.h\"\n",
        "\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "const int32_t NCHW_DIM_C = 1;\n",
        "constexpr size_t kCaffeBNMeanIndex = 2;\n",
        "constexpr size_t kTFBNMeanIndex = 4;\n",
        "bool IsBatchNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_BatchNorm || type == schema::PrimitiveType_FusedBatchNorm;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "bool IsNotConvNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type != schema::PrimitiveType_Conv2D && type != schema::PrimitiveType_DepthwiseConv2D;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "const AnfNodePtr MakeNewSaleNode() {\n",
        "  auto attr = std::make_unique<schema::ScaleT>();\n",
        "  attr->axis = NCHW_DIM_C;\n",
        "  auto prim = std::make_unique<schema::PrimitiveT>();\n",
        "  prim->value.value = attr.release();\n",
        "  prim->value.type = schema::PrimitiveType_Scale;\n",
        "  auto primTValue = std::make_shared<lite::PrimitiveTValue>(prim.release());\n",
        "  auto value_node = NewValueNode(primTValue);\n",
        "  return value_node;\n",
        "}\n",
        "\n",
        "}  // namespace\n",
        "const BaseRef BatchNormToScaleFusion::DefinePattern() const {\n",
        "  auto any_var = std::make_shared<CondVar>(IsNotConvNode);\n",
        "  auto bn_var = std::make_shared<CondVar>(IsBatchNode);\n",
        "  auto bn_mean_var = std::make_shared<CondVar>(IsParamNode);\n",
        "  auto bn_variable_var = std::make_shared<CondVar>(IsParamNode);\n",
        "  auto bn_other_var = std::make_shared<SeqVar>();\n",
        "  return VectorRef({bn_var, any_var, bn_mean_var, bn_variable_var, bn_other_var});;\n",
        "}\n",
        "// BatchNorm weight Tensor definition:\n",
        "// caffe\n",
        "//   estimated_mean  --0\n",
        "//   estimated_variance  --1\n",
        "// tensorflow\n",
        "//   scale    -- 0\n",
        "//   bias        --1\n",
        "//   estimated_mean  --2\n",
        "//   estimated_variance  --3\n",
        "const AnfNodePtr BatchNormToScaleFusion::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                                 const EquivPtr &) const {\n",
        "  MS_ASSERT(nullptr != func_graph);\n",
        "  MS_ASSERT(nullptr != node);\n",
        "  auto bn_node = utils::cast<CNodePtr>(node);\n",
        "  if (bn_node == nullptr) {\n",
        "    MS_LOG(ERROR) << \"batchnorm to scale fusion not found cnode.\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto input_size = bn_node->inputs().size();\n",
        "  if (input_size != 4 && input_size != 6) {\n",
        "    return nullptr;\n",
        "  }\n",
        "  // tf or caffe mean tensor shape as bn channels\n",
        "  AnfNodePtr bn_mean_node = nullptr;\n",
        "  if (GetCNodeType(bn_node) == schema::PrimitiveType_BatchNorm) {\n",
        "    bn_mean_node = bn_node->input(kCaffeBNMeanIndex);\n",
        "  } else if (GetCNodeType(bn_node) == schema::PrimitiveType_FusedBatchNorm) {\n",
        "    bn_mean_node = bn_node->input(kTFBNMeanIndex);\n",
        "  } else {\n",
        "    MS_LOG(EXCEPTION) << \"not caffe or tf batchnorm op.\";\n",
        "  }\n",
        "  auto bn_mean_param = bn_mean_node->cast<ParameterPtr>()->default_param();\n",
        "  auto bn_mean_tensor = std::dynamic_pointer_cast<ParamValueLite>(bn_mean_param);\n",
        "  auto bn_channels = bn_mean_tensor->tensor_shape_size();\n",
        "  if (bn_channels <= 0) {\n",
        "    MS_LOG(ERROR) << \"Unsupported bn chanle size\";\n",
        "    return node;\n",
        "  }\n",
        "  auto trans_scale = new(std::nothrow) float[bn_channels];\n",
        "  if (trans_scale == nullptr) {\n",
        "    delete[] trans_scale;\n",
        "    MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto trans_bias = new(std::nothrow) float[bn_channels];\n",
        "  if (trans_bias == nullptr) {\n",
        "    MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "    delete[] trans_bias;\n",
        "    return nullptr;\n",
        "  }\n",
        "  GenTransParam(bn_node, bn_channels, trans_scale, trans_bias);\n",
        "  auto scale_weight_node = AddNewBiasNode(trans_scale, func_graph, bn_channels, bn_mean_tensor);\n",
        "  auto scale_bias_node = AddNewBiasNode(trans_bias, func_graph, bn_channels, bn_mean_tensor);\n",
        "  std::vector<AnfNodePtr> op_inputs = {MakeNewSaleNode(), bn_node->input(1), scale_weight_node, scale_bias_node};\n",
        "  auto new_scale_node = func_graph->NewCNode(op_inputs);\n",
        "  new_scale_node->set_fullname_with_scope(\"scale_from_bn\");\n",
        "  new_scale_node->set_abstract(bn_node->abstract());\n",
        "  return new_scale_node;\n",
        "}\n",
        "}  // namespace mindspore::opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKFrOdEZhwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0tjHVJLWiu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.Inf\n",
        "        else:\n",
        "            self.val_score = -np.Inf\n",
        "\n",
        "    def __call__(self, epoch_score, model, model_path):\n",
        "\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        self.val_score = epoch_score\n",
        "\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZii14QqRPRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals) / len(vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPemFrvdRRGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ./input/roberta-base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPGTmCBaRTyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = './input/roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "config = RobertaConfig.from_pretrained('roberta-base')\n",
        "tokenizer.save_vocabulary(save_path)\n",
        "model.save_pretrained(save_path)\n",
        "config.save_pretrained(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0aXDt1bSAbW",
        "colab_type": "text"
      },
      "source": [
        "#config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYLQJVsWRVro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class config:\n",
        "    LEARNING_RATE = 4e-5\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    VALID_BATCH_SIZE = 32\n",
        "    EPOCHS = 3\n",
        "    TRAINING_FILE = \"./tweet-sentiment/train_8folds.csv\"\n",
        "    ROBERTA_PATH = \"./input/roberta-base\"\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
        "        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
        "        lowercase=True,\n",
        "        add_prefix_space=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TM1V8MRXZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "    \n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "    \n",
        "    targets_start = target_idx[0]\n",
        "    targets_end = target_idx[-1]\n",
        "\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "533cqoc8RYLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item], \n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxH2zzU_R7Rr",
        "colab_type": "text"
      },
      "source": [
        "#TweetModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPEb3HPjRbWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n",
        "        self.drop_out = nn.Dropout(0.1)\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "        out = self.drop_out(out)\n",
        "        logits = self.l0(out)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMZTa4VvRcdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvGvBvYSR42T",
        "colab_type": "text"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoSeSyC1Rf7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n",
        "    model.train()\n",
        "    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n",
        "    for bi, d in enumerate(tk0):\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        sentiment = d[\"sentiment\"]\n",
        "        orig_selected = d[\"orig_selected\"]\n",
        "        orig_tweet = d[\"orig_tweet\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        offsets = d[\"offsets\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets_start = targets_start.to(device, dtype=torch.long)\n",
        "        targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs_start, outputs_end = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer, barrier=True)\n",
        "        scheduler.step()\n",
        "        tk0.set_postfix(loss=loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l801oRiR16g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate_jaccard_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfPApEVQRh7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    jaccards = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Validating\")\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            sentiment = d[\"sentiment\"]\n",
        "            orig_selected = d[\"orig_selected\"]\n",
        "            orig_tweet = d[\"orig_tweet\"]\n",
        "            targets_start = d[\"targets_start\"]\n",
        "            targets_end = d[\"targets_end\"]\n",
        "            offsets = d[\"offsets\"].cpu().numpy()\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets_start = targets_start.to(device, dtype=torch.long)\n",
        "            targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                tweet_sentiment = sentiment[px]\n",
        "                jaccard_score, _ = calculate_jaccard_score(\n",
        "                    original_tweet=tweet,\n",
        "                    target_string=selected_tweet,\n",
        "                    sentiment_val=tweet_sentiment,\n",
        "                    idx_start=np.argmax(outputs_start[px, :]),\n",
        "                    idx_end=np.argmax(outputs_end[px, :]),\n",
        "                    offsets=offsets[px]\n",
        "                )\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            losses.update(loss.item(), ids.size(0))\n",
        "            tk0.set_postfix(loss=loss.item())\n",
        "\n",
        "    return jaccards.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rab2AgntRk3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfx = pd.read_csv(config.TRAINING_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__u6TEtURyXQ",
        "colab_type": "text"
      },
      "source": [
        "#run fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McDosqmsRley",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(fold):\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n",
        "    model_config.output_hidden_states = True\n",
        "    MX = TweetModel(conf=model_config)\n",
        "    \n",
        "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    device = xm.xla_device(fold + 1)\n",
        "    model = MX.to(device)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\n",
        "        \"bias\",\n",
        "        \"LayerNorm.bias\",\n",
        "        \"LayerNorm.weight\"\n",
        "    ]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if not any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "         'weight_decay': 0.001\n",
        "        },\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "            'weight_decay': 0.0\n",
        "        },\n",
        "    ]\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        "    )\n",
        "    optimizer = AdamW(\n",
        "        optimizer_parameters, \n",
        "        lr=config.LEARNING_RATE\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_jac = 0\n",
        "    es = EarlyStopping(patience=2, mode=\"max\")\n",
        "    num_batches = int(len(df_train) / config.TRAIN_BATCH_SIZE)\n",
        "    \n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_fn(\n",
        "            train_data_loader, \n",
        "            model, \n",
        "            optimizer, \n",
        "            device,\n",
        "            num_batches,\n",
        "            scheduler\n",
        "        )\n",
        "\n",
        "        jac = eval_fn(\n",
        "            valid_data_loader, \n",
        "            model, \n",
        "            device\n",
        "        )\n",
        "        print(f'Epoch={epoch}, Fold={fold}, Jaccard={jac}')\n",
        "        if jac > best_jac:\n",
        "            xm.save(model.state_dict(), f\"model_{fold}.bin\")\n",
        "            best_jac = jac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP2bUXHURpc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from joblib import Parallel,delayed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hB75xirRrXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Parallel(n_jobs=8, backend=\"threading\")(delayed(run)(i) for i in range(8))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}