{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuppI1BtB91H"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/fusion/batchmatmul_fusion.h\"\n",
        "#include <memory>\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "#include \"src/param_value_lite.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"utils/utils.h\"\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include \"securec/include/securec.h\"\n",
        "\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "bool IsStackNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Stack;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "bool IsFullConnectNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_FullConnection;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "void *GetInputAddr(const AnfNodePtr &node, int input_index) {\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  if (!node->isa<CNode>()) {\n",
        "    MS_LOG(ERROR) << \"GetInputAddr not cnode\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  if (input_index >= cnode->inputs().size()) {\n",
        "    MS_LOG(ERROR) << \"input index error\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  if (cnode->input(input_index)->isa<Parameter>()) {\n",
        "    auto param_input = cnode->input(input_index)->cast<ParameterPtr>();\n",
        "    auto param_value = std::dynamic_pointer_cast<ParamValueLite>(param_input->default_param());;\n",
        "    if (param_value == nullptr) {\n",
        "      MS_LOG(ERROR) << \"param not paramValueLite\";\n",
        "      return nullptr;\n",
        "    }\n",
        "    return param_value->tensor_addr();\n",
        "  }\n",
        "  MS_LOG(ERROR) << \"input not paramter\";\n",
        "  return nullptr;\n",
        "}\n",
        "STATUS GetRightMatmulInputParamter(CNodePtr &stack_node, ParameterPtr &rmatmul_input) {\n",
        "  MS_ASSERT(stack_node != nullptr);\n",
        "  MS_ASSERT(right_matmul_input != nullptr);\n",
        "  auto joint_fullconnect_size = stack_node->inputs().size() - 1;\n",
        "  auto fc = stack_node->input(1)->cast<CNodePtr>();\n",
        "  auto fc_weight = fc->input(2)->cast<ParameterPtr>();\n",
        "  auto fc_weight_param = std::dynamic_pointer_cast<ParamValueLite>(fc_weight->default_param());\n",
        "  auto tensor_size = fc_weight_param->tensor_size();\n",
        "  auto rmatmul_input_shape = fc_weight_param->tensor_shape();\n",
        "  auto new_tensor_data = new(std::nothrow) int8_t[joint_fullconnect_size * tensor_size];\n",
        "  if (new_tensor_data == nullptr) {\n",
        "    MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  for (int i = 1; i < joint_fullconnect_size + 1; i++) {\n",
        "    auto tensor_addr = GetInputAddr(stack_node->input(i), 2);\n",
        "    if (tensor_addr == nullptr) {\n",
        "      MS_LOG(ERROR) << \"input tensor addr nullptr\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    if (EOK != memcpy_s(new_tensor_data + (i - 1) * tensor_size, tensor_size, tensor_addr, tensor_size)) {\n",
        "      MS_LOG(ERROR) << \"memcpy_s data failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  rmatmul_input_shape.insert(rmatmul_input_shape.begin(), joint_fullconnect_size);\n",
        "  auto type_ptr = TypeIdToType(fc_weight_param->tensor_type());\n",
        "  auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(type_ptr, rmatmul_input_shape);\n",
        "  rmatmul_input->set_abstract(abstract_tensor);\n",
        "  rmatmul_input->set_name(stack_node->fullname_with_scope() + \"right_parameter\");\n",
        "  ParamValueLitePtr param_value = std::make_shared<ParamValueLite>();\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  param_value->set_tensor_shape(rmatmul_input_shape);\n",
        "  param_value->set_tensor_type(fc_weight_param->tensor_type());\n",
        "  param_value->set_format(fc_weight_param->format());\n",
        "  param_value->set_tensor_addr(new_tensor_data);\n",
        "  param_value->set_tensor_size(joint_fullconnect_size * tensor_size);\n",
        "  rmatmul_input->set_default_param(param_value);\n",
        "  return RET_OK;\n",
        "}\n",
        "}  // namespace\n",
        "const BaseRef BatchMatMulFusion::DefinePattern() const {\n",
        "  auto pack_var = std::make_shared<CondVar>(IsStackNode);\n",
        "  auto fullconnect_var = std::make_shared<CondVar>(IsFullConnectNode);\n",
        "  auto bn_other_var = std::make_shared<SeqVar>();\n",
        "  return VectorRef({pack_var, fullconnect_var, fullconnect_var, bn_other_var});\n",
        "}\n",
        "\n",
        "// slice +fullconnect ->batchmatmul\n",
        "const AnfNodePtr BatchMatMulFusion::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                            const EquivPtr &) const {\n",
        "\n",
        "  MS_ASSERT(func_graph != nullptr);\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  auto stack_cnode = node->cast<CNodePtr>();\n",
        "  // check stack node all inputs must fullconnect\n",
        "  for (int i = 1; i < stack_cnode->inputs().size(); i++) {\n",
        "    auto input_node = stack_cnode->input(i);\n",
        "    if (!IsFullConnectNode(input_node)) {\n",
        "      MS_LOG(WARNING) << \"batchmatmulfusion stack node all inputs must fullconnect type\";\n",
        "      return nullptr;\n",
        "    }\n",
        "  }\n",
        "  auto fullconnect_node = stack_cnode->input(1);\n",
        "  MS_ASSERT(fullconnnect_node != nullptr);\n",
        "  auto fullconnect_cnode = fullconnect_node->cast<CNodePtr>();\n",
        "  MS_ASSERT(fullconnect_cnode->inputs().size() == 3);\n",
        "  auto left_slice_node = fullconnect_cnode->input(1);\n",
        "  auto left_slice_cnode = left_slice_node->cast<CNodePtr>();\n",
        "  auto left_matmul_input = left_slice_cnode->input(1);\n",
        "  auto right_reshape_node = fullconnect_cnode->input(2);\n",
        "\n",
        "  auto matmul_primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  std::unique_ptr<schema::MatMulT> attr = std::make_unique<schema::MatMulT>();\n",
        "  matmul_primitive->value.type = schema::PrimitiveType_MatMul;\n",
        "  matmul_primitive->value.value = attr.release();\n",
        "  auto matmul_cvalue = lite::PrimitiveC::Create(matmul_primitive.release());\n",
        "  // get matmul quantParams\n",
        "  std::vector<schema::QuantParamT> jointed_quant_params;\n",
        "  for (int i = 1; i < 9; i++) {\n",
        "    auto fullconnect_node2 = stack_cnode->input(i)->cast<CNodePtr>();\n",
        "    auto fc_prim = GetValueNode<std::shared_ptr<lite::PrimitiveC>>(fullconnect_node2->input(0));\n",
        "    auto fc_input_quantParams = fc_prim->GetInputQuantParams();\n",
        "    if (fc_input_quantParams.size() > 1 && !fc_input_quantParams[1].empty()) {\n",
        "      jointed_quant_params.push_back(fc_input_quantParams[1][0]);\n",
        "    }\n",
        "  }\n",
        "  auto fc_prim = GetValueNode<std::shared_ptr<lite::PrimitiveC>>(fullconnect_cnode->input(0));\n",
        "  auto rmatmul_quant_params = fc_prim->GetInputQuantParams();\n",
        "  rmatmul_quant_params.pop_back();\n",
        "  rmatmul_quant_params.pop_back();\n",
        "  // no bias quantParams\n",
        "  rmatmul_quant_params.emplace_back(jointed_quant_params);\n",
        "  matmul_cvalue->SetInputQuantParam(rmatmul_quant_params);\n",
        "  matmul_cvalue->SetOutputQuantParam(fc_prim->GetOutputQuantParams());\n",
        "  auto matmul_value_node = NewValueNode(std::shared_ptr<lite::PrimitiveC>(matmul_cvalue));\n",
        "  std::vector<AnfNodePtr> matmul_inputs = {matmul_value_node, left_matmul_input};\n",
        "\n",
        "  // batchmatmul right node may be const\n",
        "  if (right_reshape_node->isa<Parameter>()) {\n",
        "//    return stack_cnode;\n",
        "    auto rmatmul_paramter = func_graph->add_parameter();\n",
        "    if (GetRightMatmulInputParamter(stack_cnode, rmatmul_paramter) != RET_OK) {\n",
        "      MS_LOG(ERROR) << \"GetRightMatmulInputParamter failed\";\n",
        "      return node;\n",
        "    }\n",
        "    auto prim = GetValueNode<std::shared_ptr<lite::PrimitiveC>>(matmul_value_node);\n",
        "    prim->GetPrimitiveT()->value.AsMatMul()->transposeB = true;\n",
        "    matmul_inputs.push_back(rmatmul_paramter);\n",
        "  } else {\n",
        "    auto right_reshape_cnode = right_reshape_node->cast<CNodePtr>();\n",
        "    MS_ASSERT(right_reshape_cnode->inputs().size() > 1);\n",
        "    auto right_transpose_node = right_reshape_cnode->input(1);\n",
        "    auto right_transpose_cnode = right_transpose_node->cast<CNodePtr>();\n",
        "    auto right_slice_node = right_transpose_cnode->input(1);\n",
        "    auto right_slice_cnode = right_slice_node->cast<CNodePtr>();\n",
        "    auto right_matmul_input = right_slice_cnode->input(1);\n",
        "    matmul_inputs.push_back(right_matmul_input);\n",
        "  }\n",
        "  auto matmul_cnode = func_graph->NewCNode(matmul_inputs);\n",
        "  matmul_cnode->set_fullname_with_scope(\"matmul_\" + stack_cnode->fullname_with_scope());\n",
        "  MS_LOG(INFO) << \"stack node:\" << stack_cnode->fullname_with_scope() << \" batchmatmul fusion success\";\n",
        "  return matmul_cnode;\n",
        "}\n",
        "}  // namespace mindspore::opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SYOlFaHCfBI"
      },
      "source": [
        " if (input_tensor->shape().size() == 3\n",
        "      && input_tensor->GetQuantParams().size() == input_tensor->shape()[0]) { // per batch matmul\n",
        "    auto per_batch_size = input_tensor->shape()[0];\n",
        "    auto quant_param = input_tensor->GetQuantParams();\n",
        "    for (size_t i = 0; i < per_batch_size; i++) {\n",
        "      auto param = quant_param.at(i);\n",
        "      auto scale = param.scale;\n",
        "      auto zero_point = param.zeroPoint;\n",
        "      auto matrix_size = input_tensor->ElementsNum() / per_batch_size;\n",
        "      for (int64_t j = 0; j < matrix_size; j++) {\n",
        "        dequant_datas[i * matrix_size + j] =\n",
        "            static_cast<float>((quant_datas[i * matrix_size + j] - zero_point) * scale);\n",
        "      }\n",
        "    }\n",
        "    return dequant_datas;\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxrNyOcoKYZ2"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"src/runtime/kernel/arm/fp32/matmul.h\"\n",
        "#include \"nnacl/fp32/matmul.h\"\n",
        "#include \"src/runtime/runtime_api.h\"\n",
        "#include \"include/errorcode.h\"\n",
        "\n",
        "using mindspore::lite::RET_ERROR;\n",
        "using mindspore::lite::RET_INPUT_TENSOR_ERROR;\n",
        "using mindspore::lite::RET_MEMORY_FAILED;\n",
        "using mindspore::lite::RET_OK;\n",
        "\n",
        "namespace mindspore::kernel {\n",
        "MatmulCPUKernel::~MatmulCPUKernel() { FreeTmpBuffer(); }\n",
        "\n",
        "void MatmulCPUKernel::FreeTmpBuffer() {\n",
        "  if (a_c12_ptr_ != nullptr) {\n",
        "    free(a_c12_ptr_);\n",
        "    a_c12_ptr_ = nullptr;\n",
        "  }\n",
        "  if (b_r8_ptr_ != nullptr) {\n",
        "    free(b_r8_ptr_);\n",
        "    b_r8_ptr_ = nullptr;\n",
        "  }\n",
        "  if (bias_ptr_ != nullptr) {\n",
        "    free(bias_ptr_);\n",
        "    bias_ptr_ = nullptr;\n",
        "  }\n",
        "}\n",
        "\n",
        "int MatmulCPUKernel::ReSize() {\n",
        "  FreeTmpBuffer();\n",
        "  auto a_shape = in_tensors_[0]->shape();\n",
        "  auto b_shape = in_tensors_[1]->shape();\n",
        "  auto c_shape = out_tensors_[0]->shape();\n",
        "  if (in_tensors_.size() == 3) {\n",
        "    auto bias_shape = in_tensors_[2]->shape();\n",
        "    if (bias_shape[bias_shape.size() - 1] != c_shape[c_shape.size() - 1]) {\n",
        "      MS_LOG(ERROR) << \"The bias' dimension is not equal with column\";\n",
        "      return RET_INPUT_TENSOR_ERROR;\n",
        "    }\n",
        "  }\n",
        "  bool a_broadcast = false;\n",
        "  bool b_broadcast = false;\n",
        "  if (a_shape.size() == 2 && b_shape.size() == 2) {\n",
        "    a_broadcast = false;\n",
        "    b_broadcast = false;\n",
        "  } else if (a_shape.size() == 2 && (b_shape.size() != 2 && b_shape[b_shape.size() - 3] != 1)) {\n",
        "    a_broadcast = true;\n",
        "  } else if ((a_shape.size() != 2 && a_shape[a_shape.size() - 3] != 1) && b_shape.size() == 2) {\n",
        "    b_broadcast = true;\n",
        "  } else if (a_shape[a_shape.size() - 3] == 1 && b_shape.size() > 2 && b_shape[b_shape.size() - 3] != 1) {\n",
        "    a_broadcast = true;\n",
        "  } else if (a_shape[a_shape.size() - 3] != 1 && a_shape.size() > 2 && b_shape[b_shape.size() - 3] == 1) {\n",
        "    b_broadcast = true;\n",
        "  }\n",
        "  params_->a_broadcast_ = a_broadcast;\n",
        "  params_->b_broadcast_ = b_broadcast;\n",
        "  int a_batch = 1;\n",
        "  int b_batch = 1;\n",
        "  for (size_t i = 0; i < a_shape.size() - 2; ++i) {\n",
        "    a_batch *= a_shape[i];\n",
        "  }\n",
        "  for (size_t i = 0; i < b_shape.size() - 2; ++i) {\n",
        "    b_batch *= b_shape[i];\n",
        "  }\n",
        "  params_->a_batch = a_batch;\n",
        "  params_->b_batch = b_batch;\n",
        "  params_->row_ = c_shape[c_shape.size() - 2];\n",
        "  params_->col_ = c_shape[c_shape.size() - 1];\n",
        "  params_->deep_ = params_->a_transpose_ ? a_shape[a_shape.size() - 2] : a_shape[a_shape.size() - 1];\n",
        "  params_->row_4_ = UP_ROUND(params_->row_, C4NUM);\n",
        "  params_->row_12_ = UP_ROUND(params_->row_, C12NUM);\n",
        "  params_->col_8_ = UP_ROUND(params_->col_, 8);\n",
        "  thread_count_ = MSMIN(thread_count_, UP_DIV(params_->col_8_, 8));\n",
        "  thread_stride_ = UP_DIV(UP_DIV(params_->col_8_, 8), thread_count_);\n",
        "\n",
        "#ifdef ENABLE_ARM32\n",
        "  a_c12_ptr_ = reinterpret_cast<float *>(malloc(params_->a_batch * params_->row_4_ * params_->deep_ * sizeof(float)));\n",
        "  if (a_c12_ptr_ == nullptr) {\n",
        "    FreeTmpBuffer();\n",
        "    return RET_MEMORY_FAILED;\n",
        "  }\n",
        "  memset(a_c12_ptr_, 0, params_->row_4_ * params_->deep_ * sizeof(float));\n",
        "#else\n",
        "  a_c12_ptr_ = reinterpret_cast<float *>(malloc(params_->a_batch * params_->row_12_ * params_->deep_ * sizeof(float)));\n",
        "  if (a_c12_ptr_ == nullptr) {\n",
        "    FreeTmpBuffer();\n",
        "    return RET_MEMORY_FAILED;\n",
        "  }\n",
        "  memset(a_c12_ptr_, 0, params_->row_12_ * params_->deep_ * sizeof(float));\n",
        "#endif\n",
        "\n",
        "  b_r8_ptr_ = reinterpret_cast<float *>(malloc(params_->b_batch * params_->col_8_ * params_->deep_ * sizeof(float)));\n",
        "  if (b_r8_ptr_ == nullptr) {\n",
        "    FreeTmpBuffer();\n",
        "    return RET_MEMORY_FAILED;\n",
        "  }\n",
        "  memset(b_r8_ptr_, 0, params_->col_8_ * params_->deep_ * sizeof(float));\n",
        "\n",
        "  params_->a_const_ = (in_tensors_[0]->data_c() != nullptr);\n",
        "  params_->b_const_ = (in_tensors_[1]->data_c() != nullptr);\n",
        "  if (params_->a_const_ == true) {\n",
        "    InitMatrixA(reinterpret_cast<float *>(in_tensors_[0]->data_c()), a_c12_ptr_);\n",
        "  }\n",
        "  if (params_->b_const_ == true) {\n",
        "    InitMatrixB(reinterpret_cast<float *>(in_tensors_[1]->data_c()), b_r8_ptr_);\n",
        "  }\n",
        "\n",
        "  bias_ptr_ = reinterpret_cast<float *>(malloc(params_->col_8_ * sizeof(float)));\n",
        "  if (bias_ptr_ == nullptr) {\n",
        "    FreeTmpBuffer();\n",
        "    return RET_MEMORY_FAILED;\n",
        "  }\n",
        "  memset(bias_ptr_, 0, params_->col_8_ * sizeof(float));\n",
        "  if (in_tensors_.size() == 3) {\n",
        "    memcpy(bias_ptr_, in_tensors_[2]->data_c(), params_->col_ * sizeof(float));\n",
        "  }\n",
        "\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "void MatmulCPUKernel::InitMatrixA(float *src_ptr, float *dst_ptr) {\n",
        "  for (int i = 0; i < params_->a_batch; i++) {\n",
        "    float *src = src_ptr + i * params_->deep_ * params_->row_;\n",
        "#ifdef ENABLE_ARM32\n",
        "    float *dst = dst_ptr + i * params_->deep_ * params_->row_4_;\n",
        "    if (params_->a_transpose_) {\n",
        "      RowMajor2Row4Major(src, dst, params_->deep_, params_->row_);\n",
        "    } else {\n",
        "      RowMajor2Col4Major(src, dst, params_->row_, params_->deep_);\n",
        "    }\n",
        "#else\n",
        "    float *dst = dst_ptr + i * params_->deep_ * params_->row_12_;\n",
        "    if (params_->a_transpose_) {\n",
        "      RowMajor2Row12Major(src, dst, params_->deep_, params_->row_);\n",
        "    } else {\n",
        "      RowMajor2Col12Major(src, dst, params_->row_, params_->deep_);\n",
        "    }\n",
        "#endif\n",
        "  }\n",
        "  return;\n",
        "}\n",
        "\n",
        "void MatmulCPUKernel::InitMatrixB(float *src_ptr, float *dst_ptr) {\n",
        "  for (int i = 0; i < params_->b_batch; i++) {\n",
        "    float *src = src_ptr + i * params_->deep_ * params_->col_;\n",
        "    float *dst = dst_ptr + i * params_->deep_ * params_->col_8_;\n",
        "    if (params_->b_transpose_) {\n",
        "      RowMajor2Col8Major(src, dst, params_->col_, params_->deep_);\n",
        "    } else {\n",
        "      RowMajor2Row8Major(src, dst, params_->deep_, params_->col_);\n",
        "    }\n",
        "  }\n",
        "  return;\n",
        "}\n",
        "\n",
        "int MatmulCPUKernel::Init() {\n",
        "  if (!InferShapeDone()) {\n",
        "    return RET_OK;\n",
        "  }\n",
        "  return ReSize();\n",
        "}\n",
        "\n",
        "int MatmulCPUKernel::RunImpl(int task_id) {\n",
        "  int cur_oc = MSMIN(thread_stride_ * C8NUM, params_->col_ - task_id * thread_stride_ * C8NUM);\n",
        "  if (cur_oc <= 0) {\n",
        "    return RET_OK;\n",
        "  }\n",
        "  MatMulOpt(a_ptr_, b_ptr_ + task_id * thread_stride_ * C8NUM * params_->deep_,\n",
        "            c_ptr_ + task_id * thread_stride_ * C8NUM, bias_ptr_ + task_id * thread_stride_ * C8NUM, ActType_No,\n",
        "            params_->deep_, params_->row_, cur_oc, params_->col_, OutType_Nhwc);\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "int MatmulFloatRun(void *cdata, int task_id) {\n",
        "  auto op = reinterpret_cast<MatmulCPUKernel *>(cdata);\n",
        "  auto error_code = op->RunImpl(task_id);\n",
        "  if (error_code != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"MatmulFp32Run error task_id[\" << task_id << \"] error_code[\" << error_code << \"]\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "int MatmulCPUKernel::Run() {\n",
        "  auto prepare_ret = Prepare();\n",
        "  if (prepare_ret != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"Prepare fail!ret: \" << prepare_ret;\n",
        "    return prepare_ret;\n",
        "  }\n",
        "  auto a_src = reinterpret_cast<float *>(in_tensors_[0]->data_c());\n",
        "  auto b_src = reinterpret_cast<float *>(in_tensors_[1]->data_c());\n",
        "  auto c_src = reinterpret_cast<float *>(out_tensors_[0]->data_c());\n",
        "  auto a_shape = in_tensors_[0]->shape();\n",
        "  auto b_shape = in_tensors_[1]->shape();\n",
        "\n",
        "  if (params_->a_const_ == false || is_train()) {\n",
        "    InitMatrixA(a_src, a_c12_ptr_);\n",
        "  }\n",
        "  if (params_->b_const_ == false || is_train()) {\n",
        "    InitMatrixB(b_src, b_r8_ptr_);\n",
        "  }\n",
        "  if (!params_->a_broadcast_ && !params_->b_broadcast_) {\n",
        "    for (int i = 0; i < params_->a_batch; ++i) {\n",
        "      a_ptr_ = a_c12_ptr_ + i * params_->row_12_ * params_->deep_;\n",
        "      b_ptr_ = b_r8_ptr_ + i * params_->deep_ * params_->col_8_;\n",
        "      c_ptr_ = c_src + i * params_->row_ * params_->col_;\n",
        "      ParallelLaunch(this->context_->thread_pool_, MatmulFloatRun, this, thread_count_);\n",
        "    }\n",
        "    return RET_OK;\n",
        "  } else if (params_->a_broadcast_) {\n",
        "    for (int i = 0; i < params_->a_batch; i++) {\n",
        "      a_ptr_ = a_c12_ptr_ + i * params_->row_12_ * params_->deep_;\n",
        "      for (int j = 0; j < params_->b_batch; j++) {\n",
        "        b_ptr_ = b_r8_ptr_ + j * params_->deep_ * params_->col_8_;\n",
        "        c_ptr_ = c_src + (i * params_->a_batch + j) * params_->row_ * params_->col_;\n",
        "        ParallelLaunch(this->context_->thread_pool_, MatmulFloatRun, this, thread_count_);\n",
        "      }\n",
        "    }\n",
        "    return RET_OK;\n",
        "  } else if (params_->b_broadcast_) {\n",
        "    for (int i = 0; i < params_->b_batch; i++) {\n",
        "      b_ptr_ = b_r8_ptr_ + i * params_->deep_ * params_->col_8_;\n",
        "      for (int j = 0; j < params_->a_batch; j++) {\n",
        "        a_ptr_ = a_c12_ptr_ + j * params_->row_12_ * params_->deep_;\n",
        "        c_ptr_ = c_src + (i * params_->a_batch + j) * params_->row_ * params_->col_;\n",
        "        ParallelLaunch(this->context_->thread_pool_, MatmulFloatRun, this, thread_count_);\n",
        "      }\n",
        "    }\n",
        "    return RET_OK;\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"Matmul op input shape error ,cannot broadcast\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "}\n",
        "\n",
        "void MatmulCPUKernel::eval() {\n",
        "  // Copy weights after training\n",
        "  LiteKernel::eval();\n",
        "  if (params_->a_const_ == true) {\n",
        "    InitMatrixA(reinterpret_cast<float *>(in_tensors_[0]->MutableData()), a_c12_ptr_);\n",
        "  }\n",
        "  if (params_->b_const_ == true) {\n",
        "    InitMatrixB(reinterpret_cast<float *>(in_tensors_[1]->MutableData()), b_r8_ptr_);\n",
        "  }\n",
        "}\n",
        "\n",
        "}  // namespace mindspore::kernel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_hh1PrC1-r9"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_LITE_NNACL_MATMUL_H_\n",
        "#define MINDSPORE_LITE_NNACL_MATMUL_H_\n",
        "\n",
        "#include \"nnacl/op_base.h\"\n",
        "\n",
        "typedef void (*MATMUL_OPT_R4_FUNC)(const int8_t *a, const int8_t *b, int *dst, int row_4, int col_4, int deep_16,\n",
        "                                   const int *input_sum, const int *bias);\n",
        "\n",
        "typedef void (*MATMUL_OPT_R_FUNC)(const int8_t *a, const int8_t *b, int8_t *dst, size_t row, size_t col, size_t deep_4,\n",
        "                                  size_t stride, const int32_t *input_sum, const int32_t *bias, int32_t *left_shift,\n",
        "                                  int32_t *right_shift, int32_t *multiplier, int32_t output_zp, int32_t mini,\n",
        "                                  int32_t maxi, size_t per_channel);\n",
        "\n",
        "typedef enum OutType { OutType_C8 = 0, OutType_Nhwc = 1, OutType_TileC8 = 2 } OutType;\n",
        "\n",
        "typedef struct MatMulParameter {\n",
        "  OpParameter op_parameter_;\n",
        "  int row_;\n",
        "  int col_;\n",
        "  int row_4_;\n",
        "  int row_8_;\n",
        "  int row_12_;\n",
        "  int row_16_;\n",
        "  int col_2_;\n",
        "  int col_4_;\n",
        "  int col_8_;\n",
        "  int deep_;\n",
        "  int deep_4_;\n",
        "  int deep_16_;\n",
        "  bool has_bias_;\n",
        "  int batch;\n",
        "  int a_batch;\n",
        "  int b_batch;\n",
        "  bool a_transpose_; /* false :  row-major  */\n",
        "  bool b_transpose_; /* true  :  col-major  */\n",
        "  bool a_const_;\n",
        "  bool b_const_;\n",
        "  bool a_broadcast_;\n",
        "  bool b_broadcast_;\n",
        "  ActType act_type_;\n",
        "} MatMulParameter;\n",
        "\n",
        "#endif  // MINDSPORE_LITE_NNACL_MATMUL_H_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRqLIUpO3C4Y"
      },
      "source": [
        "TEST_F(TestMatMulFp32, broadcast_adims_large) {\n",
        "  std::vector<lite::Tensor *> inputs_;\n",
        "  std::vector<lite::Tensor *> outputs_;\n",
        "  auto matmul_param = new MatMulParameter();\n",
        "  matmul_param->a_transpose_ = false;\n",
        "  matmul_param->b_transpose_ = false;\n",
        "  matmul_param->has_bias_ = false;\n",
        "  float a[] = {-0.65235930681229, -0.83840399980545, -1.78766810894012,\n",
        "               0.70013177394867, 1.59338378906250, -0.27830731868744,\n",
        "               -0.21799579262733, 0.43158695101738, -0.55378085374832,\n",
        "               0.36990931630135, -1.17206823825836, -1.11876296997070,\n",
        "               -0.71741801500320, 3.30769562721252, -0.90681165456772,\n",
        "               0.91257780790329, -0.95812422037125, -1.72401988506317,\n",
        "               0.38048243522644, -0.81240177154541, 0.01727002300322,\n",
        "               -0.42814204096794, 1.09676754474640, -0.09681072831154,\n",
        "               0.45356941223145, -1.34337413311005, 1.40391993522644,\n",
        "               0.89548885822296, -0.21618857979774, 1.02820229530334,\n",
        "               1.03970003128052, -0.71132820844650, -0.70775115489960,\n",
        "               -0.72431832551956, 0.33484363555908, -0.30216658115387,\n",
        "               1.33590710163116, 0.95974528789520, 0.30546423792839,\n",
        "               0.68026691675186, 0.26281154155731, 1.30050456523895,\n",
        "               -0.27555844187737, 0.86635190248489, -0.34620115160942};\n",
        "  float b[] = {-2.10949733853340e-01, -1.75398275256157e-01, 6.04069411754608e-01,\n",
        "               9.28257405757904e-01, -4.25255388021469e-01, -6.50140106678009e-01,\n",
        "               4.82616513967514e-01, 2.19438716769218e-01, 5.27316093444824e-01,\n",
        "               5.60069322586060e-01, 1.32913506031036e+00, -1.19743621349335e+00,\n",
        "               -7.01747953891754e-01, 4.76675212383270e-01, 9.75661203265190e-02,\n",
        "               -6.11941099166870e-01, 1.29469335079193e+00, -6.02801263332367e-01,\n",
        "               -6.93235933780670e-01, 2.75707364082336e-01, 3.32885072566569e-04,\n",
        "               -8.51491987705231e-01, 1.40798020362854e+00, 6.13633811473846e-01,\n",
        "               -4.24172759056091e-01};\n",
        "  std::vector<int> a_shape = {3, 3, 5};\n",
        "  std::vector<int> b_shape = {5, 5};\n",
        "  std::vector<int> c_shape = {3, 3, 5};\n",
        "  int total_size = MMTestInit(&inputs_, &outputs_, a, b, a_shape, b_shape, c_shape);\n",
        "  auto ctx = new lite::InnerContext;\n",
        "  ctx->thread_num_ = 1;\n",
        "  ASSERT_EQ(lite::RET_OK, ctx->Init());\n",
        "  auto mm = new kernel::MatmulCPUKernel(reinterpret_cast<OpParameter *>(matmul_param), inputs_, outputs_, ctx, nullptr);\n",
        "  mm->Init();\n",
        "  mm->Run();\n",
        "  float correct[] = {-2.12126612663269, 1.40011596679688, 2.49785637855530,\n",
        "                     -1.40740084648132, -0.84939938783646, 1.11307847499847,\n",
        "                     -1.60514271259308, 0.33582586050034, 0.44332292675972,\n",
        "                     -0.27121970057487, -2.00336194038391, 5.57930183410645,\n",
        "                     -3.72071981430054, -4.86936187744141, 1.09844863414764,\n",
        "                     -2.09415149688721, 2.62629437446594, 0.17763727903366,\n",
        "                     -1.24220609664917, -0.64340001344681, 1.79185485839844,\n",
        "                     -2.03451776504517, -0.15619860589504, 0.65850496292114,\n",
        "                     -0.35920926928520, 0.69350230693817, -1.31451427936554,\n",
        "                     0.44618299603462, 0.70097935199738, 0.94919872283936,\n",
        "                     -0.25420671701431, -0.90106028318405, 1.87669420242310,\n",
        "                     0.96024185419083, -1.25131511688232, 0.28414657711983,\n",
        "                     -0.63526278734207, 0.21078896522522, 1.08711969852448,\n",
        "                     0.76600521802902, -1.79747617244720, 2.32795929908752,\n",
        "                     -0.37217232584953, -0.01464513223618, 0.97543424367905};\n",
        "  CompareOutputData(reinterpret_cast<float *>(outputs_[0]->MutableData()), correct, total_size, 0.0001);\n",
        "  delete mm;\n",
        "  for (auto t : inputs_) delete t;\n",
        "  for (auto t : outputs_) delete t;\n",
        "}\n",
        "TEST_F(TestMatMulFp32, broadcast_bdims_large) {\n",
        "  std::vector<lite::Tensor *> inputs_;\n",
        "  std::vector<lite::Tensor *> outputs_;\n",
        "  auto matmul_param = new MatMulParameter();\n",
        "  matmul_param->a_transpose_ = false;\n",
        "  matmul_param->b_transpose_ = false;\n",
        "  matmul_param->has_bias_ = false;\n",
        "  float b[] = {-0.65235930681229, -0.83840399980545, -1.78766810894012,\n",
        "               0.70013177394867, 1.59338378906250, -0.27830731868744,\n",
        "               -0.21799579262733, 0.43158695101738, -0.55378085374832,\n",
        "               0.36990931630135, -1.17206823825836, -1.11876296997070,\n",
        "               -0.71741801500320, 3.30769562721252, -0.90681165456772,\n",
        "               0.91257780790329, -0.95812422037125, -1.72401988506317,\n",
        "               0.38048243522644, -0.81240177154541, 0.01727002300322,\n",
        "               -0.42814204096794, 1.09676754474640, -0.09681072831154,\n",
        "               0.45356941223145, -1.34337413311005, 1.40391993522644,\n",
        "               0.89548885822296, -0.21618857979774, 1.02820229530334,\n",
        "               1.03970003128052, -0.71132820844650, -0.70775115489960,\n",
        "               -0.72431832551956, 0.33484363555908, -0.30216658115387,\n",
        "               1.33590710163116, 0.95974528789520, 0.30546423792839,\n",
        "               0.68026691675186, 0.26281154155731, 1.30050456523895,\n",
        "               -0.27555844187737, 0.86635190248489, -0.34620115160942};\n",
        "  float a[] = {0.60076111555099,  1.64438819885254,  1.48806667327881,\n",
        "               2.10185551643372,  0.82506781816483,  0.03017991222441,\n",
        "               -0.08507440239191,  0.55047357082367,  1.09753358364105,\n",
        "               0.74297022819519,  0.25211459398270,  1.22624528408051,\n",
        "               -0.50102031230927,  2.38538837432861, -0.02676716446877};\n",
        "  std::vector<int> a_shape = {1,5, 3};\n",
        "  std::vector<int> b_shape = {3, 3, 5};\n",
        "  std::vector<int> c_shape = {3, 5, 5};\n",
        "  int total_size = MMTestInit(&inputs_, &outputs_, a, b, a_shape, b_shape, c_shape);\n",
        "  auto ctx = new lite::InnerContext;\n",
        "  ctx->thread_num_ = 1;\n",
        "  ASSERT_EQ(lite::RET_OK, ctx->Init());\n",
        "  auto mm = new kernel::MatmulCPUKernel(reinterpret_cast<OpParameter *>(matmul_param), inputs_, outputs_, ctx, nullptr);\n",
        "  mm->Init();\n",
        "  mm->Run();\n",
        "  float correct[] = {-2.59367299079895e+00, -2.52694416046143e+00, -1.43183088302612e+00,\n",
        "                     4.43205308914185e+00,  2.16121345758438e-01, -1.63616037368774e+00,\n",
        "                     -1.97582948207855e+00, -3.42298316955566e+00,  1.11449503898621e+00,\n",
        "                     3.62689518928528e+00, -1.38408601284027e+00, -1.27655410766602e+00,\n",
        "                     -3.97728353738785e-01,  3.26590204238892e+00, -9.27187144756317e-01,\n",
        "                     -1.99209201335907e+00, -2.04974699020386e+00, -2.09910511970520e+00,\n",
        "                     4.43660688400269e+00,  1.65122762322426e-01, -3.05652856826782e-01,\n",
        "                     -7.00010731816292e-02,  1.94436383247375e+00, -1.76030027866364e+00,\n",
        "                     1.08332492411137e-01, -1.42239034175873e+00,  8.09490919113159e-01,\n",
        "                     2.10033464431763e+00, -2.52318382263184e-01,  1.78781831264496e+00,\n",
        "                     1.89181268215179e+00, -2.32471489906311e+00, -2.69170737266541e+00,\n",
        "                     7.13319122791290e-01, -1.30229461193085e+00, -1.54252851009369e+00,\n",
        "                     1.38668024539948e+00,  1.73324060440063e+00, -3.22935283184052e-01,\n",
        "                     1.44727909564972e+00, -9.64934051036835e-01,  9.01751577854156e-01,\n",
        "                     9.37045887112617e-02, -6.82048546150327e-03,  7.71589398384094e-01,\n",
        "                     -3.80065977573395e-01, -5.78824281692505e-01,  3.45601582527161e+00,\n",
        "                     -4.15773868560791e-01,  1.46144688129425e+00,  5.18813312053680e-01,\n",
        "                     3.70464897155762e+00,  7.42955088615417e-01,  1.35634887218475e+00,\n",
        "                     8.04613530635834e-01,  1.94392287731171e+00, -3.53646010160446e-01,\n",
        "                     -7.04052031040192e-01, -1.24423730373383e+00,  1.25461089611053e+00,\n",
        "                     3.36579121649265e-02,  2.22324490547180e+00,  2.86091297864914e-01,\n",
        "                     1.18062126636505e+00, -3.39850485324860e-02,  1.01855707168579e+00,\n",
        "                     1.40304362773895e+00, -6.21774435043335e-01,  6.01224958896637e-01,\n",
        "                     -4.24346700310707e-03, -1.24873030185699e+00,  3.50823640823364e+00,\n",
        "                     2.65133881568909e+00,  1.06835925579071e+00,  1.46420419216156e+00};\n",
        "  CompareOutputData(reinterpret_cast<float *>(outputs_[0]->MutableData()), correct, total_size, 0.0001);\n",
        "  delete mm;\n",
        "  for (auto t : inputs_) delete t;\n",
        "  for (auto t : outputs_) delete t;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}