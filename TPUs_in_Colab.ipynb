{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1C06NDZSB_c"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import com.mindspore.lite.MSTensor;\n",
        "import com.mindspore.lite.TrainSession;\n",
        "import com.mindspore.lite.config.MSConfig;\n",
        "import mindspore.schema.FeatureMap;\n",
        "\n",
        "import java.nio.ByteBuffer;\n",
        "import java.nio.ByteOrder;\n",
        "import java.util.ArrayList;\n",
        "import java.util.HashMap;\n",
        "import java.util.List;\n",
        "import java.util.Map;\n",
        "\n",
        "public class SessionUtil {\n",
        "    public static Map<String,float[]> convertTensorTofeatures(List<MSTensor> tensors) {\n",
        "        Map<String,float[]> features = new HashMap<>(tensors.size());\n",
        "        for(MSTensor mstensor:tensors) {\n",
        "            features.put(mstensor.tensorName(),mstensor.getFloatData());\n",
        "        }\n",
        "        return features;\n",
        "    }\n",
        "    public static List<MSTensor> getFeatures(TrainSession trainSession) {\n",
        "        List<MSTensor> featuresMap= trainSession.getFeaturesMap();\n",
        "        for(int i=0;i<5;i++){\n",
        "            MSTensor feature = featuresMap.get(i);\n",
        "            float[] data= feature.getFloatData();\n",
        "            String name = feature.tensorName();\n",
        "            int elements = feature.elementsNum();\n",
        "            System.out.println(\"tensorname:\"+name+\",len,\"+elements+\",\"+data[0]);\n",
        "        }\n",
        "        return featuresMap;\n",
        "    }\n",
        "    public static int updateFeatures(TrainSession trainSession, String modelName, List<FeatureMap> featureMaps) {\n",
        "        List<MSTensor> tensors = new ArrayList<>(featureMaps.size());\n",
        "        for (FeatureMap newFeature:featureMaps) {\n",
        "           ByteBuffer by = newFeature.dataAsByteBuffer();\n",
        "           ByteBuffer newData = ByteBuffer.allocateDirect(by.remaining());\n",
        "           newData.order(ByteOrder.nativeOrder());\n",
        "           newData.put(by);\n",
        "           tensors.add(new MSTensor(newFeature.weightFullname(),newData));\n",
        "        }\n",
        "        trainSession.updateFeatures(modelName,tensors);\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "    public static TrainSession initSession(String modelPath) {\n",
        "        MSConfig msConfig = new MSConfig();\n",
        "        // arg 0: DeviceType:DT_CPU -> 0\n",
        "        // arg 1: ThreadNum -> 2\n",
        "        // arg 2: cpuBindMode:NO_BIND ->  0\n",
        "        // arg 3: enable_fp16 -> false\n",
        "        msConfig.init(0, 1, 0, false);\n",
        "        TrainSession trainSession = new TrainSession();\n",
        "        boolean status = trainSession.init(modelPath, msConfig);\n",
        "        if(!status) {\n",
        "            System.out.println(\"init session failed,\"+modelPath);\n",
        "            return null;\n",
        "        }\n",
        "        trainSession.setLearningRate(0.01f);\n",
        "        return trainSession;\n",
        "    }\n",
        "    public static  MSTensor searchOutputsForSize(TrainSession trainSession,int size) {\n",
        "        Map<String, MSTensor> outputs = trainSession.getOutputMapByTensor();\n",
        "        for (MSTensor tensor : outputs.values()) {\n",
        "            if (tensor.elementsNum() == size) {\n",
        "                return tensor;\n",
        "            }\n",
        "        }\n",
        "        System.err.println(\"can not find output the tensor which element num is \" + size);\n",
        "        return null;\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hdRcKFSzmm"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import com.mindspore.lite.MSTensor;\n",
        "import com.mindspore.lite.TrainSession;\n",
        "\n",
        "import java.io.*;\n",
        "import java.nio.ByteBuffer;\n",
        "import java.nio.ByteOrder;\n",
        "import java.util.List;\n",
        "\n",
        "public class AdTrainBert {\n",
        "    private int batchSize;\n",
        "    private int batchNum;\n",
        "    private int dataSize;\n",
        "    private TrainSession trainSession;\n",
        "    private List<Feature> features;\n",
        "    private ByteBuffer inputIdBufffer;\n",
        "    private ByteBuffer tokenIdBufffer;\n",
        "    private ByteBuffer maskIdBufffer;\n",
        "    private ByteBuffer labelIdBufffer;\n",
        "\n",
        "    static {\n",
        "        System.loadLibrary(\"mindspore-lite-jni\");\n",
        "    }\n",
        "\n",
        "    private int initSessionAndInputs(String modelPath) {\n",
        "        trainSession = SessionUtil.initSession(modelPath);\n",
        "        if(trainSession == null) {\n",
        "            return -1;\n",
        "        }\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "        MSTensor labelIdTensor = inputs.get(0);\n",
        "        int inputLength = labelIdTensor.elementsNum(); // labelId,tokenId,inputId,maskId has same size\n",
        "        if(batchSize <= 0) {\n",
        "            System.out.println(\"batch size need more than 0\");\n",
        "            return -1;\n",
        "        }\n",
        "        dataSize = inputLength / batchSize;\n",
        "        inputIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        tokenIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        maskIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        labelIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        inputIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        tokenIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        maskIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        labelIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        return 0;\n",
        "    }\n",
        "    private void fillAdTrainBertInput(int batchIdx) {\n",
        "        inputIdBufffer.clear();\n",
        "        tokenIdBufffer.clear();\n",
        "        maskIdBufffer.clear();\n",
        "        labelIdBufffer.clear();\n",
        "        for (int i = 0; i < batchSize; i++) {\n",
        "            Feature feature = features.get(batchIdx * batchSize + i);\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                inputIdBufffer.putInt(feature.inputIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                tokenIdBufffer.putInt(feature.tokenIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                maskIdBufffer.putInt(feature.inputMasks[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                labelIdBufffer.putInt(feature.inputIds[j]);\n",
        "            }\n",
        "        }\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "        MSTensor labelIdTensor = inputs.get(0);\n",
        "        MSTensor tokenIdTensor = inputs.get(1);\n",
        "        MSTensor inputIdTensor = inputs.get(2);\n",
        "        MSTensor maskIdTensor = inputs.get(3);\n",
        "        labelIdTensor.setData(labelIdBufffer);\n",
        "        tokenIdTensor.setData(tokenIdBufffer);\n",
        "        inputIdTensor.setData(inputIdBufffer);\n",
        "        maskIdTensor.setData(maskIdBufffer);\n",
        "    }\n",
        "\n",
        "    private float getLoss() {\n",
        "        MSTensor tensor = SessionUtil.searchOutputsForSize(trainSession,1);\n",
        "        if(tensor == null) {\n",
        "            System.err.println(\"cannot find loss tensor\");\n",
        "            return Float.NaN;\n",
        "        }\n",
        "        return tensor.getFloatData()[0];\n",
        "    }\n",
        "    private int trainLoop(int epoches) {\n",
        "        trainSession.train();\n",
        "        long startTime=System.currentTimeMillis();\n",
        "        for (int i = 0; i < epoches; i++) {\n",
        "            float sumLossPerEpoch = 0.0f;\n",
        "            for (int j = 0; j < batchNum; j++) {\n",
        "                fillAdTrainBertInput(j);\n",
        "                trainSession.runGraph();\n",
        "                float loss = getLoss();\n",
        "                if(Float.isNaN(loss)) {\n",
        "                    System.out.println(\"loss is nan\");\n",
        "                    return -1;\n",
        "                }\n",
        "                sumLossPerEpoch += loss;\n",
        "                System.out.println(\"------batch:\"+j+\",loss:\"+loss+\"-----------\");\n",
        "            }\n",
        "            System.out.println(\"----------epoch:\" + i + \",mean loss:\" + sumLossPerEpoch / batchNum + \"----------\");\n",
        "            long endTime=System.currentTimeMillis();\n",
        "            System.out.println(\"total train time \"+(endTime-startTime)+\"ms\");\n",
        "        }\n",
        "        return 0;\n",
        "    }\n",
        "    public TrainSession getSession(){\n",
        "        return trainSession;\n",
        "    }\n",
        "\n",
        "    public int initDataSet(String dataFile, String vocabFile,String idsFile, int batchSize) {\n",
        "        System.out.println(\"==========Init dataFile,\"+dataFile+ \",vocabFile,\"+vocabFile+\"=============\");\n",
        "        if(batchSize <= 0) {\n",
        "            System.out.println(\"batch size need more than 0\");\n",
        "            return -1;\n",
        "        }\n",
        "        features = DataSet.init(dataFile, vocabFile,idsFile,true);\n",
        "        this.batchSize = batchSize;\n",
        "        batchNum = features.size() / batchSize;\n",
        "        return features.size();\n",
        "    }\n",
        "\n",
        "    public int trainModel(String modelPath,int epoches) {\n",
        "        System.out.println(\"==========Loading Model,\"+modelPath+\" Create Train Session=============\");\n",
        "        int status = initSessionAndInputs(modelPath);\n",
        "        if(status== -1) {\n",
        "            System.out.println(\"init session and inputs failed\");\n",
        "            return -1;\n",
        "        }\n",
        "        System.out.println(\"==========Begin Train Model=============\");\n",
        "        status = trainLoop(epoches);\n",
        "        if(status== -1) {\n",
        "            System.out.println(\"train loop failed\");\n",
        "            return -1;\n",
        "        }\n",
        "        if (epoches > 0) {\n",
        "            trainSession.saveToFile(modelPath);\n",
        "        }\n",
        "\n",
        "        return 0;\n",
        "    }\n",
        "    public void free() {\n",
        "        trainSession.free();\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) throws IOException {\n",
        "        AdTrainBert adTrainBert = new AdTrainBert();\n",
        "        String dataFile = \"/home/meng/zj10/mindspore/mindspore/lite/101.txt\";\n",
        "        String vocabFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/flclient/src/main/native/dataset/vocab.txt\";\n",
        "        String idsFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/vocab_map_ids.txt\";\n",
        "        String modelPath = \"/home/meng/zj10/fl/mindspore/mindspore/lite/albert_ad_train_new.mindir.ms\";\n",
        "        int epoches = 1;\n",
        "        int batchSize = 16;\n",
        "        adTrainBert.initDataSet(dataFile,vocabFile,idsFile,batchSize);\n",
        "        adTrainBert.trainModel(modelPath,epoches);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoM3NNAtS1Nq"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import com.mindspore.lite.MSTensor;\n",
        "import com.mindspore.lite.TrainSession;\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.nio.ByteBuffer;\n",
        "import java.nio.ByteOrder;\n",
        "import java.util.ArrayList;\n",
        "import java.util.List;\n",
        "\n",
        "public class AdInferBert {\n",
        "    private int batchSize;\n",
        "    private int batchNum;\n",
        "    private int dataSize;\n",
        "    private TrainSession trainSession;\n",
        "    private List<Feature> features;\n",
        "    private ByteBuffer inputIdBufffer;\n",
        "    private ByteBuffer tokenIdBufffer;\n",
        "    private ByteBuffer maskIdBufffer;\n",
        "\n",
        "    static {\n",
        "        System.loadLibrary(\"mindspore-lite-jni\");\n",
        "    }\n",
        "    private int initDataSet(String exampleFile, String vocabFile, String idsFile,int batchSize) {\n",
        "        if(batchSize <= 0) {\n",
        "            System.err.println(\"batch size need more than 0\");\n",
        "            return -1;\n",
        "        }\n",
        "        features = DataSet.init(exampleFile, vocabFile,idsFile,false);\n",
        "        this.batchSize = batchSize;\n",
        "        batchNum = features.size() / batchSize;\n",
        "        return features.size();\n",
        "    }\n",
        "    private int initSessionAndInputs(String modelPath) {\n",
        "        trainSession = SessionUtil.initSession(modelPath);\n",
        "        if(trainSession == null) {\n",
        "            return -1;\n",
        "        }\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "        MSTensor tokenIdTensor = inputs.get(0);\n",
        "        int inputLength = tokenIdTensor.elementsNum(); // labelId,tokenId,inputId,maskId has same size\n",
        "        if(batchSize <= 0) {\n",
        "            System.out.println(\"batch size need more than 0\");\n",
        "            return -1;\n",
        "        }\n",
        "        dataSize = inputLength / batchSize;\n",
        "        inputIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        tokenIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        maskIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        inputIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        tokenIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        maskIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        return 0;\n",
        "    }\n",
        "    private List<Integer> fillAdInferBertInput(int batchIdx) {\n",
        "        inputIdBufffer.clear();\n",
        "        tokenIdBufffer.clear();\n",
        "        maskIdBufffer.clear();\n",
        "        List<Integer> labels = new ArrayList<>(batchSize);\n",
        "        for (int i = 0; i < batchSize; i++) {\n",
        "            Feature feature = features.get(batchIdx * batchSize + i);\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                inputIdBufffer.putInt(feature.inputIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                tokenIdBufffer.putInt(feature.tokenIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                maskIdBufffer.putInt(feature.inputMasks[j]);\n",
        "            }\n",
        "            labels.add(feature.labelIds);\n",
        "        }\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "\n",
        "        MSTensor tokenIdTensor = inputs.get(0);\n",
        "        MSTensor inputIdTensor = inputs.get(1);\n",
        "        MSTensor maskIdTensor = inputs.get(2);\n",
        "        tokenIdTensor.setData(tokenIdBufffer);\n",
        "        inputIdTensor.setData(inputIdBufffer);\n",
        "        maskIdTensor.setData(maskIdBufffer);\n",
        "        return labels;\n",
        "    }\n",
        "\n",
        "    private float calculateAccracy( List<Integer> labels) {\n",
        "        int numOfClass = 5;\n",
        "        MSTensor outputTensor = SessionUtil.searchOutputsForSize(trainSession,batchSize* numOfClass);\n",
        "        if(outputTensor == null) {\n",
        "            return Float.NaN;\n",
        "        }\n",
        "        float[] scores = outputTensor.getFloatData();\n",
        "        float accuracy = 0.0f;\n",
        "        for(int b=0;b<batchSize;b++) {\n",
        "            int maxIdx = 0;\n",
        "            float maxScore = scores[numOfClass *b];\n",
        "            for(int c = 0; c< numOfClass; c++) {\n",
        "                if(scores[numOfClass *b+c] >maxScore) {\n",
        "                    maxScore = scores[numOfClass *b+c];\n",
        "                    maxIdx = c;\n",
        "                }\n",
        "            }\n",
        "            if(labels.get(b) == maxIdx) {\n",
        "                accuracy+=1;\n",
        "            }\n",
        "        }\n",
        "        return accuracy/batchSize;\n",
        "    }\n",
        "    private float infer() {\n",
        "        trainSession.eval();\n",
        "        float totalAccuracy = 0.0f;\n",
        "        for (int j = 0; j < batchNum; j++) {\n",
        "            List<Integer> labels= fillAdInferBertInput(j);\n",
        "            trainSession.runGraph();\n",
        "            float curAcc = calculateAccracy(labels);\n",
        "            if(Float.isNaN(curAcc)) {\n",
        "                return Float.NaN;\n",
        "            }\n",
        "            totalAccuracy += curAcc;\n",
        "            System.out.println(\"batch num:\"+j+\",acc is:\"+curAcc);\n",
        "        }\n",
        "        System.out.println(\"total acc:\"+totalAccuracy/batchNum);\n",
        "        return totalAccuracy/batchNum;\n",
        "    }\n",
        "\n",
        "    public TrainSession getSession(){\n",
        "        return trainSession;\n",
        "    }\n",
        "\n",
        "    public float inferModel(String modelPath,String dataFile,String vocabFile,String idsFile,int batchSize)  {\n",
        "        System.out.println(\"==========Init dataFile,\"+dataFile+ \",vocabFile,\"+vocabFile+\"=============\");\n",
        "        int inferSize = initDataSet(dataFile,vocabFile,idsFile,batchSize);\n",
        "        if(inferSize == -1) {\n",
        "            System.out.println(\"init dataset failed\");\n",
        "            return -1;\n",
        "        }\n",
        "        System.out.println(\"==========Train size,\"+inferSize);\n",
        "        System.out.println(\"==========Loading Model,\"+modelPath+\" Create Train Session=============\");\n",
        "        initSessionAndInputs(modelPath);\n",
        "        System.out.println(\"==========Begin Infer Model=============\");\n",
        "        return infer();\n",
        "    }\n",
        "\n",
        "    public void free() {\n",
        "        trainSession.free();\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) throws IOException {\n",
        "        AdInferBert adInferBert = new AdInferBert();\n",
        "        String dataFile = \"/home/meng/zj10/mindspore/mindspore/lite/eval.txt\";\n",
        "        String vocabFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/flclient/src/main/native/dataset/vocab.txt\";\n",
        "        String idsFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/vocab_map_ids.txt\";\n",
        "        String modelPath = \"/home/meng/zj10/fl/mindspore/mindspore/lite/albert_ad_infer_new.mindir.ms\";\n",
        "        int batchSize =16;\n",
        "        adInferBert.inferModel(modelPath,dataFile,vocabFile,idsFile,batchSize);\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCSj4j9tS4mT"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.nio.charset.StandardCharsets;\n",
        "import java.nio.file.Files;\n",
        "import java.nio.file.Path;\n",
        "import java.nio.file.Paths;\n",
        "import java.util.*;\n",
        "\n",
        "class Feature {\n",
        "    int[] inputIds;\n",
        "    int[] inputMasks;\n",
        "    int[] tokenIds;\n",
        "    int labelIds;\n",
        "    int seqLen;\n",
        "\n",
        "    public Feature(int[] inputIds, int[] inputMasks, int[] tokenIds, int labelIds, int seqLen) {\n",
        "        this.inputIds = inputIds;\n",
        "        this.inputMasks = inputMasks;\n",
        "        this.tokenIds = tokenIds;\n",
        "        this.labelIds = labelIds;\n",
        "        this.seqLen = seqLen;\n",
        "    }\n",
        "}\n",
        "\n",
        "public class CustomTokenizer {\n",
        "    private Map<String, Integer> vocabs = new HashMap<>();\n",
        "    private Boolean doLowerCase = Boolean.TRUE;\n",
        "    private int maxInputChars = 100;\n",
        "    private String[] NotSplitStrs = {\"UNK\"};\n",
        "    private String unkToken = \"[UNK]\";\n",
        "    private int maxSeqLen = 16;\n",
        "    private int vocabSize = 11682;\n",
        "    private Map<String, Integer> labelMap = new HashMap<String, Integer>() {{\n",
        "        put(\"beauty\", 0);\n",
        "        put(\"education\", 1);\n",
        "        put(\"hotel\", 2);\n",
        "        put(\"travel\", 3);\n",
        "        put(\"other\", 4);\n",
        "    }};\n",
        "\n",
        "\n",
        "    public void init(String vocabFile,String idsFile ,boolean trainMod,boolean doLowerCase) {\n",
        "        this.doLowerCase = doLowerCase;\n",
        "        Path vocabPath = Paths.get(vocabFile);\n",
        "        List<String> vocabLines = null;\n",
        "        try {\n",
        "            vocabLines = Files.readAllLines(vocabPath, StandardCharsets.UTF_8);\n",
        "        } catch (IOException e) {\n",
        "            e.printStackTrace();\n",
        "        }\n",
        "        Path idsPath = Paths.get(idsFile);\n",
        "        List<String> idsLines = null;\n",
        "        try {\n",
        "            idsLines = Files.readAllLines(idsPath, StandardCharsets.UTF_8);\n",
        "        } catch (IOException e) {\n",
        "            e.printStackTrace();\n",
        "        }\n",
        "        for (int i=0;i<idsLines.size();++i) {\n",
        "            vocabs.put(vocabLines.get(i), Integer.parseInt(idsLines.get(i)));\n",
        "        }\n",
        "        if(!trainMod) {\n",
        "            maxSeqLen = 256;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // is chinses or punctuation\n",
        "    public Boolean isChineseOrPunc(char trimChar) {\n",
        "        // is chinese char\n",
        "        if (trimChar >= '\\u4e00' && trimChar <= '\\u9fa5') {\n",
        "            return true;\n",
        "        }\n",
        "        // is puncuation char\n",
        "        if ((trimChar >= 33 && trimChar <= 47) || (trimChar >= 58 && trimChar <= 64) || (trimChar >= 91 && trimChar <= 96) || (trimChar >= 123 && trimChar <= 126)) {\n",
        "            return true;\n",
        "        }\n",
        "        return false;\n",
        "    }\n",
        "\n",
        "    public String[] splitText(String text) {\n",
        "        // clean remove white and control char\n",
        "        String trimText = text.trim();\n",
        "        StringBuilder cleanText = new StringBuilder();\n",
        "        for (int i = 0; i < trimText.length(); i++) {\n",
        "            if (isChineseOrPunc(trimText.charAt(i))) {\n",
        "                cleanText.append(\" \" + trimText.charAt(i) + \" \");\n",
        "            } else {\n",
        "                cleanText.append(trimText.charAt(i));\n",
        "            }\n",
        "        }\n",
        "        return cleanText.toString().trim().split(\"\\\\s+\");\n",
        "    }\n",
        "\n",
        "    //   input = \"unaffable\" , output = [\"un\", \"##aff\", \"##able\"]\n",
        "    public List<String> wordPieceTokenize(String[] tokens) {\n",
        "        List<String> outputTokens = new ArrayList<>();\n",
        "        for (String token : tokens) {\n",
        "            List<String> subTokens = new ArrayList<>();\n",
        "            boolean isBad = false;\n",
        "            int start = 0;\n",
        "            while (start < token.length()) {\n",
        "                int end = token.length();\n",
        "                String curStr = \"\";\n",
        "                while (start < end) {\n",
        "                    String subStr = token.substring(start, end);\n",
        "                    if (start > 0) {\n",
        "                        subStr = \"##\" + subStr;\n",
        "                    }\n",
        "                    if (vocabs.get(subStr) != null) {\n",
        "                        curStr = subStr;\n",
        "                        break;\n",
        "                    }\n",
        "                    end = end - 1;\n",
        "                }\n",
        "                if (curStr.isEmpty()) {\n",
        "                    isBad = true;\n",
        "                    break;\n",
        "                }\n",
        "                subTokens.add(curStr);\n",
        "                start = end;\n",
        "            }\n",
        "            if (isBad) {\n",
        "                outputTokens.add(unkToken);\n",
        "            } else {\n",
        "                outputTokens.addAll(subTokens);\n",
        "            }\n",
        "        }\n",
        "        return outputTokens;\n",
        "\n",
        "    }\n",
        "\n",
        "    public List<Integer> convertTokensToIds(List<String> tokens,boolean cycTrunc) {\n",
        "        int seqLen = tokens.size();\n",
        "        if (tokens.size() > maxSeqLen - 2) {\n",
        "            if(cycTrunc) {\n",
        "               int randIndex = (int) (Math.random() * seqLen);\n",
        "               if(randIndex>seqLen-maxSeqLen+2) {\n",
        "                   List<String> rearPart = tokens.subList(randIndex,seqLen);\n",
        "                   List<String> frontPart = tokens.subList(0,randIndex+maxSeqLen-2-seqLen);\n",
        "                   rearPart.addAll(frontPart);\n",
        "                   tokens = rearPart;\n",
        "               } else {\n",
        "                   tokens = tokens.subList(randIndex,randIndex+maxSeqLen-2);\n",
        "               }\n",
        "            } else {\n",
        "                tokens = tokens.subList(0, maxSeqLen - 2);\n",
        "            }\n",
        "        }\n",
        "        tokens.add(0, \"[CLS]\");\n",
        "        tokens.add(\"[SEP]\");\n",
        "        List<Integer> ids = new ArrayList<>(tokens.size());\n",
        "        for (String token : tokens) {\n",
        "            ids.add(vocabs.getOrDefault(token, vocabs.get(\"[UNK]\")));\n",
        "        }\n",
        "        return ids;\n",
        "    }\n",
        "\n",
        "    public void addRandomMaskAndReplace(Feature feature, boolean keepFirstUnchange, boolean keepLastUnchange) {\n",
        "        int[] masks = new int[maxSeqLen];\n",
        "        Arrays.fill(masks, 1);\n",
        "        int[] replaces = new int[maxSeqLen];\n",
        "        Arrays.fill(replaces, 1);\n",
        "        int[] inputIds = feature.inputIds;\n",
        "        for (int i = 0; i < feature.seqLen; i++) {\n",
        "            double rand1 = Math.random();\n",
        "            if (rand1 < 0.15) {\n",
        "                masks[i] = 0;\n",
        "                double rand2 = Math.random();\n",
        "                if (rand2 < 0.8) {\n",
        "                    replaces[i] = 103;\n",
        "                } else if (rand2 < 0.9) {\n",
        "                    masks[i] = 1;\n",
        "                } else {\n",
        "                    replaces[i] = (int) (Math.random() * vocabSize);\n",
        "                }\n",
        "            }\n",
        "            if(keepFirstUnchange) {\n",
        "                masks[i] = 1;\n",
        "                replaces[i] = 0;\n",
        "            }\n",
        "            if(keepLastUnchange) {\n",
        "                masks[feature.seqLen-1] = 1;\n",
        "                replaces[feature.seqLen-1] =0;\n",
        "            }\n",
        "            inputIds[i] = inputIds[i] * masks[i] + replaces[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    public Feature getFeatures(List<Integer> tokens, String label) {\n",
        "        int[] segmentIds = new int[maxSeqLen];\n",
        "        Arrays.fill(segmentIds, 0);\n",
        "        int[] masks = new int[maxSeqLen];\n",
        "        Arrays.fill(masks, 0);\n",
        "        Arrays.fill(masks, 0, tokens.size(), 1); // tokens size can ensure less than masks\n",
        "        int[] inputIds = new int[maxSeqLen];\n",
        "        Arrays.fill(inputIds, 0);\n",
        "        for (int i = 0; i < tokens.size(); i++) {\n",
        "            inputIds[i] = tokens.get(i);\n",
        "        }\n",
        "        return new Feature(inputIds, masks, segmentIds, labelMap.get(label), tokens.size());\n",
        "    }\n",
        "\n",
        "    public List<Integer> tokenize(String text,boolean trainMod) {\n",
        "        String[] splitTokens = splitText(text);\n",
        "        List<String> wordPieceTokens = wordPieceTokenize(splitTokens);\n",
        "        return convertTokensToIds(wordPieceTokens,trainMod); // trainMod need cyclicTrunc\n",
        "\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) throws IOException {\n",
        "        String test = \"\\u9EC4abc\\u5927\";\n",
        "        CustomTokenizer customTokenizer = new CustomTokenizer();\n",
        "        String line = \"<<<other>>>unaffable\";\n",
        "        String[] tokens = line.split(\">>>\");\n",
        "        if (tokens.length != 2) {\n",
        "            System.out.println(\"Input line ERROR\");\n",
        "        }\n",
        "\n",
        "\n",
        "        String vocabFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/flclient/src/main/native/dataset/vocab.txt\";\n",
        "        String idsFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/vocab_map_ids.txt\";\n",
        "        customTokenizer.init(vocabFile, idsFile,true,true);\n",
        "        customTokenizer.tokenize(tokens[1],true);\n",
        "        tokens = tokens[0].split(\"<<<\");\n",
        "        System.out.println(tokens[0]);\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od1hcFEtS5PK"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.nio.charset.StandardCharsets;\n",
        "import java.nio.file.Files;\n",
        "import java.nio.file.Path;\n",
        "import java.nio.file.Paths;\n",
        "import java.util.ArrayList;\n",
        "import java.util.List;\n",
        "\n",
        "public class DataSet {\n",
        "\n",
        "    public static List<Feature> init(String trainFile,String vocabFile,String idsFile,boolean trainMod) {\n",
        "        // read train file\n",
        "        CustomTokenizer customTokenizer = new CustomTokenizer();\n",
        "\n",
        "        customTokenizer.init(vocabFile,idsFile,trainMod,true);\n",
        "        Path path = Paths.get(trainFile);\n",
        "        List<String> allLines = null;\n",
        "        try {\n",
        "            allLines = Files.readAllLines(path, StandardCharsets.UTF_8);\n",
        "        } catch (IOException e) {\n",
        "            e.printStackTrace();\n",
        "        }\n",
        "        List<String> examples = new ArrayList<>();\n",
        "        List<String> labels = new ArrayList<>();\n",
        "        for(String line:allLines) {\n",
        "            String[] tokens= line.split(\">>>\");\n",
        "            if(tokens.length != 2) {\n",
        "                System.out.println(\"Input line ERROR\");\n",
        "                continue;\n",
        "            }\n",
        "            examples.add(tokens[1]);\n",
        "            tokens = tokens[0].split(\"<<<\");\n",
        "            if(tokens.length != 2) {\n",
        "                System.out.println(\"Input line ERROR\");\n",
        "                continue;\n",
        "            }\n",
        "            labels.add(tokens[1]);\n",
        "        }\n",
        "\n",
        "        List<Feature> features= new ArrayList<>(examples.size());\n",
        "        for(int i=0;i< examples.size();i++) {\n",
        "            List<Integer> tokens = customTokenizer.tokenize(examples.get(i),trainMod);\n",
        "            Feature feature = customTokenizer.getFeatures(tokens,labels.get(i));\n",
        "            if(trainMod) {\n",
        "                customTokenizer.addRandomMaskAndReplace(feature,true,true);\n",
        "            }\n",
        "            features.add(feature);\n",
        "        }\n",
        "        return features;\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}