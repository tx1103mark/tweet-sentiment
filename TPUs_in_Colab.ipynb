{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuppI1BtB91H"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/fusion/batchmatmul_fusion.h\"\n",
        "#include <memory>\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "#include \"src/param_value_lite.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"utils/utils.h\"\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include \"securec/include/securec.h\"\n",
        "\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "bool IsStackNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Stack;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "bool IsFullConnectNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_FullConnection;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "void *GetInputAddr(const AnfNodePtr &node, int input_index) {\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  if (!node->isa<CNode>()) {\n",
        "    MS_LOG(ERROR) << \"GetInputAddr not cnode\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  if (input_index >= cnode->inputs().size()) {\n",
        "    MS_LOG(ERROR) << \"input index error\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  if (cnode->input(input_index)->isa<Parameter>()) {\n",
        "    auto param_input = cnode->input(input_index)->cast<ParameterPtr>();\n",
        "    auto param_value = std::dynamic_pointer_cast<ParamValueLite>(param_input->default_param());;\n",
        "    if (param_value == nullptr) {\n",
        "      MS_LOG(ERROR) << \"param not paramValueLite\";\n",
        "      return nullptr;\n",
        "    }\n",
        "    return param_value->tensor_addr();\n",
        "  }\n",
        "  MS_LOG(ERROR) << \"input not paramter\";\n",
        "  return nullptr;\n",
        "}\n",
        "STATUS GetRightMatmulInputParamter(CNodePtr &stack_node, ParameterPtr &rmatmul_input) {\n",
        "  MS_ASSERT(stack_node != nullptr);\n",
        "  MS_ASSERT(right_matmul_input != nullptr);\n",
        "  auto joint_fullconnect_size = stack_node->inputs().size() - 1;\n",
        "  auto fc = stack_node->input(1)->cast<CNodePtr>();\n",
        "  auto fc_weight = fc->input(2)->cast<ParameterPtr>();\n",
        "  auto fc_weight_param = std::dynamic_pointer_cast<ParamValueLite>(fc_weight->default_param());\n",
        "  auto tensor_size = fc_weight_param->tensor_size();\n",
        "  auto rmatmul_input_shape = fc_weight_param->tensor_shape();\n",
        "  auto new_tensor_data = new(std::nothrow) int8_t[joint_fullconnect_size * tensor_size];\n",
        "  if (new_tensor_data == nullptr) {\n",
        "    MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  for (int i = 1; i < joint_fullconnect_size + 1; i++) {\n",
        "    auto tensor_addr = GetInputAddr(stack_node->input(i), 2);\n",
        "    if (tensor_addr == nullptr) {\n",
        "      MS_LOG(ERROR) << \"input tensor addr nullptr\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    if (EOK != memcpy_s(new_tensor_data + (i - 1) * tensor_size, tensor_size, tensor_addr, tensor_size)) {\n",
        "      MS_LOG(ERROR) << \"memcpy_s data failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  rmatmul_input_shape.insert(rmatmul_input_shape.begin(), joint_fullconnect_size);\n",
        "  auto type_ptr = TypeIdToType(fc_weight_param->tensor_type());\n",
        "  auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(type_ptr, rmatmul_input_shape);\n",
        "  rmatmul_input->set_abstract(abstract_tensor);\n",
        "  rmatmul_input->set_name(stack_node->fullname_with_scope() + \"right_parameter\");\n",
        "  ParamValueLitePtr param_value = std::make_shared<ParamValueLite>();\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  param_value->set_tensor_shape(rmatmul_input_shape);\n",
        "  param_value->set_tensor_type(fc_weight_param->tensor_type());\n",
        "  param_value->set_format(fc_weight_param->format());\n",
        "  param_value->set_tensor_addr(new_tensor_data);\n",
        "  param_value->set_tensor_size(joint_fullconnect_size * tensor_size);\n",
        "  rmatmul_input->set_default_param(param_value);\n",
        "  return RET_OK;\n",
        "}\n",
        "}  // namespace\n",
        "const BaseRef BatchMatMulFusion::DefinePattern() const {\n",
        "  auto pack_var = std::make_shared<CondVar>(IsStackNode);\n",
        "  auto fullconnect_var = std::make_shared<CondVar>(IsFullConnectNode);\n",
        "  auto bn_other_var = std::make_shared<SeqVar>();\n",
        "  return VectorRef({pack_var, fullconnect_var, fullconnect_var, bn_other_var});\n",
        "}\n",
        "\n",
        "// slice +fullconnect ->batchmatmul\n",
        "const AnfNodePtr BatchMatMulFusion::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                            const EquivPtr &) const {\n",
        "\n",
        "  MS_ASSERT(func_graph != nullptr);\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  auto stack_cnode = node->cast<CNodePtr>();\n",
        "  // check stack node all inputs must fullconnect\n",
        "  for (int i = 1; i < stack_cnode->inputs().size(); i++) {\n",
        "    auto input_node = stack_cnode->input(i);\n",
        "    if (!IsFullConnectNode(input_node)) {\n",
        "      MS_LOG(WARNING) << \"batchmatmulfusion stack node all inputs must fullconnect type\";\n",
        "      return nullptr;\n",
        "    }\n",
        "  }\n",
        "  auto fullconnect_node = stack_cnode->input(1);\n",
        "  MS_ASSERT(fullconnnect_node != nullptr);\n",
        "  auto fullconnect_cnode = fullconnect_node->cast<CNodePtr>();\n",
        "  MS_ASSERT(fullconnect_cnode->inputs().size() == 3);\n",
        "  auto left_slice_node = fullconnect_cnode->input(1);\n",
        "  auto left_slice_cnode = left_slice_node->cast<CNodePtr>();\n",
        "  auto left_matmul_input = left_slice_cnode->input(1);\n",
        "  auto right_reshape_node = fullconnect_cnode->input(2);\n",
        "\n",
        "  auto matmul_primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  std::unique_ptr<schema::MatMulT> attr = std::make_unique<schema::MatMulT>();\n",
        "  matmul_primitive->value.type = schema::PrimitiveType_MatMul;\n",
        "  matmul_primitive->value.value = attr.release();\n",
        "  auto matmul_cvalue = lite::PrimitiveC::Create(matmul_primitive.release());\n",
        "  // get matmul quantParams\n",
        "  std::vector<schema::QuantParamT> jointed_quant_params;\n",
        "  for (int i = 1; i < 9; i++) {\n",
        "    auto fullconnect_node2 = stack_cnode->input(i)->cast<CNodePtr>();\n",
        "    auto fc_prim = GetValueNode<std::shared_ptr<lite::PrimitiveC>>(fullconnect_node2->input(0));\n",
        "    auto fc_input_quantParams = fc_prim->GetInputQuantParams();\n",
        "    if (fc_input_quantParams.size() > 1 && !fc_input_quantParams[1].empty()) {\n",
        "      jointed_quant_params.push_back(fc_input_quantParams[1][0]);\n",
        "    }\n",
        "  }\n",
        "  auto fc_prim = GetValueNode<std::shared_ptr<lite::PrimitiveC>>(fullconnect_cnode->input(0));\n",
        "  auto rmatmul_quant_params = fc_prim->GetInputQuantParams();\n",
        "  rmatmul_quant_params.pop_back();\n",
        "  rmatmul_quant_params.pop_back();\n",
        "  // no bias quantParams\n",
        "  rmatmul_quant_params.emplace_back(jointed_quant_params);\n",
        "  matmul_cvalue->SetInputQuantParam(rmatmul_quant_params);\n",
        "  matmul_cvalue->SetOutputQuantParam(fc_prim->GetOutputQuantParams());\n",
        "  auto matmul_value_node = NewValueNode(std::shared_ptr<lite::PrimitiveC>(matmul_cvalue));\n",
        "  std::vector<AnfNodePtr> matmul_inputs = {matmul_value_node, left_matmul_input};\n",
        "\n",
        "  // batchmatmul right node may be const\n",
        "  if (right_reshape_node->isa<Parameter>()) {\n",
        "//    return stack_cnode;\n",
        "    auto rmatmul_paramter = func_graph->add_parameter();\n",
        "    if (GetRightMatmulInputParamter(stack_cnode, rmatmul_paramter) != RET_OK) {\n",
        "      MS_LOG(ERROR) << \"GetRightMatmulInputParamter failed\";\n",
        "      return node;\n",
        "    }\n",
        "    auto prim = GetValueNode<std::shared_ptr<lite::PrimitiveC>>(matmul_value_node);\n",
        "    prim->GetPrimitiveT()->value.AsMatMul()->transposeB = true;\n",
        "    matmul_inputs.push_back(rmatmul_paramter);\n",
        "  } else {\n",
        "    auto right_reshape_cnode = right_reshape_node->cast<CNodePtr>();\n",
        "    MS_ASSERT(right_reshape_cnode->inputs().size() > 1);\n",
        "    auto right_transpose_node = right_reshape_cnode->input(1);\n",
        "    auto right_transpose_cnode = right_transpose_node->cast<CNodePtr>();\n",
        "    auto right_slice_node = right_transpose_cnode->input(1);\n",
        "    auto right_slice_cnode = right_slice_node->cast<CNodePtr>();\n",
        "    auto right_matmul_input = right_slice_cnode->input(1);\n",
        "    matmul_inputs.push_back(right_matmul_input);\n",
        "  }\n",
        "  auto matmul_cnode = func_graph->NewCNode(matmul_inputs);\n",
        "  matmul_cnode->set_fullname_with_scope(\"matmul_\" + stack_cnode->fullname_with_scope());\n",
        "  MS_LOG(INFO) << \"stack node:\" << stack_cnode->fullname_with_scope() << \" batchmatmul fusion success\";\n",
        "  return matmul_cnode;\n",
        "}\n",
        "}  // namespace mindspore::opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SYOlFaHCfBI"
      },
      "source": [
        " if (input_tensor->shape().size() == 3\n",
        "      && input_tensor->GetQuantParams().size() == input_tensor->shape()[0]) { // per batch matmul\n",
        "    auto per_batch_size = input_tensor->shape()[0];\n",
        "    auto quant_param = input_tensor->GetQuantParams();\n",
        "    for (size_t i = 0; i < per_batch_size; i++) {\n",
        "      auto param = quant_param.at(i);\n",
        "      auto scale = param.scale;\n",
        "      auto zero_point = param.zeroPoint;\n",
        "      auto matrix_size = input_tensor->ElementsNum() / per_batch_size;\n",
        "      for (int64_t j = 0; j < matrix_size; j++) {\n",
        "        dequant_datas[i * matrix_size + j] =\n",
        "            static_cast<float>((quant_datas[i * matrix_size + j] - zero_point) * scale);\n",
        "      }\n",
        "    }\n",
        "    return dequant_datas;\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxrNyOcoKYZ2"
      },
      "source": [
        "/**\n",
        " * Copyright 2019-2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"src/ops/matmul.h\"\n",
        "#include <memory>\n",
        "#include <utility>\n",
        "#ifdef PRIMITIVE_WRITEABLE\n",
        "#include \"tools/converter/quantizer/quantize_util.h\"\n",
        "#endif\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "#ifdef PRIMITIVE_WRITEABLE\n",
        "bool MatMul::GetTransposeA() const { return this->primitive_->value.AsMatMul()->transposeA; }\n",
        "bool MatMul::GetTransposeB() const { return this->primitive_->value.AsMatMul()->transposeB; }\n",
        "\n",
        "void MatMul::SetTransposeA(bool transpose_a) { this->primitive_->value.AsMatMul()->transposeA = transpose_a; }\n",
        "void MatMul::SetTransposeB(bool transpose_b) { this->primitive_->value.AsMatMul()->transposeB = transpose_b; }\n",
        "\n",
        "int MatMul::UnPackAttr(const Primitive &prim, const std::vector<AnfNodePtr> &inputs) {\n",
        "  if (this->primitive_ == nullptr) {\n",
        "    this->primitive_ = new(std::nothrow) schema::PrimitiveT;\n",
        "    if (this->primitive_ == nullptr) {\n",
        "      MS_LOG(ERROR) << \"new primitiveT failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    this->primitive_->value.type = schema::PrimitiveType_MatMul;\n",
        "  }\n",
        "  if (this->primitive_->value.type != schema::PrimitiveType_MatMul) {\n",
        "    MS_LOG(ERROR) << \"Primitive type is error :\" << this->primitive_->value.type;\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  if (this->primitive_->value.value == nullptr) {\n",
        "    auto attr = new(std::nothrow) schema::MatMulT();\n",
        "    if (attr == nullptr) {\n",
        "      MS_LOG(ERROR) << \"new primitiveT value failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    attr->transposeA = GetValue<bool>(prim.GetAttr(\"transpose_a\"));\n",
        "    attr->transposeB = GetValue<bool>(prim.GetAttr(\"transpose_b\"));\n",
        "    this->primitive_->value.value = attr;\n",
        "    if (this->primitive_->value.value == nullptr) {\n",
        "      MS_LOG(ERROR) << \"primitive value is nullptr\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  if (GetQuantType() == schema::QuantType_AwareTraining) {\n",
        "    std::vector<std::vector<schema::QuantParamT>> vecInputQuantParam;\n",
        "    std::vector<std::vector<schema::QuantParamT>> vecOutputQuantParam;\n",
        "    PopulaterQuantParam(prim, &vecInputQuantParam, &vecOutputQuantParam, inputs);\n",
        "    SetInputQuantParam(vecInputQuantParam);\n",
        "    SetOutputQuantParam(vecOutputQuantParam);\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "#else\n",
        "\n",
        "bool MatMul::GetTransposeA() const { return this->primitive_->value_as_MatMul()->transposeA(); }\n",
        "bool MatMul::GetTransposeB() const { return this->primitive_->value_as_MatMul()->transposeB(); }\n",
        "\n",
        "int MatMul::UnPackToFlatBuilder(const schema::Primitive *primitive, flatbuffers::FlatBufferBuilder *fbb) {\n",
        "  MS_ASSERT(nullptr != primitive);\n",
        "  MS_ASSERT(nullptr != fbb);\n",
        "  auto attr = primitive->value_as_MatMul();\n",
        "  if (attr == nullptr) {\n",
        "    MS_LOG(ERROR) << \"value_as_MatMul return nullptr\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  auto val_offset = schema::CreateMatMul(*fbb, attr->broadcast(), attr->transposeA(), attr->transposeB());\n",
        "  auto prim_offset = schema::CreatePrimitive(*fbb, schema::PrimitiveType_MatMul, val_offset.o);\n",
        "  fbb->Finish(prim_offset);\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "#endif\n",
        "\n",
        "int MatMul::InferShape(std::vector<Tensor *> inputs_, std::vector<Tensor *> outputs_) {\n",
        "  MS_ASSERT(this->primitive_ != nullptr);\n",
        "  auto input0 = inputs_.front();\n",
        "  MS_ASSERT(input0 != nullptr);\n",
        "  auto input1 = inputs_.at(1);\n",
        "  MS_ASSERT(input1 != nullptr);\n",
        "  auto output = outputs_.front();\n",
        "  MS_ASSERT(output != nullptr);\n",
        "\n",
        "  output->set_data_type(input0->data_type());\n",
        "  output->SetFormat(input0->GetFormat());\n",
        "  if (!GetInferFlag()) {\n",
        "    return RET_OK;\n",
        "  }\n",
        "\n",
        "  std::vector<int> a_shape = input0->shape();\n",
        "  std::vector<int> b_shape = input1->shape();\n",
        "  if (a_shape.size() < 2 || b_shape.size() < 2) {\n",
        "    MS_LOG(ERROR) << \"inputs shape is invalid\";\n",
        "    return RET_INPUT_TENSOR_ERROR;\n",
        "  }\n",
        "  bool need_broadcast = false;\n",
        "  if (a_shape.size() != b_shape.size() || (a_shape[0] == 1 && b_shape[0] != 1)\n",
        "      || (a_shape[0] != 1 && b_shape[0] == 1)) {\n",
        "    need_broadcast = true;\n",
        "  }\n",
        "  if (!need_broadcast) {\n",
        "    for (size_t i = 0; i < a_shape.size() - 2; ++i) {\n",
        "      if (a_shape[i] != b_shape[i]) {\n",
        "        MS_LOG(ERROR) << \"Op MatMul's dimensions must be equal\";\n",
        "        return RET_INPUT_TENSOR_ERROR;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (GetTransposeA()) {\n",
        "    std::swap(a_shape[a_shape.size() - 1], a_shape[a_shape.size() - 2]);\n",
        "  }\n",
        "  if (GetTransposeB()) {\n",
        "    std::swap(b_shape[b_shape.size() - 1], b_shape[b_shape.size() - 2]);\n",
        "  }\n",
        "  std::vector<int> c_shape(a_shape);\n",
        "  c_shape[c_shape.size() - 1] = b_shape[b_shape.size() - 1];\n",
        "  if (need_broadcast) {\n",
        "    size_t batch = 1;\n",
        "    if (a_shape.size() != b_shape.size()) {\n",
        "      batch = a_shape.size() > b_shape.size() ? a_shape[0] : b_shape[0];\n",
        "      if (b_shape.size() > a_shape.size()) {\n",
        "        c_shape.insert(c_shape.begin(), batch);\n",
        "      }\n",
        "    } else {\n",
        "      batch = a_shape[0] != 1 ? a_shape[0] : b_shape[0];\n",
        "      c_shape[0] = batch;\n",
        "    }\n",
        "  }\n",
        "  output->set_shape(c_shape);\n",
        "  return RET_OK;\n",
        "}\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}