{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlxZqc0j9sp5"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import com.mindspore.lite.MSTensor;\n",
        "import com.mindspore.lite.TrainSession;\n",
        "import com.mindspore.lite.config.MSConfig;\n",
        "\n",
        "import java.io.*;\n",
        "import java.nio.ByteBuffer;\n",
        "import java.nio.ByteOrder;\n",
        "import java.util.ArrayList;\n",
        "import java.util.List;\n",
        "import java.util.Map;\n",
        "\n",
        "public class AdTrainBert {\n",
        "    private int batchSize;\n",
        "    private int batchNum;\n",
        "    private int dataSize;\n",
        "    private TrainSession trainSession;\n",
        "    private List<Feature> features;\n",
        "    private ByteBuffer inputIdBufffer;\n",
        "    private ByteBuffer tokenIdBufffer;\n",
        "    private ByteBuffer maskIdBufffer;\n",
        "    private ByteBuffer labelIdBufffer;\n",
        "\n",
        "    static {\n",
        "        System.loadLibrary(\"mindspore-lite-jni\");\n",
        "    }\n",
        "    public int initDataSet(String exampleFile, String vocabFile, int batchSize) throws IOException {\n",
        "        features = DataSet.init(exampleFile, vocabFile,true);\n",
        "        this.batchSize = batchSize;\n",
        "        batchNum = features.size() / batchSize;\n",
        "        return features.size();\n",
        "    }\n",
        "    public void initSessionAndInputs(String modelPath) {\n",
        "        MSConfig msConfig = new MSConfig();\n",
        "        // arg 0: DeviceType:DT_CPU -> 0\n",
        "        // arg 1: ThreadNum -> 2\n",
        "        // arg 2: cpuBindMode:NO_BIND ->  0\n",
        "        // arg 3: enable_fp16 -> false\n",
        "        msConfig.init(0, 1, 0, false);\n",
        "        trainSession = new TrainSession();\n",
        "        trainSession.init(modelPath, msConfig);\n",
        "        trainSession.setLearningRate(0.01f);\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "        MSTensor labelIdTensor = inputs.get(0);\n",
        "        int inputLength = labelIdTensor.elementsNum(); // labelId,tokenId,inputId,maskId has same size\n",
        "        dataSize = inputLength / batchSize;\n",
        "        inputIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        tokenIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        maskIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        labelIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        inputIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        tokenIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        maskIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        labelIdBufffer.order(ByteOrder.nativeOrder());\n",
        "    }\n",
        "    public void fillAdTrainBertInput(int batchIdx) throws IOException {\n",
        "        inputIdBufffer.clear();\n",
        "        tokenIdBufffer.clear();\n",
        "        maskIdBufffer.clear();\n",
        "        labelIdBufffer.clear();\n",
        "        for (int i = 0; i < batchSize; i++) {\n",
        "            Feature feature = features.get(batchIdx * batchSize + i);\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                inputIdBufffer.putInt(feature.inputIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                tokenIdBufffer.putInt(feature.tokenIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                maskIdBufffer.putInt(feature.inputMasks[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                labelIdBufffer.putInt(feature.inputIds[j]);\n",
        "            }\n",
        "        }\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "        MSTensor labelIdTensor = inputs.get(0);\n",
        "        MSTensor tokenIdTensor = inputs.get(1);\n",
        "        MSTensor inputIdTensor = inputs.get(2);\n",
        "        MSTensor maskIdTensor = inputs.get(3);\n",
        "        labelIdTensor.setData(labelIdBufffer);\n",
        "        tokenIdTensor.setData(tokenIdBufffer);\n",
        "        inputIdTensor.setData(inputIdBufffer);\n",
        "        maskIdTensor.setData(maskIdBufffer);\n",
        "    }\n",
        "\n",
        "    private MSTensor searchOutputsForSize(int size) {\n",
        "        Map<String, MSTensor> outputs = trainSession.getOutputMapByTensor();\n",
        "        for (MSTensor tensor : outputs.values()) {\n",
        "            if (tensor.elementsNum() == size) {\n",
        "                return tensor;\n",
        "            }\n",
        "        }\n",
        "        System.err.println(\"can not find output the tensor which element num is \" + size);\n",
        "        return null;\n",
        "    }\n",
        "\n",
        "    public float getLoss() {\n",
        "        MSTensor tensor = searchOutputsForSize(1);\n",
        "        return tensor.getFloatData()[0];\n",
        "    }\n",
        "\n",
        "    public int trainLoop(int epoches) throws IOException {\n",
        "        trainSession.train();\n",
        "        for (int i = 0; i < epoches; i++) {\n",
        "            float sumLossPerEpoch = 0.0f;\n",
        "            for (int j = 0; j < batchNum; j++) {\n",
        "                fillAdTrainBertInput(j);\n",
        "                trainSession.runGraph();\n",
        "                float loss = getLoss();\n",
        "                sumLossPerEpoch += loss;\n",
        "                System.out.println(\"------batch:\"+j+\",loss:\"+loss+\"-----------\");\n",
        "            }\n",
        "            System.out.println(\"----------epoch:\" + i + \",mean loss:\" + sumLossPerEpoch / batchNum + \"----------\");\n",
        "        }\n",
        "        return 0;\n",
        "    }\n",
        "    public int trainModel(String modelPath,String dataFile,String vocabFile,int epoches,int batchSize) throws IOException {\n",
        "        System.out.println(\"==========Init dataFile,\"+dataFile+ \",vocabFile,\"+vocabFile+\"=============\");\n",
        "        initDataSet(dataFile,vocabFile,batchSize);\n",
        "        System.out.println(\"==========Loading Model,\"+modelPath+\" Create Train Session=============\");\n",
        "        initSessionAndInputs(modelPath);\n",
        "        System.out.println(\"==========Begin Train Model=============\");\n",
        "        getFeatures();\n",
        "        updateFeatures(modelPath);\n",
        "        System.out.println(\"-----after update features------\");\n",
        "        getFeatures();\n",
        "        trainLoop(epoches);\n",
        "        trainSession.free();\n",
        "        return 0;\n",
        "    }\n",
        "    public int getFeatures() {\n",
        "        List<MSTensor> featuresMap= trainSession.getFeaturesMap();\n",
        "        for(int i=0;i<5;i++){\n",
        "            MSTensor feature = featuresMap.get(i);\n",
        "            float[] data= feature.getFloatData();\n",
        "            String name = feature.tensorName();\n",
        "            int elements = feature.elementsNum();\n",
        "            System.out.println(\"tensorname:\"+name+\",len,\"+elements+\",\"+data[0]);\n",
        "        }\n",
        "        return 0;\n",
        "    }\n",
        "    public int updateFeatures(String modelName) {\n",
        "        List<MSTensor> tensors = new ArrayList<>();\n",
        "        String name = \"albert.embedding_postprocessor.layernorm.gamma\";\n",
        "        ByteBuffer newData = ByteBuffer.allocateDirect( 128*Float.BYTES);\n",
        "        newData.order(ByteOrder.nativeOrder());\n",
        "        for (int j = 0; j <128; j++) {\n",
        "            newData.putFloat(2.2f);\n",
        "        }\n",
        "        MSTensor ms = new MSTensor(name,newData);\n",
        "        tensors.add(ms);\n",
        "        trainSession.updateFeatures(modelName,tensors);\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) throws IOException {\n",
        "        AdTrainBert adTrainBert = new AdTrainBert();\n",
        "        String dataFile = \"/home/meng/zj10/mindspore/mindspore/lite/102.txt\";\n",
        "        String vocabFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/flclient/src/main/native/dataset/vocab.txt\";\n",
        "        String modelPath = \"/home/meng/zj10/mindspore/mindspore/lite/albert_ad_train.mindir.ms\";\n",
        "        int epoches = 1;\n",
        "        int batchSize = 4;\n",
        "        adTrainBert.trainModel(modelPath,dataFile,vocabFile,epoches,batchSize);\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njx1DKcD961Z"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import com.mindspore.lite.MSTensor;\n",
        "import com.mindspore.lite.TrainSession;\n",
        "import com.mindspore.lite.config.MSConfig;\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.nio.ByteBuffer;\n",
        "import java.nio.ByteOrder;\n",
        "import java.util.ArrayList;\n",
        "import java.util.List;\n",
        "import java.util.Map;\n",
        "\n",
        "public class AdInferBert {\n",
        "    private int batchSize;\n",
        "    private int batchNum;\n",
        "    private int dataSize;\n",
        "    private int numOfClass =  5;\n",
        "    private TrainSession trainSession;\n",
        "    private List<Feature> features;\n",
        "    private ByteBuffer inputIdBufffer;\n",
        "    private ByteBuffer tokenIdBufffer;\n",
        "    private ByteBuffer maskIdBufffer;\n",
        "\n",
        "    static {\n",
        "        System.loadLibrary(\"mindspore-lite-jni\");\n",
        "    }\n",
        "    public int initDataSet(String exampleFile, String vocabFile, int batchSize) throws IOException {\n",
        "        features = DataSet.init(exampleFile, vocabFile,false);\n",
        "        this.batchSize = batchSize;\n",
        "        batchNum = features.size() / batchSize;\n",
        "        return features.size();\n",
        "    }\n",
        "    public void initSessionAndInputs(String modelPath) {\n",
        "        MSConfig msConfig = new MSConfig();\n",
        "        // arg 0: DeviceType:DT_CPU -> 0\n",
        "        // arg 1: ThreadNum -> 2\n",
        "        // arg 2: cpuBindMode:NO_BIND ->  0\n",
        "        // arg 3: enable_fp16 -> false\n",
        "        msConfig.init(0, 1, 0, false);\n",
        "        trainSession = new TrainSession();\n",
        "        trainSession.init(modelPath, msConfig);\n",
        "        trainSession.setLearningRate(0.01f);\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "        MSTensor tokenIdTensor = inputs.get(0);\n",
        "        MSTensor inputIdTensor = inputs.get(1);\n",
        "        MSTensor maskIdTensor = inputs.get(2);\n",
        "        int inputLength = tokenIdTensor.elementsNum(); // labelId,tokenId,inputId,maskId has same size\n",
        "        dataSize = inputLength / batchSize;\n",
        "        inputIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        tokenIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        maskIdBufffer = ByteBuffer.allocateDirect(inputLength * Integer.BYTES);\n",
        "        inputIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        tokenIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        maskIdBufffer.order(ByteOrder.nativeOrder());\n",
        "        tokenIdTensor.setData(tokenIdBufffer);\n",
        "        inputIdTensor.setData(inputIdBufffer);\n",
        "        maskIdTensor.setData(maskIdBufffer);\n",
        "    }\n",
        "    public List<Integer> fillAdInferBertInput(int batchIdx) throws IOException {\n",
        "        inputIdBufffer.clear();\n",
        "        tokenIdBufffer.clear();\n",
        "        maskIdBufffer.clear();\n",
        "        List<Integer> labels = new ArrayList<>(batchSize);\n",
        "        for (int i = 0; i < batchSize; i++) {\n",
        "            Feature feature = features.get(batchIdx * batchSize + i);\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                inputIdBufffer.putInt(feature.inputIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                tokenIdBufffer.putInt(feature.tokenIds[j]);\n",
        "            }\n",
        "            for (int j = 0; j < dataSize; j++) {\n",
        "                maskIdBufffer.putInt(feature.inputMasks[j]);\n",
        "            }\n",
        "            labels.add(feature.labelIds);\n",
        "        }\n",
        "        List<MSTensor> inputs = trainSession.getInputs();\n",
        "\n",
        "        MSTensor tokenIdTensor = inputs.get(0);\n",
        "        MSTensor inputIdTensor = inputs.get(1);\n",
        "        MSTensor maskIdTensor = inputs.get(2);\n",
        "        tokenIdTensor.setData(tokenIdBufffer);\n",
        "        inputIdTensor.setData(inputIdBufffer);\n",
        "        maskIdTensor.setData(maskIdBufffer);\n",
        "        return labels;\n",
        "    }\n",
        "\n",
        "    private MSTensor searchOutputsForSize(int size) {\n",
        "        Map<String, MSTensor> outputs = trainSession.getOutputMapByTensor();\n",
        "        for (MSTensor tensor : outputs.values()) {\n",
        "            if (tensor.elementsNum() == size) {\n",
        "                return tensor;\n",
        "            }\n",
        "        }\n",
        "        System.err.println(\"can not find output the tensor which element num is \" + size);\n",
        "        return null;\n",
        "    }\n",
        "    public float calculateAccracy( List<Integer> labels) {\n",
        "        MSTensor outputTensor = searchOutputsForSize(batchSize*numOfClass);\n",
        "        float[] scores = outputTensor.getFloatData();\n",
        "        float accuracy = 0.0f;\n",
        "        for(int b=0;b<batchSize;b++) {\n",
        "            int maxIdx = 0;\n",
        "            float maxScore = scores[numOfClass*b];\n",
        "            for(int c = 0;c<numOfClass;c++) {\n",
        "                if(scores[numOfClass*b+c] >maxScore) {\n",
        "                    maxScore = scores[numOfClass*b+c];\n",
        "                    maxIdx = c;\n",
        "                }\n",
        "            }\n",
        "            if(labels.get(b) == maxIdx) {\n",
        "                accuracy+=1;\n",
        "            }\n",
        "        }\n",
        "        return accuracy/batchSize;\n",
        "    }\n",
        "    public float infer() throws IOException {\n",
        "        trainSession.eval();\n",
        "        float totalAccuracy = 0.0f;\n",
        "        for (int j = 0; j < batchNum; j++) {\n",
        "                List<Integer> labels= fillAdInferBertInput(j);\n",
        "                trainSession.runGraph();\n",
        "                float curAcc = calculateAccracy(labels);\n",
        "                totalAccuracy += curAcc;\n",
        "            System.out.println(\"batch num:\"+j+\",acc is:\"+curAcc);\n",
        "        }\n",
        "        System.out.println(\"total acc:\"+totalAccuracy/batchNum);\n",
        "        return totalAccuracy/batchNum;\n",
        "    }\n",
        "\n",
        "    public float inferModel(String modelPath,String dataFile,String vocabFile,int batchSize) throws IOException {\n",
        "        System.out.println(\"==========Init dataFile,\"+dataFile+ \",vocabFile,\"+vocabFile+\"=============\");\n",
        "        initDataSet(dataFile,vocabFile,batchSize);\n",
        "        System.out.println(\"==========Loading Model,\"+modelPath+\" Create Train Session=============\");\n",
        "        initSessionAndInputs(modelPath);\n",
        "        System.out.println(\"==========Begin Infer Model=============\");\n",
        "        float  acc = infer();\n",
        "        trainSession.free();\n",
        "        return acc;\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) throws IOException {\n",
        "        AdInferBert adInferBert = new AdInferBert();\n",
        "        String dataFile = \"/home/meng/zj10/mindspore/mindspore/lite/eval.txt\";\n",
        "        String vocabFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/flclient/src/main/native/dataset/vocab.txt\";\n",
        "        String modelPath = \"/home/meng/zj10/mindspore/mindspore/lite/albert_ad_infer.mindir.ms\";\n",
        "        int batchSize =16;\n",
        "        adInferBert.inferModel(modelPath,dataFile,vocabFile,batchSize);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQCSsiaN9959"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.nio.charset.StandardCharsets;\n",
        "import java.nio.file.Files;\n",
        "import java.nio.file.Path;\n",
        "import java.nio.file.Paths;\n",
        "import java.util.*;\n",
        "\n",
        "class Feature {\n",
        "    int[] inputIds;\n",
        "    int[] inputMasks;\n",
        "    int[] tokenIds;\n",
        "    int labelIds;\n",
        "    int seqLen;\n",
        "\n",
        "    public Feature(int[] inputIds, int[] inputMasks, int[] tokenIds, int labelIds, int seqLen) {\n",
        "        this.inputIds = inputIds;\n",
        "        this.inputMasks = inputMasks;\n",
        "        this.tokenIds = tokenIds;\n",
        "        this.labelIds = labelIds;\n",
        "        this.seqLen = seqLen;\n",
        "    }\n",
        "}\n",
        "\n",
        "public class CustomTokenizer {\n",
        "    private Map<String, Integer> vocabs = new HashMap<>();\n",
        "    private Boolean doLowerCase = Boolean.TRUE;\n",
        "    private int maxInputChars = 100;\n",
        "    private String[] NotSplitStrs = {\"UNK\"};\n",
        "    private String unkToken = \"[UNK]\";\n",
        "    private int maxSeqLen = 256;\n",
        "    private int vocabSize = 21128;\n",
        "    private Map<String, Integer> labelMap = new HashMap<String, Integer>() {{\n",
        "        put(\"beauty\", 0);\n",
        "        put(\"education\", 1);\n",
        "        put(\"hotel\", 2);\n",
        "        put(\"travel\", 3);\n",
        "        put(\"other\", 4);\n",
        "    }};\n",
        "\n",
        "\n",
        "    public void init(String vocabFile, boolean doLowerCase) throws IOException {\n",
        "        this.doLowerCase = doLowerCase;\n",
        "        Path path = Paths.get(vocabFile);\n",
        "        List<String> allLines = Files.readAllLines(path, StandardCharsets.UTF_8);\n",
        "        int idx = 0;\n",
        "        for (String line : allLines) {\n",
        "            vocabs.put(line, idx++);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // is chinses or punctuation\n",
        "    public Boolean isChineseOrPunc(char trimChar) {\n",
        "        // is chinese char\n",
        "        if (trimChar >= '\\u4e00' && trimChar <= '\\u9fa5') {\n",
        "            return true;\n",
        "        }\n",
        "        // is puncuation char\n",
        "        if ((trimChar >= 33 && trimChar <= 47) || (trimChar >= 58 && trimChar <= 64) || (trimChar >= 91 && trimChar <= 96) || (trimChar >= 123 && trimChar <= 126)) {\n",
        "            return true;\n",
        "        }\n",
        "        return false;\n",
        "    }\n",
        "\n",
        "    public String[] splitText(String text) {\n",
        "        // clean remove white and control char\n",
        "        String trimText = text.trim();\n",
        "        StringBuilder cleanText = new StringBuilder();\n",
        "        for (int i = 0; i < trimText.length(); i++) {\n",
        "            if (isChineseOrPunc(trimText.charAt(i))) {\n",
        "                cleanText.append(\" \" + trimText.charAt(i) + \" \");\n",
        "            } else {\n",
        "                cleanText.append(trimText.charAt(i));\n",
        "            }\n",
        "        }\n",
        "        return cleanText.toString().trim().split(\"\\\\s+\");\n",
        "    }\n",
        "\n",
        "    //   input = \"unaffable\" , output = [\"un\", \"##aff\", \"##able\"]\n",
        "    public List<String> wordPieceTokenize(String[] tokens) {\n",
        "        List<String> outputTokens = new ArrayList<>();\n",
        "        for (String token : tokens) {\n",
        "            List<String> subTokens = new ArrayList<>();\n",
        "            boolean isBad = false;\n",
        "            int start = 0;\n",
        "            while (start < token.length()) {\n",
        "                int end = token.length();\n",
        "                String curStr = \"\";\n",
        "                while (start < end) {\n",
        "                    String subStr = token.substring(start, end);\n",
        "                    if (start > 0) {\n",
        "                        subStr = \"##\" + subStr;\n",
        "                    }\n",
        "                    if (vocabs.get(subStr) != null) {\n",
        "                        curStr = subStr;\n",
        "                        break;\n",
        "                    }\n",
        "                    end = end - 1;\n",
        "                }\n",
        "                if (curStr.isEmpty()) {\n",
        "                    isBad = true;\n",
        "                    break;\n",
        "                }\n",
        "                subTokens.add(curStr);\n",
        "                start = end;\n",
        "            }\n",
        "            if (isBad) {\n",
        "                outputTokens.add(unkToken);\n",
        "            } else {\n",
        "                outputTokens.addAll(subTokens);\n",
        "            }\n",
        "        }\n",
        "        return outputTokens;\n",
        "\n",
        "    }\n",
        "\n",
        "    public List<Integer> convertTokensToIds(List<String> tokens) {\n",
        "        if (tokens.size() > maxSeqLen - 2) {\n",
        "            tokens = tokens.subList(0, maxSeqLen - 2);\n",
        "        }\n",
        "        tokens.add(0, \"[CLS]\");\n",
        "        tokens.add(\"[SEP]\");\n",
        "        List<Integer> ids = new ArrayList<>(tokens.size());\n",
        "        for (String token : tokens) {\n",
        "            ids.add(vocabs.getOrDefault(token, vocabs.get(\"[UNK]\")));\n",
        "        }\n",
        "        return ids;\n",
        "    }\n",
        "\n",
        "    public void addRandomMaskAndReplace(Feature feature, boolean keepFirstUnchange, boolean keepLastUnchange) {\n",
        "        int[] masks = new int[maxSeqLen];\n",
        "        Arrays.fill(masks, 1);\n",
        "        int[] replaces = new int[maxSeqLen];\n",
        "        Arrays.fill(replaces, 1);\n",
        "        int[] inputIds = feature.inputIds;\n",
        "        for (int i = 0; i < feature.seqLen; i++) {\n",
        "            double rand1 = Math.random();\n",
        "            if (rand1 < 0.15) {\n",
        "                masks[i] = 0;\n",
        "                double rand2 = Math.random();\n",
        "                if (rand2 < 0.8) {\n",
        "                    replaces[i] = 103;\n",
        "                } else if (rand2 < 0.9) {\n",
        "                    masks[i] = 1;\n",
        "                } else {\n",
        "                    replaces[i] = (int) (Math.random() * vocabSize);\n",
        "                }\n",
        "            }\n",
        "            if(keepFirstUnchange) {\n",
        "                masks[i] = 1;\n",
        "                replaces[i] = 0;\n",
        "            }\n",
        "            if(keepLastUnchange) {\n",
        "                masks[feature.seqLen-1] = 1;\n",
        "                replaces[feature.seqLen-1] =0;\n",
        "            }\n",
        "            inputIds[i] = inputIds[i] * masks[i] + replaces[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    public Feature getFeatures(List<Integer> tokens, String label) {\n",
        "        int[] segmentIds = new int[maxSeqLen];\n",
        "        Arrays.fill(segmentIds, 0);\n",
        "        int[] masks = new int[maxSeqLen];\n",
        "        Arrays.fill(masks, 0);\n",
        "        Arrays.fill(masks, 0, tokens.size(), 1); // tokens size can ensure less than masks\n",
        "        int[] inputIds = new int[maxSeqLen];\n",
        "        Arrays.fill(inputIds, 0);\n",
        "        for (int i = 0; i < tokens.size(); i++) {\n",
        "            inputIds[i] = tokens.get(i);\n",
        "        }\n",
        "        return new Feature(inputIds, masks, segmentIds, labelMap.get(label), tokens.size());\n",
        "    }\n",
        "\n",
        "    public List<Integer> tokenize(String text) {\n",
        "        String[] splitTokens = splitText(text);\n",
        "        List<String> wordPieceTokens = wordPieceTokenize(splitTokens);\n",
        "        return convertTokensToIds(wordPieceTokens);\n",
        "\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) throws IOException {\n",
        "        String test = \"\\u9EC4abc\\u5927\";\n",
        "        CustomTokenizer customTokenizer = new CustomTokenizer();\n",
        "        String line = \"<<<other>>>unaffable\";\n",
        "        String[] tokens = line.split(\">>>\");\n",
        "        if (tokens.length != 2) {\n",
        "            System.out.println(\"Input line ERROR\");\n",
        "        }\n",
        "\n",
        "\n",
        "        String vocabFile = \"/home/meng/zj10/fl/mindspore/mindspore/lite/flclient/src/main/native/dataset/vocab.txt\";\n",
        "        customTokenizer.init(vocabFile, true);\n",
        "        customTokenizer.tokenize(tokens[1]);\n",
        "        tokens = tokens[0].split(\"<<<\");\n",
        "        System.out.println(tokens[0]);\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiBX0fg1-CQI"
      },
      "source": [
        "package com.huawei.flclient.model;\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.nio.charset.StandardCharsets;\n",
        "import java.nio.file.Files;\n",
        "import java.nio.file.Path;\n",
        "import java.nio.file.Paths;\n",
        "import java.util.ArrayList;\n",
        "import java.util.List;\n",
        "\n",
        "public class DataSet {\n",
        "\n",
        "    public static List<Feature> init(String trainFile,String vocabFile,boolean trainMod) throws IOException {\n",
        "        // read train file\n",
        "        CustomTokenizer customTokenizer = new CustomTokenizer();\n",
        "        customTokenizer.init(vocabFile,true);\n",
        "\n",
        "        Path path = Paths.get(trainFile);\n",
        "        List<String> allLines = Files.readAllLines(path, StandardCharsets.UTF_8);\n",
        "        List<String> examples = new ArrayList<>();\n",
        "        List<String> labels = new ArrayList<>();\n",
        "        for(String line:allLines) {\n",
        "            String[] tokens= line.split(\">>>\");\n",
        "            if(tokens.length != 2) {\n",
        "                System.out.println(\"Input line ERROR\");\n",
        "                continue;\n",
        "            }\n",
        "            examples.add(tokens[1]);\n",
        "            tokens = tokens[0].split(\"<<<\");\n",
        "            if(tokens.length != 2) {\n",
        "                System.out.println(\"Input line ERROR\");\n",
        "                continue;\n",
        "            }\n",
        "            labels.add(tokens[1]);\n",
        "        }\n",
        "\n",
        "        List<Feature> features= new ArrayList<>(examples.size());\n",
        "        for(int i=0;i< examples.size();i++) {\n",
        "            List<Integer> tokens = customTokenizer.tokenize(examples.get(i));\n",
        "            Feature feature = customTokenizer.getFeatures(tokens,labels.get(i));\n",
        "            if(trainMod) {\n",
        "                customTokenizer.addRandomMaskAndReplace(feature,true,true);\n",
        "            }\n",
        "            features.add(feature);\n",
        "        }\n",
        "        return features;\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}