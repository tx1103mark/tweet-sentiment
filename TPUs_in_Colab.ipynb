{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18a4343c030b4371b899e99d4f8c513c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97212f9b24d549d08e65ef631c788e5e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c9b0b798db64f1298760c85a57f097c",
              "IPY_MODEL_e3fe3c2345634b088bff27a09a10b640"
            ]
          }
        },
        "97212f9b24d549d08e65ef631c788e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c9b0b798db64f1298760c85a57f097c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33c8818e4b764cdcba3e4b6f52f1dbea",
            "_dom_classes": [],
            "description": "Training:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 171,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94e37f508ab34e898ec927961919de34"
          }
        },
        "e3fe3c2345634b088bff27a09a10b640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49f8aa1ae9f84e4b9a2c5177bc74ea4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/171 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c95bbed6e4ba46b18898146970ea9de6"
          }
        },
        "33c8818e4b764cdcba3e4b6f52f1dbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94e37f508ab34e898ec927961919de34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49f8aa1ae9f84e4b9a2c5177bc74ea4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c95bbed6e4ba46b18898146970ea9de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdBPxeSSlAF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include <memory>\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"include/model.h\"\n",
        "#include \"common/common_test.h\"\n",
        "#include \"include/lite_session.h\"\n",
        "#include \"include/context.h\"\n",
        "#include \"include/errorcode.h\"\n",
        "#include \"utils/log_adapter.h\"\n",
        "#include \"tools/converter/model_parser.h\"\n",
        "#include \"tools/converter/anf_transform.h\"\n",
        "#include \"tools/optimizer/fusion/constant_folding_fusion.h\"\n",
        "#include \"tools/anf_exporter/anf_exporter.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "class ConstantFoldingFusionTest : public mindspore::CommonTest {\n",
        " public:\n",
        "  ConstantFoldingFusionTest() = default;\n",
        "};\n",
        "using MetaGraphTptr = std::shared_ptr<schema::MetaGraphT>;\n",
        "using CNodeTptr = std::unique_ptr<schema::CNodeT>;\n",
        "\n",
        "namespace {\n",
        "\n",
        "MetaGraphTptr BuildGraph(schema::PrimitiveType op_type, void *op_node) {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // biasadd node\n",
        "  auto example_node = std::make_unique<schema::CNodeT>();\n",
        "  example_node->inputIndex = {0, 1};\n",
        "  example_node->outputIndex = {2};\n",
        "  example_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  example_node->primitive->value.type = op_type;\n",
        "  example_node->primitive->value.value = op_node;\n",
        "  example_node->name = \"example\";\n",
        "  meta_graph->nodes.emplace_back(std::move(example_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0, 1};\n",
        "  meta_graph->outputIndex = {2};\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // input 1: data2\n",
        "  auto input1 = std::make_unique<schema::TensorT>();\n",
        "  input1->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input1->format = schema::Format_NHWC;\n",
        "  input1->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input1->dims = {1, 2, 2, 3};\n",
        "  input1->offset = -1;\n",
        "  input1->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto input1_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input1_data[i] = i;\n",
        "  }\n",
        "  memcpy(input1->data.data(), input1_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input1_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input1));\n",
        "\n",
        "  // final add output\n",
        "  auto add_output = std::make_unique<schema::TensorT>();\n",
        "  add_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  add_output->format = schema::Format_NHWC;\n",
        "  add_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  add_output->dims = {1, 2, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(add_output));\n",
        "  // final output\n",
        "  return meta_graph;\n",
        "}\n",
        "\n",
        "MetaGraphTptr BuildGraphForOneInput(schema::PrimitiveType op_type, void *op_node) {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // biasadd node\n",
        "  auto example_node = std::make_unique<schema::CNodeT>();\n",
        "  example_node->inputIndex = {0};\n",
        "  example_node->outputIndex = {1};\n",
        "  example_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  example_node->primitive->value.type = op_type;\n",
        "  example_node->primitive->value.value = op_node;\n",
        "  example_node->name = \"example\";\n",
        "  meta_graph->nodes.emplace_back(std::move(example_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0};\n",
        "  meta_graph->outputIndex = {1};\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i + 1;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // final add output\n",
        "  auto add_output = std::make_unique<schema::TensorT>();\n",
        "  add_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  add_output->format = schema::Format_NHWC;\n",
        "  add_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  add_output->dims = {1, 2, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(add_output));\n",
        "\n",
        "  // final output\n",
        "  return meta_graph;\n",
        "}\n",
        "\n",
        "MetaGraphTptr BuildMixGraph() {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // add node\n",
        "  auto add_node = std::make_unique<schema::CNodeT>();\n",
        "  add_node->inputIndex = {0, 1};\n",
        "  add_node->outputIndex = {2};\n",
        "  add_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  add_node->primitive->value.type = schema::PrimitiveType_Add;\n",
        "  add_node->primitive->value.value = new schema::AddT;\n",
        "  add_node->name = \"add\";\n",
        "  meta_graph->nodes.emplace_back(std::move(add_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0, 1, 2};\n",
        "  meta_graph->outputIndex = {4};\n",
        "\n",
        "  auto mul_node = std::make_unique<schema::CNodeT>();\n",
        "  mul_node->inputIndex = {2, 3};\n",
        "  mul_node->outputIndex = {4};\n",
        "  mul_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  mul_node->primitive->value.type = schema::PrimitiveType_Mul;\n",
        "  mul_node->primitive->value.value = new schema::MulT;\n",
        "  mul_node->name = \"mul\";\n",
        "  meta_graph->nodes.emplace_back(std::move(mul_node));\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // input 1: data2\n",
        "  auto input1 = std::make_unique<schema::TensorT>();\n",
        "  input1->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input1->format = schema::Format_NHWC;\n",
        "  input1->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input1->dims = {1, 2, 2, 3};\n",
        "  input1->offset = -1;\n",
        "  input1->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto input1_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input1_data[i] = i;\n",
        "  }\n",
        "  memcpy(input1->data.data(), input1_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input1_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input1));\n",
        "\n",
        "  // addoutput\n",
        "  auto add_output = std::make_unique<schema::TensorT>();\n",
        "  add_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  add_output->format = schema::Format_NHWC;\n",
        "  add_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  add_output->dims = {1, 2, 2, 3};\n",
        "  add_output->offset = -1;\n",
        "  add_output->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto add_output_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  memcpy(add_output->data.data(), add_output_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] add_output_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(add_output));\n",
        "\n",
        "  // input 2: data3\n",
        "  auto input2 = std::make_unique<schema::TensorT>();\n",
        "  input2->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input2->format = schema::Format_NHWC;\n",
        "  input2->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input2->dims = {1, 2, 2, 3};\n",
        "  input2->offset = -1;\n",
        "  input2->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  auto input2_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input2_data[i] = 10;\n",
        "  }\n",
        "  memcpy(input2->data.data(), input2_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input2_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input2));\n",
        "\n",
        "  // final mul output\n",
        "  auto mul_output = std::make_unique<schema::TensorT>();\n",
        "  mul_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  mul_output->format = schema::Format_NHWC;\n",
        "  mul_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  mul_output->dims = {1, 2, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(mul_output));\n",
        "  // final output\n",
        "  return meta_graph;\n",
        "}\n",
        "MetaGraphTptr BuildSplitGraph() {\n",
        "  auto meta_graph = std::make_shared<schema::MetaGraphT>();\n",
        "  meta_graph->name = \"graph\";\n",
        "  // slice node\n",
        "  auto split_node = std::make_unique<schema::CNodeT>();\n",
        "  split_node->inputIndex = {0};\n",
        "  split_node->outputIndex = {1, 2};\n",
        "  split_node->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  split_node->primitive->value.type = schema::PrimitiveType_Split;\n",
        "  std::unique_ptr<schema::SplitT> attr = std::make_unique<schema::SplitT>();\n",
        "  attr->numberSplit = 2;\n",
        "  attr->splitDim = 1;\n",
        "  split_node->primitive->value.value = attr.release();\n",
        "  split_node->name = \"split\";\n",
        "  meta_graph->nodes.emplace_back(std::move(split_node));\n",
        "\n",
        "  meta_graph->inputIndex = {0, 3, 4};\n",
        "  meta_graph->outputIndex = {5, 6};\n",
        "\n",
        "  auto mul_node1 = std::make_unique<schema::CNodeT>();\n",
        "  mul_node1->inputIndex = {1, 3};\n",
        "  mul_node1->outputIndex = {5};\n",
        "  mul_node1->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  mul_node1->primitive->value.type = schema::PrimitiveType_Mul;\n",
        "  std::unique_ptr<schema::MulT> mul_attr = std::make_unique<schema::MulT>();\n",
        "  mul_node1->primitive->value.value = mul_attr.release();\n",
        "  mul_node1->name = \"mul1\";\n",
        "  meta_graph->nodes.emplace_back(std::move(mul_node1));\n",
        "\n",
        "  auto mul_node2 = std::make_unique<schema::CNodeT>();\n",
        "  mul_node2->inputIndex = {2, 4};\n",
        "  mul_node2->outputIndex = {6};\n",
        "  mul_node2->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "  mul_node2->primitive->value.type = schema::PrimitiveType_Mul;\n",
        "  std::unique_ptr<schema::MulT> mul2_attr = std::make_unique<schema::MulT>();\n",
        "  mul_node2->primitive->value.value = mul2_attr.release();\n",
        "  mul_node2->name = \"mul2\";\n",
        "  meta_graph->nodes.emplace_back(std::move(mul_node2));\n",
        "\n",
        "  // input 0: data1\n",
        "  auto input0 = std::make_unique<schema::TensorT>();\n",
        "  input0->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input0->format = schema::Format_NHWC;\n",
        "  input0->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input0->dims = {1, 2, 2, 3};\n",
        "  input0->offset = -1;\n",
        "  auto input0_data = new(std::nothrow) float[2 * 2 * 3];\n",
        "  for (auto i = 0; i < 2 * 2 * 3; i++) {\n",
        "    input0_data[i] = i;\n",
        "  }\n",
        "  input0->data.resize(sizeof(float) * 2 * 2 * 3);\n",
        "  memcpy(input0->data.data(), input0_data, 2 * 2 * 3 * sizeof(float));\n",
        "  delete[] input0_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input0));\n",
        "\n",
        "  // split output1\n",
        "  auto split_output1 = std::make_unique<schema::TensorT>();\n",
        "  split_output1->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  split_output1->format = schema::Format_NHWC;\n",
        "  split_output1->dataType = TypeId::kNumberTypeFloat32;\n",
        "  split_output1->dims = {1, 1, 2, 3};\n",
        "  split_output1->offset = -1;\n",
        "  split_output1->data.resize(sizeof(float) * 1 * 2 * 3);\n",
        "  auto split_output_data1 = new(std::nothrow) float[1 * 2 * 3];\n",
        "  memcpy(split_output1->data.data(), split_output_data1, 1 * 2 * 3 * sizeof(float));\n",
        "  delete[] split_output_data1;\n",
        "  meta_graph->allTensors.emplace_back(std::move(split_output1));\n",
        "\n",
        "  // split output2\n",
        "  auto split_output2 = std::make_unique<schema::TensorT>();\n",
        "  split_output2->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  split_output2->format = schema::Format_NHWC;\n",
        "  split_output2->dataType = TypeId::kNumberTypeFloat32;\n",
        "  split_output2->dims = {1, 1, 2, 3};\n",
        "  split_output2->offset = -1;\n",
        "  split_output2->data.resize(sizeof(float) * 1 * 2 * 3);\n",
        "  auto split_output_data2 = new(std::nothrow) float[1 * 2 * 3];\n",
        "  memcpy(split_output2->data.data(), split_output_data2, 1 * 2 * 3 * sizeof(float));\n",
        "  delete[] split_output_data2;\n",
        "  meta_graph->allTensors.emplace_back(std::move(split_output2));\n",
        "\n",
        "  // input 1: data2\n",
        "  auto input1 = std::make_unique<schema::TensorT>();\n",
        "  input1->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input1->format = schema::Format_NHWC;\n",
        "  input1->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input1->dims = {1, 1, 2, 3};\n",
        "  input1->offset = -1;\n",
        "  input1->data.resize(sizeof(float) * 2 * 3);\n",
        "  auto input1_data = new(std::nothrow) float[2 * 3];\n",
        "  for (auto i = 0; i < 2 * 3; i++) {\n",
        "    input1_data[i] = i;\n",
        "  }\n",
        "  memcpy(input1->data.data(), input1_data, 2 * 3 * sizeof(float));\n",
        "  delete[] input1_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input1));\n",
        "\n",
        "  // input 2: data3\n",
        "  auto input2 = std::make_unique<schema::TensorT>();\n",
        "  input2->nodeType = schema::NodeType::NodeType_ValueNode;\n",
        "  input2->format = schema::Format_NHWC;\n",
        "  input2->dataType = TypeId::kNumberTypeFloat32;\n",
        "  input2->dims = {1, 1, 2, 3};\n",
        "  input2->offset = -1;\n",
        "  input2->data.resize(sizeof(float) * 2 * 3);\n",
        "  auto input2_data = new(std::nothrow) float[2 * 3];\n",
        "  for (auto i = 0; i < 2 * 3; i++) {\n",
        "    input2_data[i] = 10;\n",
        "  }\n",
        "  memcpy(input2->data.data(), input2_data, 2 * 3 * sizeof(float));\n",
        "  delete[] input2_data;\n",
        "  meta_graph->allTensors.emplace_back(std::move(input2));\n",
        "\n",
        "  // final mul output1\n",
        "  auto mul_output = std::make_unique<schema::TensorT>();\n",
        "  mul_output->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  mul_output->format = schema::Format_NHWC;\n",
        "  mul_output->dataType = TypeId::kNumberTypeFloat32;\n",
        "  mul_output->dims = {1, 1, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(mul_output));\n",
        "\n",
        "  // final mul output2\n",
        "  auto mul_output2 = std::make_unique<schema::TensorT>();\n",
        "  mul_output2->nodeType = schema::NodeType::NodeType_Parameter;\n",
        "  mul_output2->format = schema::Format_NHWC;\n",
        "  mul_output2->dataType = TypeId::kNumberTypeFloat32;\n",
        "  mul_output2->dims = {1, 1, 2, 3};\n",
        "  meta_graph->allTensors.emplace_back(std::move(mul_output2));\n",
        "  return meta_graph;\n",
        "}\n",
        "}  //  namespace\n",
        "TEST_F(ConstantFoldingFusionTest, TestADDConstantFold) {\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Add, new schema::AddT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestMixedConstantFold) {\n",
        "  auto meta_graph = BuildMixGraph();\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestSubConstantFold) {\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Sub, new schema::SubT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestMulConstantFold) {\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Mul, new schema::MulT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestTransposeConstantFold) {\n",
        "  auto transposeT = new schema::TransposeT;\n",
        "  transposeT->perm = {3, 0, 1, 2};\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Transpose, transposeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestTileConstantFold) {\n",
        "  auto tileT = new schema::TileT;\n",
        "  tileT->multiples = {1, 2, 2, 2};\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Tile, tileT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestStridedSliceConstantFold) {\n",
        "  auto stridedSliceT = new schema::StridedSliceT;\n",
        "  stridedSliceT->begin = {1};\n",
        "  stridedSliceT->end = {3};\n",
        "  stridedSliceT->stride = {1};\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_StridedSlice, stridedSliceT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestStackConstantFold) {\n",
        "  auto stackT = new schema::StackT;\n",
        "  stackT->axis = 1;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Stack, stackT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestSliceConstantFold) {\n",
        "  auto sliceT = new schema::SliceT;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Slice, sliceT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestShapeConstantFold) {\n",
        "  auto shapeT = new schema::ShapeT;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Shape, shapeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestRsqrtConstantFold) {\n",
        "  auto rsqrtT = new schema::RsqrtT;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Rsqrt, rsqrtT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestReshapeConstantFold) {\n",
        "  auto reshapeT = new schema::ReshapeT;\n",
        "  reshapeT->shape = {2, 6};\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Reshape, reshapeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestRangeConstantFold) {\n",
        "  auto rangeT = new schema::RangeT;\n",
        "  rangeT->limit = 10;\n",
        "  rangeT->start = 1;\n",
        "  rangeT->delta = 1;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Range, rangeT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "TEST_F(ConstantFoldingFusionTest, TestMatmulConstantFold) {\n",
        "  auto matmulT = new schema::MatMulT;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_MatMul, matmulT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestExpandDimsConstantFold) {\n",
        "  auto expandDimsT = new schema::ExpandDimsT;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_ExpandDims, expandDimsT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestConcatDimsConstantFold) {\n",
        "  auto concatT = new schema::ConcatT;\n",
        "  auto meta_graph = BuildGraph(schema::PrimitiveType_Concat, concatT);\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestCastDimsConstantFold) {\n",
        "  auto castT = new schema::CastT;\n",
        "  castT->srcT = kNumberTypeUInt8;\n",
        "  castT->dstT = kNumberTypeFloat32;\n",
        "  auto meta_graph = BuildGraphForOneInput(schema::PrimitiveType_Cast, castT);\n",
        "  auto input_tensor = meta_graph->allTensors.at(0).get();\n",
        "  input_tensor->dataType = kNumberTypeUInt8;\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>();\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "\n",
        "TEST_F(ConstantFoldingFusionTest, TestSplitConstantFold) {\n",
        "  auto meta_graph = BuildSplitGraph();\n",
        "  auto input_tensor = meta_graph->allTensors.at(0).get();\n",
        "  input_tensor->dataType = kNumberTypeFloat32;\n",
        "  auto func_graph = lite::ModelParser::Fb2Anf(meta_graph.get());\n",
        "  auto optimizer = std::make_shared<opt::GraphOptimizer>();\n",
        "  auto pm = std::make_shared<opt::PassManager>(\"test\", false);\n",
        "  pm->AddPass(std::make_shared<opt::ConstFoldPass>());\n",
        "  optimizer->AddPassManager(pm);\n",
        "  FuncGraphPtr new_graph = optimizer->Optimize(func_graph);\n",
        "  ASSERT_NE(nullptr, new_graph);\n",
        "  auto new_meta_graph = lite::Export(new_graph);\n",
        "  ASSERT_EQ(new_meta_graph->nodes.size(), 0);\n",
        "}\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIWO5XRfhbXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals) / len(vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVuWr1Zqq_Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *\n",
        "import tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGU4PDAsNQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ./input/roberta-base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B24P1lcrlmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = './input/roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "config = RobertaConfig.from_pretrained('roberta-base')\n",
        "tokenizer.save_vocabulary(save_path)\n",
        "model.save_pretrained(save_path)\n",
        "config.save_pretrained(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWGS_5Sfsenj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class config:\n",
        "    FOLD = 0\n",
        "    LEARNING_RATE = 0.2 * 3e-5\n",
        "    MAX_LEN = 192\n",
        "    TRAIN_BATCH_SIZE = 16\n",
        "    VALID_BATCH_SIZE = 8\n",
        "    EPOCHS = 3\n",
        "    TRAINING_FILE = \"./tweet-sentiment/train_folds.csv\"\n",
        "    ROBERTA_PATH = \"./input/roberta-base\"\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
        "        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
        "        lowercase=True,\n",
        "        add_prefix_space=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk",
        "colab_type": "text"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nl8SqDItPsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "    \n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "    \n",
        "    targets_start = target_idx[0]\n",
        "    targets_end = target_idx[-1]\n",
        "\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1XkJ_uOtk5U",
        "colab_type": "text"
      },
      "source": [
        "#Data loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOidlXY8uO8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item], \n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1tiaim0uWB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n",
        "        self.drop_out = nn.Dropout(0.1)\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "        out = self.drop_out(out)\n",
        "        logits = self.l0(out)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_D9iCOPueGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EJr5IHPuji1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n",
        "    model.train()\n",
        "    tk0 = tqdm(data_loader, total=num_batches, desc=\"Training\", disable=not xm.is_master_ordinal())\n",
        "    for bi, d in enumerate(tk0):\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        sentiment = d[\"sentiment\"]\n",
        "        orig_selected = d[\"orig_selected\"]\n",
        "        orig_tweet = d[\"orig_tweet\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        offsets = d[\"offsets\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets_start = targets_start.to(device, dtype=torch.long)\n",
        "        targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs_start, outputs_end = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        scheduler.step()\n",
        "        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n",
        "        tk0.set_postfix(loss=print_loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDxCSB91unsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    jaccards = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for bi, d in enumerate(data_loader):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            sentiment = d[\"sentiment\"]\n",
        "            orig_selected = d[\"orig_selected\"]\n",
        "            orig_tweet = d[\"orig_tweet\"]\n",
        "            targets_start = d[\"targets_start\"]\n",
        "            targets_end = d[\"targets_end\"]\n",
        "            offsets = d[\"offsets\"].cpu().numpy()\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets_start = targets_start.to(device, dtype=torch.long)\n",
        "            targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                tweet_sentiment = sentiment[px]\n",
        "                jaccard_score, _ = calculate_jaccard_score(\n",
        "                    original_tweet=tweet,\n",
        "                    target_string=selected_tweet,\n",
        "                    sentiment_val=tweet_sentiment,\n",
        "                    idx_start=np.argmax(outputs_start[px, :]),\n",
        "                    idx_end=np.argmax(outputs_end[px, :]),\n",
        "                    offsets=offsets[px]\n",
        "                )\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            losses.update(loss.item(), ids.size(0))\n",
        "\n",
        "    return jaccards.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb7KafbguwPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n",
        "model_config.output_hidden_states = True\n",
        "MX = TweetModel(conf=model_config)\n",
        "\n",
        "dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "\n",
        "df_train = dfx[dfx.kfold != config.FOLD].reset_index(drop=True)\n",
        "df_valid = dfx[dfx.kfold == config.FOLD].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehNC95DRu3EL",
        "colab_type": "text"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqDrsrE2u1oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run():\n",
        "    device = xm.xla_device()\n",
        "    model = MX.to(device)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      train_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=True\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      valid_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=False\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=False,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\n",
        "        \"bias\",\n",
        "        \"LayerNorm.bias\",\n",
        "        \"LayerNorm.weight\"\n",
        "    ]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if not any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "         'weight_decay': 0.001\n",
        "        },\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "            'weight_decay': 0.0\n",
        "        },\n",
        "    ]\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE / xm.xrt_world_size() * config.EPOCHS\n",
        "    )\n",
        "    optimizer = AdamW(\n",
        "        optimizer_parameters, \n",
        "        lr=config.LEARNING_RATE * xm.xrt_world_size()\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_jac = 0\n",
        "    es = EarlyStopping(patience=2, mode=\"max\")\n",
        "    num_batches = int(len(df_train) / (config.TRAIN_BATCH_SIZE * xm.xrt_world_size()))\n",
        "    \n",
        "    xm.master_print(\"Training is Starting....\")\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "        train_fn(\n",
        "            para_loader.per_device_loader(device), \n",
        "            model, \n",
        "            optimizer, \n",
        "            device,\n",
        "            num_batches,\n",
        "            scheduler\n",
        "        )\n",
        "\n",
        "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "        jac = eval_fn(\n",
        "            para_loader.per_device_loader(device), \n",
        "            model, \n",
        "            device\n",
        "        )\n",
        "        jac = xm.mesh_reduce('jac_reduce', jac, reduce_fn)\n",
        "        xm.master_print(f'Epoch={epoch}, Jaccard={jac}')\n",
        "        if jac > best_jac:\n",
        "            xm.master_print(\"Model Improved!!! Saving Model\")\n",
        "            xm.save(model.state_dict(), f\"model_{config.FOLD}.bin\")\n",
        "            best_jac = jac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHlyHRTVvEwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    a = run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99O6yUgLvHgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "18a4343c030b4371b899e99d4f8c513c",
            "97212f9b24d549d08e65ef631c788e5e",
            "2c9b0b798db64f1298760c85a57f097c",
            "e3fe3c2345634b088bff27a09a10b640",
            "33c8818e4b764cdcba3e4b6f52f1dbea",
            "94e37f508ab34e898ec927961919de34",
            "49f8aa1ae9f84e4b9a2c5177bc74ea4b",
            "c95bbed6e4ba46b18898146970ea9de6"
          ]
        },
        "outputId": "30bc90da-0640-45d1-8873-5724f16c8935"
      },
      "source": [
        "FLAGS={}\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training is Starting....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18a4343c030b4371b899e99d4f8c513c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Training', max=171.0, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}