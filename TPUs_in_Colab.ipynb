{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpSKCqncaMk6"
      },
      "source": [
        "diff --git a/mindspore/lite/examples/export_models/models/densenet_train_export.py b/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "index 20bd76f..df785e8 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "@@ -17,14 +17,14 @@\n",
        " import sys\n",
        " import os\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        " from mindspore.train.serialization import export\n",
        "-\n",
        "+from official.cv.densenet121.src.network.densenet import DenseNet121\n",
        " sys.path.append(os.environ['CLOUD_MODEL_ZOO'] + 'official/cv/densenet121/')\n",
        " #pylint: disable=wrong-import-position\n",
        "-from official.cv.densenet121.src.network.densenet import DenseNet121\n",
        "+\n",
        " \n",
        " \n",
        " \n",
        "@@ -35,7 +35,7 @@ n = DenseNet121(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.001, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=0.9)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -43,4 +43,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/densenet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"densenet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"densenet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/effnet.py b/mindspore/lite/examples/export_models/models/effnet.py\n",
        "index 4971757..fc49872 100755\n",
        "--- a/mindspore/lite/examples/export_models/models/effnet.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/effnet.py\n",
        "@@ -20,6 +20,7 @@ from mindspore.ops import operations as P\n",
        " from mindspore.common.initializer import TruncatedNormal\n",
        " from mindspore import Tensor\n",
        " \n",
        "+\n",
        " def weight_variable():\n",
        "     \"\"\"weight initial\"\"\"\n",
        "     return TruncatedNormal(0.02)\n",
        "@@ -40,6 +41,7 @@ def _make_value_divisible(value, factor, min_value=None):\n",
        "         new_value += factor\n",
        "     return new_value\n",
        " \n",
        "+\n",
        " class Swish(nn.Cell):\n",
        "     def __init__(self):\n",
        "         super().__init__()\n",
        "@@ -58,7 +60,8 @@ class AdaptiveAvgPool(nn.Cell):\n",
        "         self.output_size = output_size\n",
        " \n",
        "     def construct(self, x):\n",
        "-        return self.mean(x, (2, 3)) ## This is not a general case\n",
        "+        return self.mean(x, (2, 3)) # This is not a general case\n",
        "+\n",
        " \n",
        " class SELayer(nn.Cell):\n",
        "     \"\"\"SELayer\"\"\"\n",
        "@@ -74,24 +77,27 @@ class SELayer(nn.Cell):\n",
        "         self.act2 = nn.Sigmoid()\n",
        " \n",
        "     def construct(self, x):\n",
        "-        o = self.avg_pool(x) #.view(b,c)\n",
        "+        o = self.avg_pool(x) # .view(b,c)\n",
        "         o = self.conv_reduce(o)\n",
        "         o = self.act1(o)\n",
        "         o = self.conv_expand(o)\n",
        "-        o = self.act2(o) #.view(b, c, 1,1)\n",
        "+        o = self.act2(o) # .view(b, c, 1,1)\n",
        "         return x * o\n",
        " \n",
        "+\n",
        " class DepthwiseSeparableConv(nn.Cell):\n",
        "     \"\"\"DepthwiseSeparableConv\"\"\"\n",
        "     def __init__(self, in_chs, out_chs, dw_kernel_size=3, stride=1, noskip=False, se_ratio=0.0, drop_connect_rate=0.0):\n",
        "         super().__init__()\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR stride param\")\n",
        "+            return\n",
        "         self.has_residual = (stride == 1 and in_chs == out_chs) and not noskip\n",
        "         self.drop_connect_rate = drop_connect_rate\n",
        " \n",
        "         self.conv_dw = nn.Conv2d(in_channels=in_chs, out_channels=in_chs, kernel_size=dw_kernel_size, stride=stride,\n",
        "                                  pad_mode=\"pad\", padding=1, has_bias=False, group=in_chs)\n",
        "-        self.bn1 = nn.BatchNorm2d(in_chs, eps=0.001) #,momentum=0.1)\n",
        "+        self.bn1 = nn.BatchNorm2d(in_chs, eps=0.001) # momentum=0.1)\n",
        "         self.act1 = Swish()\n",
        " \n",
        "        # Squeeze-and-excitation\n",
        "@@ -101,7 +107,7 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "             print(\"ERRRRRORRRR -- not prepared for this one\\n\")\n",
        " \n",
        "         self.conv_pw = nn.Conv2d(in_channels=in_chs, out_channels=out_chs, kernel_size=1, stride=stride, has_bias=False)\n",
        "-        self.bn2 = nn.BatchNorm2d(out_chs, eps=0.001) #,momentum=0.1)\n",
        "+        self.bn2 = nn.BatchNorm2d(out_chs, eps=0.001) # momentum=0.1)\n",
        " \n",
        "     def construct(self, x):\n",
        "         \"\"\"construct\"\"\"\n",
        "@@ -120,12 +126,13 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "             x += residual\n",
        "         return x\n",
        " \n",
        "+\n",
        " def conv_3x3_bn(inp, oup, stride):\n",
        "     weight = weight_variable()\n",
        "     return nn.SequentialCell([\n",
        "         nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=3, stride=stride, padding=1, weight_init=weight,\n",
        "                   has_bias=False, pad_mode='pad'),\n",
        "-        nn.BatchNorm2d(oup, eps=0.001),  #, momentum=0.1),\n",
        "+        nn.BatchNorm2d(oup, eps=0.001),  # momentum=0.1),\n",
        "         nn.HSwish()])\n",
        " \n",
        " \n",
        "@@ -142,7 +149,9 @@ class InvertedResidual(nn.Cell):\n",
        "     \"\"\"InvertedResidual\"\"\"\n",
        "     def __init__(self, in_chs, out_chs, kernel_size, stride, padding, expansion, se_ratio):\n",
        "         super().__init__()\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR stride param\")\n",
        "+            return\n",
        "         mid_chs: int = _make_value_divisible(in_chs * expansion, 1)\n",
        "         self.has_residual = (in_chs == out_chs and stride == 1)\n",
        "         self.drop_connect_rate = 0\n",
        "@@ -210,7 +219,7 @@ class EfficientNet(nn.Cell):\n",
        " \n",
        "         self.conv_stem = nn.Conv2d(in_channels=3, out_channels=stem_size, kernel_size=3, stride=2, has_bias=False)\n",
        " \n",
        "-        self.bn1 = nn.BatchNorm2d(stem_size, eps=0.001) #momentum=0.1)\n",
        "+        self.bn1 = nn.BatchNorm2d(stem_size, eps=0.001) # momentum=0.1)\n",
        "         self.act1 = Swish()\n",
        "         in_chs = stem_size\n",
        " \n",
        "@@ -240,7 +249,7 @@ class EfficientNet(nn.Cell):\n",
        "         self.blocks = nn.SequentialCell(layers)\n",
        " \n",
        "         self.conv_head = nn.Conv2d(in_channels=320, out_channels=self.num_features_, kernel_size=1)\n",
        "-        self.bn2 = nn.BatchNorm2d(self.num_features_, eps=0.001) #,momentum=0.1)\n",
        "+        self.bn2 = nn.BatchNorm2d(self.num_features_, eps=0.001) # momentum=0.1)\n",
        "         self.act2 = Swish()\n",
        "         self.global_pool = AdaptiveAvgPool(output_size=(1, 1))\n",
        "         self.classifier = nn.Dense(self.num_features_, num_classes)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/effnet_train_export.py b/mindspore/lite/examples/export_models/models/effnet_train_export.py\n",
        "index bf341f2..3384cc2 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/effnet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/effnet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from effnet import effnet\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,11 +28,11 @@ n = effnet(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(2, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([2, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/effnet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"effnet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"effnet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py b/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py\n",
        "index 2b21ee8..3e61b44 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py\n",
        "@@ -17,7 +17,7 @@\n",
        " import sys\n",
        " from os import path\n",
        " import numpy as np\n",
        "-from train_utils import TrainWrap, SaveT\n",
        "+from train_utils import train_wrap, save_t\n",
        " from effnet import effnet\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -26,11 +26,13 @@ from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        " context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\", save_graphs=False)\n",
        " \n",
        "+\n",
        " class TransferNet(nn.Cell):\n",
        "     def __init__(self, backbone, head):\n",
        "         super().__init__(TransferNet)\n",
        "         self.backbone = backbone\n",
        "         self.head = head\n",
        "+\n",
        "     def construct(self, x):\n",
        "         x = self.backbone(x)\n",
        "         x = self.head(x)\n",
        "@@ -56,7 +58,7 @@ trainable_weights_list.extend(n.head.trainable_params())\n",
        " trainable_weights = ParameterTuple(trainable_weights_list)\n",
        " sgd = nn.SGD(trainable_weights, learning_rate=0.01, momentum=0.9,\n",
        "              dampening=0.01, weight_decay=0.0, nesterov=False, loss_scale=1.0)\n",
        "-net = TrainWrap(n, optimizer=sgd, weights=trainable_weights)\n",
        "+net = train_wrap(n, optimizer=sgd, weights=trainable_weights)\n",
        " \n",
        " BATCH_SIZE = 8\n",
        " X = Tensor(np.random.randn(BATCH_SIZE, 3, 224, 224), mstype.float32)\n",
        "@@ -66,10 +68,10 @@ export(net, X, label, file_name=\"mindir/effnet_tune_train\", file_format='MINDIR'\n",
        " if len(sys.argv) > 1:\n",
        "     name_prefix = sys.argv[1] + \"effnet_tune\"\n",
        "     x_name = name_prefix + \"_input1.bin\"\n",
        "-    SaveT(Tensor(X.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        "+    save_t(Tensor(X.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        " \n",
        "     l_name = name_prefix + \"_input2.bin\"\n",
        "-    SaveT(label, l_name)\n",
        "+    save_t(label, l_name)\n",
        " \n",
        "     #train network\n",
        "     n.head.set_train(True)\n",
        "@@ -80,4 +82,4 @@ if len(sys.argv) > 1:\n",
        "     n.set_train(False)\n",
        "     y = n(X)\n",
        "     y_name = name_prefix + \"_output1.bin\"\n",
        "-    SaveT(y, y_name)\n",
        "+    save_t(y, y_name)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/googlenet_train_export.py b/mindspore/lite/examples/export_models/models/googlenet_train_export.py\n",
        "index c2ddcc2..91a0062 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/googlenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/googlenet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.googlenet.src.googlenet import GoogleNet\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = GoogleNet(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=5e-4,\n",
        "                    nesterov=True, loss_scale=0.9)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/googlenet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"googlenet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"googlenet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/lenet_train_export.py b/mindspore/lite/examples/export_models/models/lenet_train_export.py\n",
        "index 1b7dfda..4e03aab 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/lenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/lenet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.lenet.src.lenet import LeNet5\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,11 +28,11 @@ n = LeNet5()\n",
        " loss_fn = nn.MSELoss()\n",
        " optimizer = nn.Adam(n.trainable_params(), learning_rate=1e-2, beta1=0.5, beta2=0.7, eps=1e-2, use_locking=True,\n",
        "                     use_nesterov=False, weight_decay=0.0, loss_scale=0.3)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(32, 1, 32, 32), mstype.float32)\n",
        " label = Tensor(np.zeros([32, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/lenet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"lenet\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"lenet\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mini_alexnet.py b/mindspore/lite/examples/export_models/models/mini_alexnet.py\n",
        "index 9a8b828..e6008fa 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mini_alexnet.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mini_alexnet.py\n",
        "@@ -17,13 +17,16 @@\n",
        " import mindspore.nn as nn\n",
        " from mindspore.ops import operations as P\n",
        " \n",
        "+\n",
        " def conv(in_channels, out_channels, kernel_size, stride=1, padding=0, pad_mode=\"valid\", has_bias=True):\n",
        "     return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                      has_bias=has_bias, pad_mode=pad_mode)\n",
        " \n",
        "+\n",
        " def fc_with_initialize(input_channels, out_channels, has_bias=True):\n",
        "     return nn.Dense(input_channels, out_channels, has_bias=has_bias)\n",
        " \n",
        "+\n",
        " class AlexNet(nn.Cell):\n",
        "     \"\"\"\n",
        "     Alexnet\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py b/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py\n",
        "index 544daad..1b9a82d 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from mini_alexnet import AlexNet\n",
        " from mindspore import context, Tensor, nn\n",
        " from mindspore.train.serialization import export\n",
        "@@ -31,11 +31,11 @@ n = AlexNet(phase='test')\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Adam(n.trainable_params(), learning_rate=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, use_locking=False,\n",
        "                     use_nesterov=False, weight_decay=0.0, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.ones([batch, 1, 32, 32]).astype(np.float32) * 0.01)\n",
        " label = Tensor(np.zeros([batch, number_of_classes]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mini_alexnet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mini_alexnet\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"mini_alexnet\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py b/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py\n",
        "index f668a96..b3b26de 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.mobilenetv1.src.mobilenet_v1 import MobileNetV1\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = MobileNetV1(10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=1e-2, momentum=0.9, dampening=0.1, weight_decay=0.0,\n",
        "                    nesterov=False, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -37,4 +37,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mobilenetv1_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mobilenetv1\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"mobilenetv1\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py b/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py\n",
        "index 8f1d543..0433063 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.mobilenetv2.src.mobilenetV2 import MobileNetV2Backbone, MobileNetV2Head, mobilenet_v2\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -31,11 +31,11 @@ n = mobilenet_v2(backbone_net, head_net)\n",
        " \n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Momentum(n.trainable_params(), 0.01, 0.9, use_nesterov=False)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mobilenetv2_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mobilenetv2\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"mobilenetv2\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py b/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py\n",
        "index 29d618d..f6743b6 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.mobilenetv3.src.mobilenetV3 import mobilenet_v3_small\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = mobilenet_v3_small(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')\n",
        " optimizer = nn.Adam(n.trainable_params(), learning_rate=1e-2, beta1=0.5, beta2=0.7, eps=1e-2, use_locking=True,\n",
        "                     use_nesterov=False, weight_decay=0.1, loss_scale=0.3)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mobilenetv3_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mobilenetv3\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"mobilenetv3\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/nin_train_export.py b/mindspore/lite/examples/export_models/models/nin_train_export.py\n",
        "index 72ccc5e..786f739 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/nin_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/nin_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from NetworkInNetwork import NiN\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = NiN(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=5e-4,\n",
        "                    nesterov=True, loss_scale=0.9)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 32, 32), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch]).astype(np.int32))\n",
        " export(net, x, label, file_name=\"mindir/nin_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"nin\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"nin\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/resnet_train_export.py b/mindspore/lite/examples/export_models/models/resnet_train_export.py\n",
        "index c0dbe90..c18bcf3 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/resnet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/resnet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.resnet.src.resnet import resnet50\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -29,11 +29,11 @@ n = resnet50(class_num=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/resnet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"resnet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"resnet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py b/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py\n",
        "index 97aa4ec..bf76d48 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.shufflenetv2.src.shufflenetv2 import ShuffleNetV2\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = ShuffleNetV2(n_class=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Momentum(n.trainable_params(), 0.01, 0.9, use_nesterov=False)\n",
        " \n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/shufflenetv2_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"shufflenetv2\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"shufflenetv2\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/train_utils.py b/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "index e32fda1..2fa6690 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "@@ -16,9 +16,11 @@\n",
        " \n",
        " from mindspore import nn, Tensor\n",
        " from mindspore.common.parameter import ParameterTuple\n",
        "+import os\n",
        " \n",
        "-def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "-    \"\"\"TrainWrap\"\"\"\n",
        "+\n",
        "+def train_wrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "+    \"\"\"train_wrap\"\"\"\n",
        "     if loss_fn is None:\n",
        "         loss_fn = nn.SoftmaxCrossEntropyWithLogits()\n",
        "     loss_net = nn.WithLossCell(net, loss_fn)\n",
        "@@ -32,22 +34,22 @@ def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "     return train_net\n",
        " \n",
        " \n",
        "-def SaveT(t, file):\n",
        "+def save_t(t, file):\n",
        "     x = t.asnumpy()\n",
        "     x.tofile(file)\n",
        " \n",
        " \n",
        "-def SaveInOut(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "-    \"\"\"SaveInOut\"\"\"\n",
        "+def save_inout(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "+    \"\"\"save_inout\"\"\"\n",
        "     x_name = name + \"_input1.bin\"\n",
        "     if sparse:\n",
        "         x_name = name + \"_input2.bin\"\n",
        "-    SaveT(Tensor(x.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        "+    save_t(Tensor(x.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        " \n",
        "     l_name = name + \"_input2.bin\"\n",
        "     if sparse:\n",
        "         l_name = name + \"_input1.bin\"\n",
        "-    SaveT(l, l_name)\n",
        "+    save_t(l, l_name)\n",
        " \n",
        "     net.set_train(False)\n",
        "     y = net(x)\n",
        "@@ -62,10 +64,10 @@ def SaveInOut(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "     if isinstance(y, tuple):\n",
        "         i = 1\n",
        "         for t in y:\n",
        "-            with open(name + \"_output\" + str(i) + \".bin\", 'w') as f:\n",
        "+            with os.fdopen(name + \"_output\" + str(i) + \".bin\", 'w') as f:\n",
        "                 for j in t.asnumpy().flatten():\n",
        "                     f.write(str(j)+' ')\n",
        "             i = i + 1\n",
        "     else:\n",
        "         y_name = name + \"_output1.bin\"\n",
        "-        SaveT(y, y_name)\n",
        "+        save_t(y, y_name)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/vgg_train_export.py b/mindspore/lite/examples/export_models/models/vgg_train_export.py\n",
        "index 0078252..c18b33c 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/vgg_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/vgg_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.vgg16.src.vgg import vgg16\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -29,11 +29,11 @@ batch = 2\n",
        " n = vgg16(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Momentum(n.trainable_params(), 0.01, 0.9, use_nesterov=False)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/vgg_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"vgg\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"vgg\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/xception_train_export.py b/mindspore/lite/examples/export_models/models/xception_train_export.py\n",
        "index 6b82b3b..e544d7e 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/xception_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/xception_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.xception.src.Xception import Xception\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -31,7 +31,7 @@ n.dropout = nn.Dropout(keep_prob=1.0)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 299, 299), mstype.float32)\n",
        "@@ -39,4 +39,4 @@ label = Tensor(np.zeros([batch, 1000]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/xception_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"xception\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"xception\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/train_lenet/model/lenet_export.py b/mindspore/lite/examples/train_lenet/model/lenet_export.py\n",
        "index 8a9cd7c..c774887 100644\n",
        "--- a/mindspore/lite/examples/train_lenet/model/lenet_export.py\n",
        "+++ b/mindspore/lite/examples/train_lenet/model/lenet_export.py\n",
        "@@ -19,7 +19,7 @@ from mindspore import context, Tensor\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore.train.serialization import export\n",
        " from lenet import LeNet5\n",
        "-from train_utils import TrainWrap\n",
        "+from train_utils import train_wrap\n",
        " \n",
        " n = LeNet5()\n",
        " n.set_train()\n",
        "@@ -28,7 +28,7 @@ context.set_context(mode=context.PYNATIVE_MODE, device_target=\"CPU\", save_graphs\n",
        " BATCH_SIZE = 32\n",
        " x = Tensor(np.ones((BATCH_SIZE, 1, 32, 32)), mstype.float32)\n",
        " label = Tensor(np.zeros([BATCH_SIZE]).astype(np.int32))\n",
        "-net = TrainWrap(n)\n",
        "+net = train_wrap(n)\n",
        " export(net, x, label, file_name=\"lenet_tod\", file_format='MINDIR')\n",
        " \n",
        " print(\"finished exporting\")\n",
        "diff --git a/mindspore/lite/examples/train_lenet/model/train_utils.py b/mindspore/lite/examples/train_lenet/model/train_utils.py\n",
        "index 9e8e3fa..9e3ad76 100644\n",
        "--- a/mindspore/lite/examples/train_lenet/model/train_utils.py\n",
        "+++ b/mindspore/lite/examples/train_lenet/model/train_utils.py\n",
        "@@ -17,9 +17,10 @@\n",
        " import mindspore.nn as nn\n",
        " from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        "-def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "+\n",
        "+def train_wrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "     \"\"\"\n",
        "-    TrainWrap\n",
        "+    train_wrap\n",
        "     \"\"\"\n",
        "     if loss_fn is None:\n",
        "         loss_fn = nn.SoftmaxCrossEntropyWithLogits(reduction='mean', sparse=True)\n",
        "diff --git a/mindspore/lite/examples/transfer_learning/model/effnet.py b/mindspore/lite/examples/transfer_learning/model/effnet.py\n",
        "index 8ed066f..eba29b5 100755\n",
        "--- a/mindspore/lite/examples/transfer_learning/model/effnet.py\n",
        "+++ b/mindspore/lite/examples/transfer_learning/model/effnet.py\n",
        "@@ -44,6 +44,7 @@ class Swish(nn.Cell):\n",
        "         m = x*s\n",
        "         return m\n",
        " \n",
        "+\n",
        " class AdaptiveAvgPool(nn.Cell):\n",
        "     def __init__(self, output_size=None):\n",
        "         super().__init__(AdaptiveAvgPool)\n",
        "@@ -53,6 +54,7 @@ class AdaptiveAvgPool(nn.Cell):\n",
        "     def construct(self, x):\n",
        "         return self.mean(x, (2, 3))\n",
        " \n",
        "+\n",
        " class SELayer(nn.Cell):\n",
        "     \"\"\"\n",
        "     SELayer\n",
        "@@ -77,6 +79,7 @@ class SELayer(nn.Cell):\n",
        "         o = self.act2(o)\n",
        "         return x * o\n",
        " \n",
        "+\n",
        " class DepthwiseSeparableConv(nn.Cell):\n",
        "     \"\"\"\n",
        "     DepthwiseSeparableConv\n",
        "@@ -84,7 +87,9 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "     def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n",
        "                  stride=1, noskip=False, se_ratio=0.0, drop_connect_rate=0.0):\n",
        "         super().__init__(DepthwiseSeparableConv)\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR\")\n",
        "+            return\n",
        "         self.has_residual = (stride == 1 and in_chs == out_chs) and not noskip\n",
        "         self.drop_connect_rate = drop_connect_rate\n",
        " \n",
        "@@ -117,6 +122,7 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "             x += residual\n",
        "         return x\n",
        " \n",
        "+\n",
        " def conv_3x3_bn(inp, oup, stride):\n",
        "     weight = weight_variable()\n",
        "     return nn.SequentialCell([\n",
        "@@ -125,6 +131,7 @@ def conv_3x3_bn(inp, oup, stride):\n",
        "         nn.BatchNorm2d(oup, eps=0.001),  # , momentum=0.1),\n",
        "         nn.HSwish()])\n",
        " \n",
        "+\n",
        " def conv_1x1_bn(inp, oup):\n",
        "     weight = weight_variable()\n",
        "     return nn.SequentialCell([\n",
        "@@ -133,13 +140,16 @@ def conv_1x1_bn(inp, oup):\n",
        "         nn.BatchNorm2d(oup, eps=0.001),\n",
        "         nn.HSwish()])\n",
        " \n",
        "+\n",
        " class InvertedResidual(nn.Cell):\n",
        "     \"\"\"\n",
        "     InvertedResidual\n",
        "     \"\"\"\n",
        "     def __init__(self, in_chs, out_chs, kernel_size, stride, padding, expansion, se_ratio):\n",
        "         super().__init__(InvertedResidual)\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR\")\n",
        "+            return\n",
        "         mid_chs: int = _make_divisible(in_chs * expansion, 1)\n",
        "         self.has_residual = (in_chs == out_chs and stride == 1)\n",
        "         self.drop_connect_rate = 0\n",
        "@@ -194,6 +204,7 @@ class InvertedResidual(nn.Cell):\n",
        "             x += residual\n",
        "         return x\n",
        " \n",
        "+\n",
        " class EfficientNet(nn.Cell):\n",
        "     \"\"\"\n",
        "     EfficientNet\n",
        "@@ -295,6 +306,7 @@ class EfficientNet(nn.Cell):\n",
        "             elif isinstance(m, nn.Dense):\n",
        "                 init_linear_weight(m)\n",
        " \n",
        "+\n",
        " def effnet(**kwargs):\n",
        "     \"\"\"\n",
        "     Constructs a EfficientNet model\n",
        "diff --git a/mindspore/lite/include/train/accuracy_metrics.h b/mindspore/lite/include/train/accuracy_metrics.h\n",
        "index e3822fd..9dfa451 100644\n",
        "--- a/mindspore/lite/include/train/accuracy_metrics.h\n",
        "+++ b/mindspore/lite/include/train/accuracy_metrics.h\n",
        "@@ -41,6 +41,7 @@ class AccuracyMetrics : public Metrics {\n",
        "   std::vector<int> output_indexes_ = {0};\n",
        "   float total_accuracy_ = 0.0;\n",
        "   float total_steps_ = 0.0;\n",
        "+  friend class ClassificationTrainAccuracyMonitor;\n",
        " };\n",
        " \n",
        " }  // namespace lite\n",
        "diff --git a/mindspore/lite/include/train/classification_train_accuracy_monitor.h b/mindspore/lite/include/train/classification_train_accuracy_monitor.h\n",
        "index 5c85592..0c461c5 100644\n",
        "--- a/mindspore/lite/include/train/classification_train_accuracy_monitor.h\n",
        "+++ b/mindspore/lite/include/train/classification_train_accuracy_monitor.h\n",
        "@@ -44,9 +44,7 @@ class ClassificationTrainAccuracyMonitor : public session::TrainLoopCallBack {\n",
        " \n",
        "  private:\n",
        "   std::vector<GraphPoint> accuracies_;\n",
        "-  int accuracy_metrics_ = METRICS_CLASSIFICATION;\n",
        "-  std::vector<int> input_indexes_ = {1};\n",
        "-  std::vector<int> output_indexes_ = {0};\n",
        "+  std::shared_ptr<AccuracyMetrics> accuracy_metrics_;\n",
        "   int print_every_n_ = 0;\n",
        " };\n",
        " \n",
        "diff --git a/mindspore/lite/src/dequant.h b/mindspore/lite/src/dequant.h\n",
        "index 919b388..1e554c9 100644\n",
        "--- a/mindspore/lite/src/dequant.h\n",
        "+++ b/mindspore/lite/src/dequant.h\n",
        "@@ -40,19 +40,9 @@ class DequantUtil {\n",
        "   static void RestoreTensorData(const std::map<Tensor *, std::pair<TypeId, void *>> &tensor_origin_data_map);\n",
        " \n",
        "   template <typename ST, typename DT = float>\n",
        "-  static DT *DequantData(lite::Tensor *input_tensor, bool channel_first = true) {\n",
        "-    const auto *quant_datas = static_cast<const ST *>(input_tensor->MutableData());\n",
        "-    if (quant_datas == nullptr) {\n",
        "-      MS_LOG(ERROR) << \"Get quant tensor failed.\";\n",
        "-      return nullptr;\n",
        "-    }\n",
        "-    DT *dequant_datas = static_cast<DT *>(malloc(input_tensor->ElementsNum() * sizeof(DT)));\n",
        "-    if (dequant_datas == nullptr) {\n",
        "-      MS_LOG(ERROR) << \"Malloc failed.\";\n",
        "-      return nullptr;\n",
        "-    }\n",
        "+  static bool IsPerBatch(lite::Tensor *input_tensor,ST * quant_datas,DT *dequant_datas) {\n",
        "     if (input_tensor->shape().size() == kPerBatch &&\n",
        "-        input_tensor->quant_params().size() == static_cast<size_t>(input_tensor->shape().at(0))) {  // per batch matmul\n",
        "+      input_tensor->quant_params().size() == static_cast<size_t>(input_tensor->shape().at(0))) {  // per batch matmul\n",
        "       auto per_batch_size = input_tensor->shape().at(0);\n",
        "       auto quant_param = input_tensor->quant_params();\n",
        "       for (int i = 0; i < per_batch_size; i++) {\n",
        "@@ -64,42 +54,63 @@ class DequantUtil {\n",
        "           dequant_datas[i * matrix_size + j] = static_cast<DT>((quant_datas[i * matrix_size + j] - zero_point) * scale);\n",
        "         }\n",
        "       }\n",
        "-    } else if (input_tensor->quant_params().size() != kPerTensor) {\n",
        "-      auto channels = static_cast<size_t>(input_tensor->Batch());\n",
        "-      if (!channel_first) {\n",
        "-        if (input_tensor->shape().size() != 2) {\n",
        "-          MS_LOG(ERROR) << \"unexpected shape size: \" << input_tensor->shape().size();\n",
        "-          free(dequant_datas);\n",
        "-          return nullptr;\n",
        "-        }\n",
        "-        channels = input_tensor->shape()[1];\n",
        "-      }\n",
        "-      if (input_tensor->quant_params().size() != channels) {\n",
        "-        MS_LOG(ERROR) << \"Quant param not equal channel num \" << input_tensor->quant_params().size() << channels;\n",
        "+      return true;\n",
        "+    }\n",
        "+    return false;\n",
        "+  }\n",
        "+  template <typename ST, typename DT = float>\n",
        "+  static DT * DequantPerChannel(lite::Tensor *input_tensor,ST * quant_datas,DT *dequant_datas,bool channel_first) {\n",
        "+    auto channels = static_cast<size_t>(input_tensor->Batch());\n",
        "+    if (!channel_first) {\n",
        "+      if (input_tensor->shape().size() != 2) {\n",
        "+        MS_LOG(ERROR) << \"unexpected shape size: \" << input_tensor->shape().size();\n",
        "         free(dequant_datas);\n",
        "         return nullptr;\n",
        "       }\n",
        "-      size_t per_channel_size = input_tensor->ElementsNum() / channels;\n",
        "-      auto quant_param = input_tensor->quant_params();\n",
        "-      for (size_t i = 0; i < channels; i++) {\n",
        "-        auto param = quant_param.at(i);\n",
        "-        auto scale = param.scale;\n",
        "-        auto zero_point = param.zeroPoint;\n",
        "-        auto var_corr = param.var_corr;\n",
        "-        auto mean_corr = param.mean_corr;\n",
        "-        if (var_corr < 0 || var_corr > 10) {\n",
        "-          MS_LOG(WARNING) << \"unexpected var_corr: \" << var_corr;\n",
        "-          var_corr = 1;\n",
        "-        }\n",
        "-        for (size_t j = 0; j < per_channel_size; j++) {\n",
        "-          auto index = per_channel_size * i + j;\n",
        "-          if (!channel_first) {\n",
        "-            index = channels * j + i;\n",
        "-          }\n",
        "-          auto dequant_data = (quant_datas[index] - zero_point) * scale;\n",
        "-          dequant_datas[index] = static_cast<DT>(dequant_data * var_corr + mean_corr);\n",
        "+      channels = input_tensor->shape()[1];\n",
        "+    }\n",
        "+    if (input_tensor->quant_params().size() != channels) {\n",
        "+      MS_LOG(ERROR) << \"Quant param not equal channel num \" << input_tensor->quant_params().size() << channels;\n",
        "+      free(dequant_datas);\n",
        "+      return nullptr;\n",
        "+    }\n",
        "+    size_t per_channel_size = input_tensor->ElementsNum() / channels;\n",
        "+    auto quant_param = input_tensor->quant_params();\n",
        "+    for (size_t i = 0; i < channels; i++) {\n",
        "+      auto param = quant_param.at(i);\n",
        "+      auto scale = param.scale;\n",
        "+      auto zero_point = param.zeroPoint;\n",
        "+      auto var_corr = param.var_corr;\n",
        "+      auto mean_corr = param.mean_corr;\n",
        "+      if (var_corr < 0 || var_corr > 10) {\n",
        "+        MS_LOG(WARNING) << \"unexpected var_corr: \" << var_corr;\n",
        "+        var_corr = 1;\n",
        "+      }\n",
        "+      for (size_t j = 0; j < per_channel_size; j++) {\n",
        "+        auto index = per_channel_size * i + j;\n",
        "+        if (!channel_first) {\n",
        "+          index = channels * j + i;\n",
        "         }\n",
        "+        auto dequant_data = (quant_datas[index] - zero_point) * scale;\n",
        "+        dequant_datas[index] = static_cast<DT>(dequant_data * var_corr + mean_corr);\n",
        "       }\n",
        "+    }\n",
        "+    return dequant_datas;\n",
        "+  }\n",
        "+  template <typename ST, typename DT = float>\n",
        "+  static DT *DequantData(lite::Tensor *input_tensor, bool channel_first = true) {\n",
        "+    const auto *quant_datas = static_cast<const ST *>(input_tensor->MutableData());\n",
        "+    if (quant_datas == nullptr) {\n",
        "+      MS_LOG(ERROR) << \"Get quant tensor failed.\";\n",
        "+      return nullptr;\n",
        "+    }\n",
        "+    DT *dequant_datas = static_cast<DT *>(malloc(input_tensor->ElementsNum() * sizeof(DT)));\n",
        "+    if (dequant_datas == nullptr) {\n",
        "+      MS_LOG(ERROR) << \"Malloc failed.\";\n",
        "+      return nullptr;\n",
        "+    }\n",
        "+    if (!IsPerBatch(input_tensor, quant_datas, dequant_datas) && input_tensor->quant_params().size() != kPerTensor) {\n",
        "+        return DequantPerChannel(input_tensor, quant_datas, dequant_datas,channel_first);\n",
        "     } else {\n",
        "       auto quant_param = input_tensor->quant_params();\n",
        "       auto quant_clusters = input_tensor->quant_clusters();\n",
        "diff --git a/mindspore/lite/src/train/classification_train_accuracy_monitor.cc b/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "index 5e54e37..8523863 100644\n",
        "--- a/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "+++ b/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "@@ -27,20 +27,11 @@ using mindspore::WARNING;\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "+\n",
        " ClassificationTrainAccuracyMonitor::ClassificationTrainAccuracyMonitor(int print_every_n, int accuracy_metrics,\n",
        "                                                                        const std::vector<int> &input_indexes,\n",
        "                                                                        const std::vector<int> &output_indexes) {\n",
        "-  if (input_indexes.size() == output_indexes.size()) {\n",
        "-    input_indexes_ = input_indexes;\n",
        "-    output_indexes_ = output_indexes;\n",
        "-  } else {\n",
        "-    MS_LOG(WARNING) << \"input to output mapping vectors sizes do not match\";\n",
        "-  }\n",
        "-  if (accuracy_metrics != METRICS_CLASSIFICATION) {\n",
        "-    MS_LOG(WARNING) << \"Only classification metrics is supported\";\n",
        "-  } else {\n",
        "-    accuracy_metrics_ = accuracy_metrics;\n",
        "-  }\n",
        "+  accuracy_metrics_ = std::make_shared<AccuracyMetrics>(accuracy_metrics,input_indexes,output_indexes);\n",
        "   print_every_n_ = print_every_n;\n",
        " }\n",
        " \n",
        "@@ -59,8 +50,8 @@ void ClassificationTrainAccuracyMonitor::EpochBegin(const session::TrainLoopCall\n",
        " int ClassificationTrainAccuracyMonitor::EpochEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "   if (cb_data.step_ > 0) accuracies_.at(cb_data.epoch_).second /= static_cast<float>(cb_data.step_ + 1);\n",
        "   if ((cb_data.epoch_ + 1) % print_every_n_ == 0) {\n",
        "-    std::cout << \"Epoch (\" << (cb_data.epoch_ + 1) << \"):\\tTraining Accuracy is \"\n",
        "-              << accuracies_.at(cb_data.epoch_).second << std::endl;\n",
        "+    std::cout << \"Epoch (\" << cb_data.epoch_ + 1 << \"):\\tTraining Accuracy is \" << accuracies_.at(cb_data.epoch_).second\n",
        "+              << std::endl;\n",
        "   }\n",
        "   return mindspore::session::RET_CONTINUE;\n",
        " }\n",
        "@@ -70,20 +61,23 @@ void ClassificationTrainAccuracyMonitor::StepEnd(const session::TrainLoopCallBac\n",
        "   auto outputs = cb_data.session_->GetPredictions();\n",
        " \n",
        "   float accuracy = 0.0;\n",
        "-  for (unsigned int i = 0; i < input_indexes_.size(); i++) {\n",
        "-    if ((inputs.size() <= static_cast<unsigned int>(input_indexes_[i])) ||\n",
        "-        (outputs.size() <= static_cast<unsigned int>(output_indexes_[i]))) {\n",
        "-      MS_LOG(WARNING) << \"indices \" << input_indexes_[i] << \"/\" << output_indexes_[i]\n",
        "+  auto input_indexes = accuracy_metrics_->input_indexes_;\n",
        "+  auto output_indexes = accuracy_metrics_->output_indexes_;\n",
        "+  for (unsigned int i = 0; i < input_indexes.size(); i++) {\n",
        "+    if ((inputs.size() <= static_cast<unsigned int>(input_indexes[i])) ||\n",
        "+        (outputs.size() <= static_cast<unsigned int>(output_indexes[i]))) {\n",
        "+      MS_LOG(WARNING) << \"indices \" << input_indexes[i] << \"/\" << output_indexes[i]\n",
        "                       << \" is outside of input/output range\";\n",
        "       return;\n",
        "     }\n",
        "-    if (inputs.at(input_indexes_[i])->data_type() == kNumberTypeInt32) {\n",
        "-      accuracy += CalculateSparseClassification(inputs.at(input_indexes_[i]), outputs.at(output_indexes_[i]));\n",
        "+    if (inputs.at(input_indexes[i])->data_type() == kNumberTypeInt32) {\n",
        "+      accuracy += CalculateSparseClassification(inputs.at(input_indexes[i]), outputs.at(output_indexes[i]));\n",
        "     } else {\n",
        "-      accuracy += CalculateOneHotClassification(inputs.at(input_indexes_[i]), outputs.at(output_indexes_[i]));\n",
        "+      accuracy += CalculateOneHotClassification(inputs.at(input_indexes[i]), outputs.at(output_indexes[i]));\n",
        "     }\n",
        "   }\n",
        "   accuracies_.at(cb_data.epoch_).second += accuracy;\n",
        " }\n",
        "+\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}