{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-5M7tpwLFil"
      },
      "source": [
        "diff --git a/build.sh b/build.sh\n",
        "index 3cc0490..8d74558 100755\n",
        "--- a/build.sh\n",
        "+++ b/build.sh\n",
        "@@ -492,25 +492,25 @@ write_commit_file() {\n",
        " build_lite_x86_64_jni_and_jar()\n",
        " {\n",
        "     # copy x86 so\n",
        "-    local inference_or_train=inference\n",
        "     local is_train=off\n",
        "     cd ${BASEPATH}/output/tmp\n",
        "-    if [ -f \"mindspore-lite-${VERSION_STR}-train-linux-x64.tar.gz\" ]; then\n",
        "-        inference_or_train=train\n",
        "-        is_train=on\n",
        "+    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+      echo \"is train on\"\n",
        "+      is_train=on\n",
        "     fi\n",
        "-    local pkg_name=mindspore-lite-${VERSION_STR}-${inference_or_train}-linux-x64\n",
        "+    local pkg_name=mindspore-lite-${VERSION_STR}-linux-x64\n",
        " \n",
        "     cd ${BASEPATH}/output/tmp/\n",
        "     rm -rf ${pkg_name}\n",
        "     tar -zxf ${BASEPATH}/output/tmp/${pkg_name}.tar.gz\n",
        "     rm -rf ${LITE_JAVA_PATH}/java/linux_x86/libs/   && mkdir -pv ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "     rm -rf ${LITE_JAVA_PATH}/native/libs/linux_x86/ && mkdir -pv ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "-    cp ./${pkg_name}/inference/lib/*.so* ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "-    cp ./${pkg_name}/inference/lib/*.so* ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "-    if [ -f \"mindspore-lite-${VERSION_STR}-train-linux-x64.tar.gz\" ]; then\n",
        "-        cp ./${pkg_name}/inference/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "-        cp ./${pkg_name}/inference/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "+    cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "+    cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "+    if [\"${MSLITE_ENABLE_TRAIN}\"]; then\n",
        "+        echo \"cp libjpeg\"\n",
        "+        cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "+        cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "     fi\n",
        "     # build jni so\n",
        "     cd ${BASEPATH}/mindspore/lite/build\n",
        "@@ -525,7 +525,7 @@ build_lite_x86_64_jni_and_jar()\n",
        "     fi\n",
        "     cp ./libmindspore-lite-jni.so ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "     cp ./libmindspore-lite-jni.so ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "-    cp ./libmindspore-lite-jni.so ${BASEPATH}/output/tmp/${pkg_name}/inference/lib/\n",
        "+    cp ./libmindspore-lite-jni.so ${BASEPATH}/output/tmp/${pkg_name}/runtime/lib/\n",
        " \n",
        "     # build java common\n",
        "     cd ${LITE_JAVA_PATH}/java/common\n",
        "@@ -537,7 +537,7 @@ build_lite_x86_64_jni_and_jar()\n",
        "     cd ${LITE_JAVA_PATH}/java/linux_x86/\n",
        "     gradle clean\n",
        "     gradle releaseJar\n",
        "-    cp ./build/lib/jar/*.jar ${BASEPATH}/output/tmp/${pkg_name}/inference/lib/\n",
        "+    cp ./build/lib/jar/*.jar ${BASEPATH}/output/tmp/${pkg_name}/runtime/lib/\n",
        " \n",
        "     # package\n",
        "     cd ${BASEPATH}/output/tmp\n",
        "@@ -643,24 +643,23 @@ build_lite_arm64_and_jni() {\n",
        "     # build arm64\n",
        "     build_lite \"arm64\"\n",
        "     # copy arm64 so\n",
        "-    local inference_or_train=inference\n",
        "     local is_train=off\n",
        "-    if [ -f \"${BASEPATH}/output/mindspore-lite-${VERSION_STR}-train-android-aarch64.tar.gz\" ]; then\n",
        "-        inference_or_train=train\n",
        "-        is_train=on\n",
        "+    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+      echo \"is train on\"\n",
        "+      is_train=on\n",
        "     fi\n",
        "-    local pkg_name=mindspore-lite-${VERSION_STR}-${inference_or_train}-android-aarch64\n",
        "+    local pkg_name=mindspore-lite-${VERSION_STR}-android-aarch64\n",
        "     cd \"${BASEPATH}/mindspore/lite/build\"\n",
        " \n",
        "     rm -rf ${pkg_name}\n",
        "     tar -zxf ${BASEPATH}/output/${pkg_name}.tar.gz\n",
        "     rm -rf ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/ && mkdir -p ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "     rm -rf ${LITE_JAVA_PATH}/native/libs/arm64-v8a/   && mkdir -p ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "-    cp ./${pkg_name}/${inference_or_train}/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "-    cp ./${pkg_name}/${inference_or_train}/lib/*.so* ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "-    if [ -f \"${BASEPATH}/output/mindspore-lite-${VERSION_STR}-train-android-aarch64.tar.gz\" ]; then\n",
        "-      cp ./${pkg_name}/train/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "-      cp ./${pkg_name}/train/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "+    cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "+    cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "+    if [\"${MSLITE_ENABLE_TRAIN}\"]; then\n",
        "+      cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "+      cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "     fi\n",
        "     # build jni so\n",
        "     [ -n \"${BASEPATH}\" ] && rm -rf java/jni && mkdir -pv java/jni\n",
        "@@ -683,24 +682,23 @@ build_lite_arm32_and_jni() {\n",
        "     # build arm32\n",
        "     build_lite \"arm32\"\n",
        "     # copy arm32 so\n",
        "-    local inference_or_train=inference\n",
        "     local is_train=off\n",
        "-    if [ -f \"${BASEPATH}/output/mindspore-lite-${VERSION_STR}-train-android-aarch32.tar.gz\" ]; then\n",
        "-        inference_or_train=train\n",
        "-        is_train=on\n",
        "+    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+      echo \"is train on\"\n",
        "+      is_train=on\n",
        "     fi\n",
        "-    local pkg_name=mindspore-lite-${VERSION_STR}-${inference_or_train}-android-aarch32\n",
        "+    local pkg_name=mindspore-lite-${VERSION_STR}-android-aarch32\n",
        "     cd \"${BASEPATH}/mindspore/lite/build\"\n",
        " \n",
        "     rm -rf ${pkg_name}\n",
        "     tar -zxf ${BASEPATH}/output/${pkg_name}.tar.gz\n",
        "     rm -rf ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/ && mkdir -pv ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "     rm -rf ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/   && mkdir -pv ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "-    cp ./${pkg_name}/${inference_or_train}/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "-    cp ./${pkg_name}/${inference_or_train}/lib/*.so* ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "-    if [ -f \"${BASEPATH}/output/mindspore-lite-${VERSION_STR}-train-android-aarch32.tar.gz\" ]; then\n",
        "-      cp ./${pkg_name}/train/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "-      cp ./${pkg_name}/train/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "+    cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "+    cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "+    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+      cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "+      cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "     fi\n",
        " \n",
        "     # build jni so\n",
        "diff --git a/cmake/package_lite.cmake b/cmake/package_lite.cmake\n",
        "index 43ae08f..6b5ab72 100644\n",
        "--- a/cmake/package_lite.cmake\n",
        "+++ b/cmake/package_lite.cmake\n",
        "@@ -8,12 +8,12 @@ set(OBFUSCATOR_ROOT_DIR ${RUNTIME_PKG_NAME}/tools/obfuscator)\n",
        " set(CROPPER_ROOT_DIR ${RUNTIME_PKG_NAME}/tools/cropper)\n",
        " set(TEST_CASE_DIR ${TOP_DIR}/mindspore/lite/test/build)\n",
        " \n",
        "-set(RUNTIME_DIR ${RUNTIME_PKG_NAME}/inference)\n",
        "-set(RUNTIME_INC_DIR ${RUNTIME_PKG_NAME}/inference/include)\n",
        "-set(RUNTIME_LIB_DIR ${RUNTIME_PKG_NAME}/inference/lib)\n",
        "-set(MIND_DATA_INC_DIR ${RUNTIME_PKG_NAME}/inference/include/dataset)\n",
        "-set(TURBO_DIR ${RUNTIME_PKG_NAME}/inference/third_party/libjpeg-turbo)\n",
        "-set(SECUREC_DIR ${RUNTIME_PKG_NAME}/inference/third_party/securec)\n",
        "+set(RUNTIME_DIR ${RUNTIME_PKG_NAME}/runtime)\n",
        "+set(RUNTIME_INC_DIR ${RUNTIME_PKG_NAME}/runtime/include)\n",
        "+set(RUNTIME_LIB_DIR ${RUNTIME_PKG_NAME}/runtime/lib)\n",
        "+set(MIND_DATA_INC_DIR ${RUNTIME_PKG_NAME}/runtime/include/dataset)\n",
        "+set(TURBO_DIR ${RUNTIME_PKG_NAME}/runtime/third_party/libjpeg-turbo)\n",
        "+set(SECUREC_DIR ${RUNTIME_PKG_NAME}/runtime/third_party/securec)\n",
        " set(MINDSPORE_LITE_LIB_NAME libmindspore-lite)\n",
        " set(BENCHMARK_NAME benchmark)\n",
        " set(BENCHMARK_ROOT_DIR ${RUNTIME_PKG_NAME}/tools/benchmark)\n",
        "diff --git a/mindspore/lite/CMakeLists.txt b/mindspore/lite/CMakeLists.txt\n",
        "index 8185ee6..fb5011f 100644\n",
        "--- a/mindspore/lite/CMakeLists.txt\n",
        "+++ b/mindspore/lite/CMakeLists.txt\n",
        "@@ -129,26 +129,24 @@ add_compile_options(-fPIC)\n",
        " \n",
        " if(SUPPORT_TRAIN)\n",
        "     set(BUILD_MINDDATA \"full\")\n",
        "-    set(TRAIN_OR_INFERENCE \"train\")\n",
        " else()\n",
        "-    set(TRAIN_OR_INFERENCE \"inference\")\n",
        " endif()\n",
        " \n",
        " if(PLATFORM_ARM64)\n",
        "-    set(RUNTIME_COMPONENT_NAME ${TRAIN_OR_INFERENCE}-android-aarch64)\n",
        "+    set(RUNTIME_COMPONENT_NAME \"android-aarch64\")\n",
        " elseif(PLATFORM_ARM32)\n",
        "-    set(RUNTIME_COMPONENT_NAME ${TRAIN_OR_INFERENCE}-android-aarch32)\n",
        "+    set(RUNTIME_COMPONENT_NAME \"android-aarch32\")\n",
        " elseif(WIN32)\n",
        "     execute_process(COMMAND \"${CMAKE_C_COMPILER}\" -dumpmachine\n",
        "         OUTPUT_VARIABLE i686_or_x86_64\n",
        "     )\n",
        "     if(i686_or_x86_64 MATCHES \"^i686-\")\n",
        "-        set(RUNTIME_COMPONENT_NAME ${TRAIN_OR_INFERENCE}-win-x86)\n",
        "+        set(RUNTIME_COMPONENT_NAME \"win-x86\")\n",
        "     else()\n",
        "-        set(RUNTIME_COMPONENT_NAME ${TRAIN_OR_INFERENCE}-win-x64)\n",
        "+        set(RUNTIME_COMPONENT_NAME \"win-x64\")\n",
        "     endif()\n",
        " else()\n",
        "-    set(RUNTIME_COMPONENT_NAME ${TRAIN_OR_INFERENCE}-linux-x64)\n",
        "+    set(RUNTIME_COMPONENT_NAME \"linux-x64\")\n",
        " endif()\n",
        " \n",
        " string(REPLACE \"/mindspore/lite\" \"\" TOP_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n",
        "diff --git a/mindspore/lite/minddata/example/CMakeLists.txt b/mindspore/lite/minddata/example/CMakeLists.txt\n",
        "index a5ac3ac..c6c753f 100644\n",
        "--- a/mindspore/lite/minddata/example/CMakeLists.txt\n",
        "+++ b/mindspore/lite/minddata/example/CMakeLists.txt\n",
        "@@ -4,7 +4,7 @@ set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror -Wall -fPIC -std=c++17\")\n",
        " \n",
        " set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-sign-compare\")\n",
        " \n",
        "-set(MS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/mindspore-lite-1.2.0-inference-linux-x64/inference\")\n",
        "+set(MS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/mindspore-lite-1.2.0-linux-x64/runtime\")\n",
        " \n",
        " include_directories(${MS_DIR})\n",
        " \n",
        "diff --git a/mindspore/lite/test/st/run_net_train.sh b/mindspore/lite/test/st/run_net_train.sh\n",
        "index 7eed84f..beb366a 100755\n",
        "--- a/mindspore/lite/test/st/run_net_train.sh\n",
        "+++ b/mindspore/lite/test/st/run_net_train.sh\n",
        "@@ -35,8 +35,8 @@ function Run_Export(){\n",
        " # Run converter on x86 platform:\n",
        " function Run_Converter() {\n",
        "     cd ${x86_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-train-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-train-linux-x64/ || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64/ || exit 1\n",
        " \n",
        "     cp tools/converter/converter/converter_lite ./ || exit 1\n",
        "     export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./tools/converter/lib/:./tools/converter/third_party/glog/lib\n",
        "@@ -73,8 +73,8 @@ function Run_Converter() {\n",
        " \n",
        " # Run on x86 platform:\n",
        " function Run_x86() {\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-train-linux-x64 || return 1\n",
        "-    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./inference/lib:./inference/third_party/libjpeg-turbo/lib\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64 || return 1\n",
        "+    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./runtime/lib:./runtime/third_party/libjpeg-turbo/lib\n",
        "     # Run mindspore converted train models:\n",
        "     fail=0\n",
        "     while read line; do\n",
        "@@ -137,24 +137,24 @@ function Run_arm() {\n",
        " \n",
        "     # Unzip\n",
        "     cd ${arm_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version_arm}-train-android-${process_unit}.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version_arm}-android-${process_unit}.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_train_test_path} || exit 1\n",
        "-    if [ -f ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/third_party/libjpeg-turbo/lib/libjpeg.so* ${benchmark_train_test_path}/ || exit 1\n",
        "-        cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/third_party/libjpeg-turbo/lib/libturbojpeg.so* ${benchmark_train_test_path}/ || exit 1\n",
        "-        cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/lib/libminddata-lite.so ${benchmark_train_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/third_party/libjpeg-turbo/lib/libjpeg.so* ${benchmark_train_test_path}/ || exit 1\n",
        "+        cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/third_party/libjpeg-turbo/lib/libturbojpeg.so* ${benchmark_train_test_path}/ || exit 1\n",
        "+        cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/lib/libminddata-lite.so ${benchmark_train_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "     if [ \"$1\" == arm64 ] || [ \"$1\" == arm32 ]; then\n",
        "-        cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_train_test_path}/libhiai.so || exit 1\n",
        "-        cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_train_test_path}/libhiai_ir.so || exit 1\n",
        "-        cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_train_test_path}/libhiai_ir_build.so || exit 1\n",
        "+        cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_train_test_path}/libhiai.so || exit 1\n",
        "+        cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_train_test_path}/libhiai_ir.so || exit 1\n",
        "+        cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_train_test_path}/libhiai_ir_build.so || exit 1\n",
        "     fi\n",
        " \n",
        "-    cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/lib/libmindspore-lite.so ${benchmark_train_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/inference/lib/libmindspore-lite-train.so ${benchmark_train_test_path}/libmindspore-lite-train.so || exit 1\n",
        "-    cp -a ${arm_path}/mindspore-lite-${version_arm}-train-android-${process_unit}/tools/benchmark_train/benchmark_train ${benchmark_train_test_path}/benchmark_train || exit 1\n",
        "+    cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/lib/libmindspore-lite.so ${benchmark_train_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/runtime/lib/libmindspore-lite-train.so ${benchmark_train_test_path}/libmindspore-lite-train.so || exit 1\n",
        "+    cp -a ${arm_path}/mindspore-lite-${version_arm}-android-${process_unit}/tools/benchmark_train/benchmark_train ${benchmark_train_test_path}/benchmark_train || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_train_test_path} /data/local/tmp/ > ${adb_push_log_file}\n",
        "@@ -316,19 +316,19 @@ fi\n",
        " echo $train_io_path\n",
        " \n",
        " arm64_path=${release_path}/android_aarch64\n",
        "-file=$(ls ${arm64_path}/*train-android-aarch64.tar.gz)\n",
        "+file=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        " file_name=\"${file##*/}\"\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version_arm64=${file_name_array[2]}\n",
        " \n",
        " arm32_path=${release_path}/android_aarch32\n",
        "-file=$(ls ${arm32_path}/*train-android-aarch32.tar.gz)\n",
        "+file=$(ls ${arm32_path}/*android-aarch32.tar.gz)\n",
        " file_name=\"${file##*/}\"\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version_arm32=${file_name_array[2]}\n",
        " \n",
        " x86_path=${release_path}/ubuntu_x86\n",
        "-file=$(ls ${x86_path}/*train-linux-x64.tar.gz)\n",
        "+file=$(ls ${x86_path}/*linux-x64.tar.gz)\n",
        " file_name=\"${file##*/}\"\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version=${file_name_array[2]}\n",
        "diff --git a/mindspore/lite/test/st/scripts/run_benchmark_arm.sh b/mindspore/lite/test/st/scripts/run_benchmark_arm.sh\n",
        "index fab0a6f..c1cc3eb 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_benchmark_arm.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_benchmark_arm.sh\n",
        "@@ -4,8 +4,8 @@\n",
        " function Run_Converter() {\n",
        "     # Unzip x86 runtime and converter\n",
        "     cd ${x86_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-inference-linux-x64/ || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64/ || exit 1\n",
        " \n",
        "     cp tools/converter/converter/converter_lite ./ || exit 1\n",
        "     export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./tools/converter/lib/:./tools/converter/third_party/glog/lib\n",
        "@@ -424,9 +424,9 @@ function Run_Converter() {\n",
        " function Run_arm64_codegen() {\n",
        "     echo \"ANDROID_NDK: ${ANDROID_NDK}\" >> ${run_arm64_fp32_codegen_log_file}\n",
        "     cd ${arm64_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch64.tar.gz || exit 1\n",
        "-    local PKG_PATH=${arm64_path}/mindspore-lite-${version}-inference-android-aarch64\n",
        "-    local CODEGEN_PATH=${x86_path}/mindspore-lite-${version}-inference-linux-x64/tools/codegen\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch64.tar.gz || exit 1\n",
        "+    local PKG_PATH=${arm64_path}/mindspore-lite-${version}-android-aarch64\n",
        "+    local CODEGEN_PATH=${x86_path}/mindspore-lite-${version}-linux-x64/tools/codegen\n",
        " \n",
        "     rm -rf ${build_path}\n",
        "     mkdir -p ${build_path}\n",
        "@@ -512,9 +512,9 @@ function Run_arm64_codegen() {\n",
        " function Run_arm32_codegen() {\n",
        "     echo \"ANDROID_NDK: ${ANDROID_NDK}\" >> ${run_arm32_fp32_codegen_log_file}\n",
        "     cd ${arm32_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch32.tar.gz || exit 1\n",
        "-    local PKG_PATH=${arm32_path}/mindspore-lite-${version}-inference-android-aarch32\n",
        "-    local CODEGEN_PATH=${x86_path}/mindspore-lite-${version}-inference-linux-x64/tools/codegen\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch32.tar.gz || exit 1\n",
        "+    local PKG_PATH=${arm32_path}/mindspore-lite-${version}-android-aarch32\n",
        "+    local CODEGEN_PATH=${x86_path}/mindspore-lite-${version}-linux-x64/tools/codegen\n",
        " \n",
        "     rm -rf ${build_path}\n",
        "     mkdir -p ${build_path}\n",
        "@@ -600,19 +600,19 @@ function Run_arm32_codegen() {\n",
        " # Run on arm64 platform:\n",
        " function Run_arm64() {\n",
        "     cd ${arm64_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch64.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch64.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "-    if [ -f ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        " \n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -1021,18 +1021,18 @@ function Run_arm64() {\n",
        " # Run on arm32 platform:\n",
        " function Run_arm32() {\n",
        "     cd ${arm32_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch32.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch32.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "-    if [ -f ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/inference/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/inference/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "-    cp -a ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "-    cp -a ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "-    cp -a ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "-    cp -a ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/inference/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${arm32_path}/mindspore-lite-${version}-inference-android-aarch32/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -1077,19 +1077,19 @@ function Run_arm32() {\n",
        " # Run on arm64-fp16 platform:\n",
        " function Run_arm64_fp16() {\n",
        "     cd ${arm64_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch64.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch64.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "-    if [ -f ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        " \n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -1249,18 +1249,18 @@ function Run_arm64_fp16() {\n",
        " # Run on armv8.2-a32-fp16 platform:\n",
        " function Run_armv82_a32_fp16() {\n",
        "     cd ${armv82_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch32.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch32.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "-    if [ -f ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/inference/minddata/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/inference/minddata/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${armv82_path}/mindspore-lite-${version}-android-aarch32/runtime/minddata/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${armv82_path}/mindspore-lite-${version}-android-aarch32/runtime/minddata/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "-    cp -a ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "-    cp -a ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "-    cp -a ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "-    cp -a ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/inference/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${armv82_path}/mindspore-lite-${version}-inference-android-aarch32/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${armv82_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "+    cp -a ${armv82_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "+    cp -a ${armv82_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "+    cp -a ${armv82_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${armv82_path}/mindspore-lite-${version}-android-aarch32/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -1485,7 +1485,7 @@ done\n",
        " # mkdir train\n",
        " \n",
        " x86_path=${release_path}/ubuntu_x86\n",
        "-file_name=$(ls ${x86_path}/*inference-linux-x64.tar.gz)\n",
        "+file_name=$(ls ${x86_path}/*linux-x64.tar.gz)\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version=${file_name_array[2]}\n",
        " \n",
        "@@ -1582,7 +1582,7 @@ isFailed=0\n",
        " if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_codegen\" ]]; then\n",
        "     # Run on arm64\n",
        "     arm64_path=${release_path}/android_aarch64\n",
        "-    file_name=$(ls ${arm64_path}/*inference-android-aarch64.tar.gz)\n",
        "+    file_name=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "@@ -1595,7 +1595,7 @@ fi\n",
        " if [[ $backend == \"all\" || $backend == \"arm32_cpu\" || $backend == \"arm32_codegen\" ]]; then\n",
        "     # Run on arm32\n",
        "     arm32_path=${release_path}/android_aarch32\n",
        "-    file_name=$(ls ${arm32_path}/*inference-android-aarch32.tar.gz)\n",
        "+    file_name=$(ls ${arm32_path}/*android-aarch32.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "@@ -1608,7 +1608,7 @@ fi\n",
        " if [[ $backend == \"all\" || $backend == \"arm32_cpu\" || $backend == \"arm32_fp16\" ]]; then\n",
        "     # Run on armv82-a32-fp16\n",
        "     armv82_path=${release_path}/android_aarch32\n",
        "-    file_name=$(ls ${armv82_path}/*inference-android-aarch32.tar.gz)\n",
        "+    file_name=$(ls ${armv82_path}/*android-aarch32.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "@@ -1622,7 +1622,7 @@ if [[ $backend == \"all\" || $backend == \"arm32_cpu\" || $backend == \"arm32_fp32\" ]\n",
        "     # Run on arm32\n",
        "     arm32_path=${release_path}/android_aarch32\n",
        "     # mv ${arm32_path}/*train-android-aarch32* ./train\n",
        "-    file_name=$(ls ${arm32_path}/*inference-android-aarch32.tar.gz)\n",
        "+    file_name=$(ls ${arm32_path}/*android-aarch32.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "@@ -1636,7 +1636,7 @@ if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_fp32\" ]\n",
        "     # Run on arm64\n",
        "     arm64_path=${release_path}/android_aarch64\n",
        "     # mv ${arm64_path}/*train-android-aarch64* ./train\n",
        "-    file_name=$(ls ${arm64_path}/*inference-android-aarch64.tar.gz)\n",
        "+    file_name=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "@@ -1650,7 +1650,7 @@ if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_fp16\" ]\n",
        "     # Run on arm64-fp16\n",
        "     arm64_path=${release_path}/android_aarch64\n",
        "     # mv ${arm64_path}/*train-android-aarch64* ./train\n",
        "-    file_name=$(ls ${arm64_path}/*inference-android-aarch64.tar.gz)\n",
        "+    file_name=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "diff --git a/mindspore/lite/test/st/scripts/run_benchmark_gpu.sh b/mindspore/lite/test/st/scripts/run_benchmark_gpu.sh\n",
        "index a0edf6c..cea1b1d 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_benchmark_gpu.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_benchmark_gpu.sh\n",
        "@@ -4,8 +4,8 @@\n",
        " function Run_Converter() {\n",
        "     # Unzip x86 runtime and converter\n",
        "     cd ${x86_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-inference-linux-x64/ || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64/ || exit 1\n",
        " \n",
        "     cp tools/converter/converter/converter_lite ./ || exit 1\n",
        "     export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./tools/converter/lib/:./tools/converter/third_party/glog/lib\n",
        "@@ -114,19 +114,19 @@ function Run_Converter() {\n",
        " # Run on gpu platform:\n",
        " function Run_gpu() {\n",
        "     cd ${arm64_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch64.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch64.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "-    if [ -f ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        " \n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -326,7 +326,7 @@ done\n",
        " # mkdir train\n",
        " \n",
        " x86_path=${release_path}/ubuntu_x86\n",
        "-file_name=$(ls ${x86_path}/*inference-linux-x64.tar.gz)\n",
        "+file_name=$(ls ${x86_path}/*linux-x64.tar.gz)\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version=${file_name_array[2]}\n",
        " \n",
        "@@ -387,7 +387,7 @@ if [[ $backend == \"all\" || $backend == \"gpu\" ]]; then\n",
        "     # Run on gpu\n",
        "     arm64_path=${release_path}/android_aarch64\n",
        "     # mv ${arm64_path}/*train-android-aarch64* ./train\n",
        "-    file_name=$(ls ${arm64_path}/*inference-android-aarch64.tar.gz)\n",
        "+    file_name=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "diff --git a/mindspore/lite/test/st/scripts/run_benchmark_npu.sh b/mindspore/lite/test/st/scripts/run_benchmark_npu.sh\n",
        "index 892be5a..ca9e2a5 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_benchmark_npu.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_benchmark_npu.sh\n",
        "@@ -4,8 +4,8 @@\n",
        " function Run_Converter() {\n",
        "     # Unzip x86 runtime and converter\n",
        "     cd ${x86_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-inference-linux-x64/ || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64/ || exit 1\n",
        " \n",
        "     cp tools/converter/converter/converter_lite ./ || exit 1\n",
        "     export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./tools/converter/lib/:./tools/converter/third_party/glog/lib\n",
        "@@ -59,19 +59,19 @@ function Run_Converter() {\n",
        " # Run on npu platform:\n",
        " function Run_npu() {\n",
        "     cd ${arm64_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch64.tar.gz || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch64.tar.gz || exit 1\n",
        " \n",
        "     # If build with minddata, copy the minddata related libs\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "-    if [ -f ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/minddata/lib/libminddata-lite.so ]; then\n",
        "-        cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/minddata/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+    if [ -f ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/minddata/lib/libminddata-lite.so ]; then\n",
        "+        cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/minddata/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "     fi\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        " \n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/inference/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "-    cp -a ${arm64_path}/mindspore-lite-${version}-inference-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -178,7 +178,7 @@ done\n",
        " # mkdir train\n",
        " \n",
        " x86_path=${release_path}/ubuntu_x86\n",
        "-file_name=$(ls ${x86_path}/*inference-linux-x64.tar.gz)\n",
        "+file_name=$(ls ${x86_path}/*linux-x64.tar.gz)\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version=${file_name_array[2]}\n",
        " \n",
        "@@ -235,7 +235,7 @@ if [[ $backend == \"all\" || $backend == \"npu\" ]]; then\n",
        "     # Run on npu\n",
        "     arm64_path=${release_path}/android_aarch64\n",
        "     # mv ${arm64_path}/*train-android-aarch64* ./train\n",
        "-    file_name=$(ls ${arm64_path}/*inference-android-aarch64.tar.gz)\n",
        "+    file_name=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        "     IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "     version=${file_name_array[2]}\n",
        " \n",
        "diff --git a/mindspore/lite/test/st/scripts/run_benchmark_x86.sh b/mindspore/lite/test/st/scripts/run_benchmark_x86.sh\n",
        "index 23ec599..1328939 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_benchmark_x86.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_benchmark_x86.sh\n",
        "@@ -4,8 +4,8 @@\n",
        " function Run_Converter() {\n",
        "     # Unzip x86 runtime and converter\n",
        "     cd ${x86_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-inference-linux-x64/ || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64/ || exit 1\n",
        " \n",
        "     cp tools/converter/converter/converter_lite ./ || exit 1\n",
        "     export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:./tools/converter/lib/:./tools/converter/third_party/glog/lib\n",
        "@@ -344,9 +344,9 @@ function Run_Converter() {\n",
        " \n",
        " # Run on x86 platform:\n",
        " function Run_x86() {\n",
        "-    echo 'cd  '${x86_path}'/mindspore-lite-'${version}'-inference-linux-x64' >> \"${run_x86_log_file}\"\n",
        "-    cd ${x86_path}/mindspore-lite-${version}-inference-linux-x64 || return 1\n",
        "-    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./inference/lib\n",
        "+    echo 'cd  '${x86_path}'/mindspore-lite-'${version}'-linux-x64' >> \"${run_x86_log_file}\"\n",
        "+    cd ${x86_path}/mindspore-lite-${version}-linux-x64 || return 1\n",
        "+    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./runtime/lib\n",
        "     cp tools/benchmark/benchmark ./ || exit 1\n",
        " \n",
        "     # Run tf converted models:\n",
        "@@ -671,9 +671,9 @@ function Run_x86() {\n",
        " # Run on x86 sse platform:\n",
        " function Run_x86_sse() {\n",
        "     cd ${x86_path}/sse || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/sse/mindspore-lite-${version}-inference-linux-x64 || return 1\n",
        "-    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./inference/lib\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/sse/mindspore-lite-${version}-linux-x64 || return 1\n",
        "+    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./runtime/lib\n",
        "     cp tools/benchmark/benchmark ./ || exit 1\n",
        " \n",
        "     # Run tflite converted models:\n",
        "@@ -943,9 +943,9 @@ function Run_x86_sse() {\n",
        " # Run on x86 avx platform:\n",
        " function Run_x86_avx() {\n",
        "     cd ${x86_path}/avx || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    cd ${x86_path}/avx/mindspore-lite-${version}-inference-linux-x64 || return 1\n",
        "-    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./inference/lib\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    cd ${x86_path}/avx/mindspore-lite-${version}-linux-x64 || return 1\n",
        "+    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./runtime/lib\n",
        "     cp tools/benchmark/benchmark ./ || exit 1\n",
        " \n",
        "     # Run tflite converted models:\n",
        "@@ -1215,11 +1215,11 @@ function Run_x86_avx() {\n",
        " # Run on x86 java platform:\n",
        " function Run_x86_java() {\n",
        "     cd ${x86_java_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-linux-x64.tar.gz || exit 1\n",
        "-    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${x86_java_path}/mindspore-lite-${version}-inference-linux-x64/inference/lib\n",
        "+    tar -zxf mindspore-lite-${version}-linux-x64.tar.gz || exit 1\n",
        "+    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${x86_java_path}/mindspore-lite-${version}-linux-x64/runtime/lib\n",
        "     # compile benchmark\n",
        "-    echo \"javac -cp ${x86_java_path}/mindspore-lite-${version}-inference-linux-x64/inference/lib/mindspore-lite-java.jar ${basepath}/java/src/main/java/Benchmark.java -d .\"\n",
        "-    javac -cp ${x86_java_path}/mindspore-lite-${version}-inference-linux-x64/inference/lib/mindspore-lite-java.jar ${basepath}/java/src/main/java/Benchmark.java -d .\n",
        "+    echo \"javac -cp ${x86_java_path}/mindspore-lite-${version}-linux-x64/runtime/lib/mindspore-lite-java.jar ${basepath}/java/src/main/java/Benchmark.java -d .\"\n",
        "+    javac -cp ${x86_java_path}/mindspore-lite-${version}-linux-x64/runtime/lib/mindspore-lite-java.jar ${basepath}/java/src/main/java/Benchmark.java -d .\n",
        " \n",
        "     count=0\n",
        "     # Run tflite converted models:\n",
        "@@ -1234,8 +1234,8 @@ function Run_x86_java() {\n",
        "           continue\n",
        "         fi\n",
        "         echo ${model_name} >> \"${run_x86_java_log_file}\"\n",
        "-        echo \"java -classpath .:${x86_java_path}/mindspore-lite-${version}-inference-linux-x64/inference/lib/mindspore-lite-java.jar Benchmark ${ms_models_path}/${model_name}.ms '${models_path}'/input_output/input/${model_name}.ms.bin '${models_path}'/input_output/output/${model_name}.ms.out 1\" >> \"${run_x86_java_log_file}\"\n",
        "-        java -classpath .:${x86_java_path}/mindspore-lite-${version}-inference-linux-x64/inference/lib/mindspore-lite-java.jar Benchmark ${ms_models_path}/${model_name}.ms ${models_path}/input_output/input/${model_name}.ms.bin ${models_path}/input_output/output/${model_name}.ms.out 1\n",
        "+        echo \"java -classpath .:${x86_java_path}/mindspore-lite-${version}-linux-x64/runtime/lib/mindspore-lite-java.jar Benchmark ${ms_models_path}/${model_name}.ms '${models_path}'/input_output/input/${model_name}.ms.bin '${models_path}'/input_output/output/${model_name}.ms.out 1\" >> \"${run_x86_java_log_file}\"\n",
        "+        java -classpath .:${x86_java_path}/mindspore-lite-${version}-linux-x64/runtime/lib/mindspore-lite-java.jar Benchmark ${ms_models_path}/${model_name}.ms ${models_path}/input_output/input/${model_name}.ms.bin ${models_path}/input_output/output/${model_name}.ms.out 1\n",
        "         if [ $? = 0 ]; then\n",
        "             run_result='x86_java: '${model_name}' pass'; echo ${run_result} >> ${run_benchmark_result_file}\n",
        "         else\n",
        "@@ -1246,7 +1246,7 @@ function Run_x86_java() {\n",
        " \n",
        " # Run on x86 codegen benchmark\n",
        " function Run_x86_codegen() {\n",
        "-    local CODEGEN_PATH=${x86_path}/mindspore-lite-${version}-inference-linux-x64/tools/codegen\n",
        "+    local CODEGEN_PATH=${x86_path}/mindspore-lite-${version}-linux-x64/tools/codegen\n",
        " \n",
        "     rm -rf ${build_path}\n",
        "     mkdir -p ${build_path}\n",
        "@@ -1261,7 +1261,7 @@ function Run_x86_codegen() {\n",
        "         ${CODEGEN_PATH}/codegen --codePath=${build_path} --modelPath=${ms_models_path}/${model_name}.ms --supportParallel=${PARALLEL} >> ${run_x86_codegen_log_file}\n",
        "         # 1. build benchmark\n",
        "         mkdir -p ${build_path}/${model_name}/build && cd ${build_path}/${model_name}/build || exit 1\n",
        "-        cmake -DPKG_PATH=${x86_path}/mindspore-lite-${version}-inference-linux-x64 ${build_path}/${model_name} >> ${run_x86_codegen_log_file}\n",
        "+        cmake -DPKG_PATH=${x86_path}/mindspore-lite-${version}-linux-x64 ${build_path}/${model_name} >> ${run_x86_codegen_log_file}\n",
        "         make >> ${run_x86_codegen_log_file}\n",
        "         # 2. run benchmark\n",
        "         echo \"net file: ${build_path}/${model_name}/src/net.bin\" >> ${run_x86_codegen_log_file}\n",
        "@@ -1341,7 +1341,7 @@ done\n",
        " # mkdir train\n",
        " \n",
        " x86_path=${release_path}/ubuntu_x86\n",
        "-file_name=$(ls ${x86_path}/*inference-linux-x64.tar.gz)\n",
        "+file_name=$(ls ${x86_path}/*linux-x64.tar.gz)\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version=${file_name_array[2]}\n",
        " \n",
        "diff --git a/mindspore/lite/test/st/scripts/run_cropper.sh b/mindspore/lite/test/st/scripts/run_cropper.sh\n",
        "index 5aef974..13b013f 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_cropper.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_cropper.sh\n",
        "@@ -24,16 +24,16 @@ function Print_Cropper_Result() {\n",
        " \n",
        " function Run_cropper() {\n",
        "     cd ${arm64_path} || exit 1\n",
        "-    tar -zxf mindspore-lite-${version}-inference-android-aarch64.tar.gz || exit 1\n",
        "-    cd mindspore-lite-${version}-inference-android-aarch64 || exit 1\n",
        "-    cp -a ./inference/third_party/hiai_ddk/lib/libhiai.so \"${cropper_test_path}\"/libhiai.so || exit 1\n",
        "-    cp -a ./inference/third_party/hiai_ddk/lib/libhiai_ir.so \"${cropper_test_path}\"/libhiai_ir.so || exit 1\n",
        "-    cp -a ./inference/third_party/hiai_ddk/lib/libhiai_ir_build.so \"${cropper_test_path}\"/libhiai_ir_build.so || exit 1\n",
        "+    tar -zxf mindspore-lite-${version}-android-aarch64.tar.gz || exit 1\n",
        "+    cd mindspore-lite-${version}-android-aarch64 || exit 1\n",
        "+    cp -a ./runtime/third_party/hiai_ddk/lib/libhiai.so \"${cropper_test_path}\"/libhiai.so || exit 1\n",
        "+    cp -a ./runtime/third_party/hiai_ddk/lib/libhiai_ir.so \"${cropper_test_path}\"/libhiai_ir.so || exit 1\n",
        "+    cp -a ./runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so \"${cropper_test_path}\"/libhiai_ir_build.so || exit 1\n",
        " \n",
        "-    cp -a ./inference/lib/libmindspore-lite.a \"${cropper_test_path}\"/libmindspore-lite.a || exit 1\n",
        "+    cp -a ./runtime/lib/libmindspore-lite.a \"${cropper_test_path}\"/libmindspore-lite.a || exit 1\n",
        "     cp -a ./tools/benchmark/benchmark \"${cropper_test_path}\"/benchmark || exit 1\n",
        " \n",
        "-    cp -r \"${x86_path}\"/mindspore-lite-${version}-inference-linux-x64/tools/cropper/ \"${cropper_test_path}\" || exit 1\n",
        "+    cp -r \"${x86_path}\"/mindspore-lite-${version}-linux-x64/tools/cropper/ \"${cropper_test_path}\" || exit 1\n",
        " \n",
        "     cd \"${cropper_test_path}\" || exit 1\n",
        "     echo \"${cropper_test_path}\"\n",
        "@@ -137,7 +137,7 @@ echo ' ' > \"${run_converter_log_file}\"\n",
        " run_converter_result_file=\"${basepath}\"/run_converter_result.txt\n",
        " echo ' ' > \"${run_converter_result_file}\"\n",
        " \n",
        "-file_name=$(ls \"${x86_path}\"/*inference-linux-x64.tar.gz)\n",
        "+file_name=$(ls \"${x86_path}\"/*linux-x64.tar.gz)\n",
        " IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        " version=${file_name_array[2]}\n",
        " ms_models_path=${basepath}/ms_models\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}