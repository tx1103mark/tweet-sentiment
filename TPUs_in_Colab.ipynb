{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdBPxeSSlAF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "#include <memory>\n",
        "#include <map>\n",
        "#include \"src/ops/space_to_batch.h\"\n",
        "#include \"src/ops/space_to_batch_nd.h\"\n",
        "#include \"src/ops/conv2d.h\"\n",
        "#include \"src/ops/roi_pooling.h\"\n",
        "#include \"src/ops/topk.h\"\n",
        "#include \"src/ops/broadcast_to.h\"\n",
        "#include \"src/ops/unsqueeze.h\"\n",
        "#include \"src/ops/unstack.h\"\n",
        "#include \"src/ops/depth_to_space.h\"\n",
        "#include \"src/ops/batch_to_space.h\"\n",
        "#include \"src/ops/prior_box.h\"\n",
        "#include \"src/ops/lstm.h\"\n",
        "#include \"src/ops/softmax.h\"\n",
        "#include \"src/ops/activation.h\"\n",
        "#include \"src/ops/deconv2d.h\"\n",
        "#include \"src/ops/reduce.h\"\n",
        "#include \"src/ops/pooling.h\"\n",
        "#include \"src/ops/fused_batchnorm.h\"\n",
        "#include \"src/ops/batch_norm.h\"\n",
        "#include \"src/ops/power.h\"\n",
        "#include \"src/ops/range.h\"\n",
        "#include \"src/ops/add.h\"\n",
        "#include \"src/ops/sub.h\"\n",
        "#include \"src/ops/div.h\"\n",
        "#include \"src/ops/bias_add.h\"\n",
        "#include \"src/ops/expand_dims.h\"\n",
        "#include \"src/ops/full_connection.h\"\n",
        "#include \"src/ops/shape.h\"\n",
        "#include \"src/ops/elu.h\"\n",
        "#include \"src/ops/embedding_lookup.h\"\n",
        "#include \"src/ops/quant_dtype_cast.h\"\n",
        "#include \"src/ops/matmul.h\"\n",
        "#include \"src/ops/resize.h\"\n",
        "#include \"src/ops/tile.h\"\n",
        "#include \"src/ops/one_hot.h\"\n",
        "#include \"src/ops/space_to_depth.h\"\n",
        "#include \"src/ops/split.h\"\n",
        "#include \"src/ops/argmax.h\"\n",
        "#include \"src/ops/argmin.h\"\n",
        "#include \"src/ops/cast.h\"\n",
        "#include \"src/ops/reshape.h\"\n",
        "#include \"src/ops/scale.h\"\n",
        "#include \"src/ops/concat.h\"\n",
        "#include \"src/ops/nchw2nhwc.h\"\n",
        "#include \"src/ops/slice.h\"\n",
        "#include \"src/ops/squeeze.h\"\n",
        "#include \"src/ops/flatten.h\"\n",
        "#include \"src/ops/mean.h\"\n",
        "#include \"src/ops/nhwc2nchw.h\"\n",
        "#include \"src/ops/stack.h\"\n",
        "#include \"src/ops/crop.h\"\n",
        "#include \"src/ops/addn.h\"\n",
        "#include \"src/ops/gather.h\"\n",
        "#include \"src/ops/gather_nd.h\"\n",
        "#include \"src/ops/local_response_normalization.h\"\n",
        "#include \"src/ops/pad.h\"\n",
        "#include \"src/ops/p_relu.h\"\n",
        "#include \"src/ops/leaky_relu.h\"\n",
        "#include \"src/ops/reverse_sequence.h\"\n",
        "#include \"src/ops/dedepthwise_conv2d.h\"\n",
        "#include \"src/ops/depthwise_conv2d.h\"\n",
        "#include \"src/ops/mul.h\"\n",
        "#include \"src/ops/eltwise.h\"\n",
        "#include \"src/ops/fill.h\"\n",
        "#include \"src/ops/transpose.h\"\n",
        "#include \"src/ops/log.h\"\n",
        "#include \"src/ops/abs.h\"\n",
        "#include \"src/ops/sin.h\"\n",
        "#include \"src/ops/cos.h\"\n",
        "#include \"src/ops/sqrt.h\"\n",
        "#include \"src/ops/square.h\"\n",
        "#include \"src/ops/exp.h\"\n",
        "#include \"src/ops/rsqrt.h\"\n",
        "#include \"src/ops/maximum.h\"\n",
        "#include \"src/ops/minimum.h\"\n",
        "#include \"src/ops/strided_slice.h\"\n",
        "#include \"src/ops/reverse.h\"\n",
        "#include \"src/ops/logical_and.h\"\n",
        "#include \"src/ops/logical_or.h\"\n",
        "#include \"src/ops/logical_not.h\"\n",
        "#include \"src/ops/floor_div.h\"\n",
        "#include \"src/ops/floor_mod.h\"\n",
        "#include \"src/ops/equal.h\"\n",
        "#include \"src/ops/not_equal.h\"\n",
        "#include \"src/ops/less.h\"\n",
        "#include \"src/ops/less_equal.h\"\n",
        "#include \"src/ops/greater_equal.h\"\n",
        "#include \"src/ops/greater.h\"\n",
        "#include \"src/ops/floor.h\"\n",
        "#include \"src/ops/squared_difference.h\"\n",
        "#include \"src/ops/ceil.h\"\n",
        "#include \"src/ops/round.h\"\n",
        "#include \"src/ops/unique.h\"\n",
        "#include \"src/ops/zeros_like.h\"\n",
        "#include \"src/ops/return.h\"\n",
        "#include \"src/ops/where.h\"\n",
        "#include \"src/ops/scatter_nd.h\"\n",
        "#include \"src/ops/constant_of_shape.h\"\n",
        "#include \"src/ops/dequant.h\"\n",
        "#include \"src/ops/make_tuple.h\"\n",
        "#include \"src/ops/quant.h\"\n",
        "#include \"src/ops/tuple_get_item.h\"\n",
        "#include \"src/ops/l2_norm.h\"\n",
        "#include \"src/ops/sparse_to_dense.h\"\n",
        "#include \"src/ops/detection_post_process.h\"\n",
        "#ifdef PRIMITIVE_WRITEABLE\n",
        "#include \"tools/converter/quantizer/quantize_util.h\"\n",
        "#endif\n",
        "\n",
        "#ifdef SUPPORT_TRAIN\n",
        "#include \"src/ops/activation_grad.h\"\n",
        "#include \"src/ops/apply_momentum.h\"\n",
        "#include \"src/ops/bias_grad.h\"\n",
        "#include \"src/ops/pooling_grad.h\"\n",
        "#include \"src/ops/conv2d_grad_filter.h\"\n",
        "#include \"src/ops/conv2d_grad_input.h\"\n",
        "#include \"src/ops/power_grad.h\"\n",
        "#include \"src/ops/softmax_cross_entropy.h\"\n",
        "#include \"src/ops/bn_grad.h\"\n",
        "#include \"src/ops/arithmetic_grad.h\"\n",
        "#endif\n",
        "\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "#ifdef PRIMITIVE_WRITEABLE\n",
        "void PrimitiveC::CalQuantParam(const double &mean, const double &stdDev, float *mMin, float *mMax) {\n",
        "  const float qmin = 0;\n",
        "  const float qmax = 255;\n",
        "  *mMin = static_cast<float>((qmin - mean) / stdDev);\n",
        "  *mMax = static_cast<float>((qmax - mean) / stdDev);\n",
        "}\n",
        "\n",
        "void PrimitiveC::PopulaterQuantParam(const Primitive &prim,\n",
        "                                     std::vector<std::vector<schema::QuantParamT>> *vecInputQuantParam,\n",
        "                                     std::vector<std::vector<schema::QuantParamT>> *vecOutputQuantParam) {\n",
        "  auto narrow_range = prim.GetAttr(\"narrow_range\");\n",
        "  bool narrowRangeQuantParam = GetValue<bool>(narrow_range);\n",
        "  auto num_bits = prim.GetAttr(\"num_bits\");\n",
        "  int32_t numbitsRangeQuantParam = GetValue<int32_t>(num_bits);\n",
        "\n",
        "  std::vector<schema::QuantParamT> quants;\n",
        "  schema::QuantParamT quantParam;\n",
        "  auto mean = prim.GetAttr(\"mean\");\n",
        "  auto std_dev = prim.GetAttr(\"std_dev\");\n",
        "  if (mean != nullptr && std_dev != nullptr) {\n",
        "    auto meanQuantOaram = GetValue<double>(mean);\n",
        "    double stddevQuantOaram = GetValue<double>(std_dev);\n",
        "    float mMin = 0.0;\n",
        "    float mMax = 0.0;\n",
        "    CalQuantParam(meanQuantOaram, stddevQuantOaram, &mMin, &mMax);\n",
        "    quantParam.min = mMin;\n",
        "    quantParam.max = mMax;\n",
        "  } else {\n",
        "    auto inputMin = prim.GetAttr(\"input_minq\");\n",
        "    auto inputMax = prim.GetAttr(\"input_maxq\");\n",
        "    auto inputMinPtr = inputMin->cast<lite::tensor::TensorPtr>();\n",
        "    auto inputMaxPtr = inputMax->cast<lite::tensor::TensorPtr>();\n",
        "    float *minBuf = static_cast<float *>(inputMinPtr->Data());\n",
        "    float *maxBuf = static_cast<float *>(inputMaxPtr->Data());\n",
        "    quantParam.min = *minBuf;\n",
        "    quantParam.max = *maxBuf;\n",
        "  }\n",
        "  quant::CalQuantizationParams(&quantParam, quantParam.min, quantParam.max, narrowRangeQuantParam,\n",
        "                               numbitsRangeQuantParam);\n",
        "  quants.emplace_back(quantParam);\n",
        "  vecInputQuantParam->emplace_back(quants);\n",
        "\n",
        "  quants.clear();\n",
        "  auto filterMin = prim.GetAttr(\"filter_minq\");\n",
        "  auto filterMax = prim.GetAttr(\"filter_maxq\");\n",
        "  if (filterMin != nullptr && filterMax != nullptr) {\n",
        "    auto filterMinPtr = filterMin->cast<lite::tensor::TensorPtr>();\n",
        "    auto filterMaxPtr = filterMax->cast<lite::tensor::TensorPtr>();\n",
        "    float *minBuf = static_cast<float *>(filterMinPtr->Data());\n",
        "    float *maxBuf = static_cast<float *>(filterMaxPtr->Data());\n",
        "    quantParam.min = FLT_MAX;\n",
        "    quantParam.max = FLT_MIN;\n",
        "    for (int i = 0; i < filterMinPtr->DataSize(); ++i) {\n",
        "      quantParam.min = (*(minBuf) < quantParam.min) ? (*minBuf) : quantParam.min;\n",
        "      quantParam.max = (*(maxBuf) > quantParam.max) ? (*maxBuf) : quantParam.max;\n",
        "      minBuf++;\n",
        "      maxBuf++;\n",
        "    }\n",
        "    quant::CalQuantizationParams(&quantParam, quantParam.min, quantParam.max, true, numbitsRangeQuantParam);\n",
        "    quants.emplace_back(quantParam);\n",
        "    vecInputQuantParam->emplace_back(quants);\n",
        "  }\n",
        "\n",
        "  quants.clear();\n",
        "  quantParam.min = 0.0;\n",
        "  quantParam.max = 0.0;\n",
        "  quantParam.zeroPoint = 0;\n",
        "  quantParam.scale = vecInputQuantParam->at(0).at(0).scale * vecInputQuantParam->at(1).at(0).scale;\n",
        "  quants.emplace_back(quantParam);\n",
        "  vecInputQuantParam->emplace_back(quants);\n",
        "\n",
        "  quants.clear();\n",
        "  auto outputMin = prim.GetAttr(\"output_minq\");\n",
        "  auto outputMax = prim.GetAttr(\"output_maxq\");\n",
        "  if (outputMin != nullptr && outputMax != nullptr) {\n",
        "    auto outputMinPtr = outputMin->cast<lite::tensor::TensorPtr>();\n",
        "    auto outputMaxPtr = outputMax->cast<lite::tensor::TensorPtr>();\n",
        "    float *minBuf = static_cast<float *>(outputMinPtr->Data());\n",
        "    float *maxBuf = static_cast<float *>(outputMaxPtr->Data());\n",
        "    quantParam.min = *minBuf;\n",
        "    quantParam.max = *maxBuf;\n",
        "    quant::CalQuantizationParams(&quantParam, quantParam.min, quantParam.max, narrowRangeQuantParam,\n",
        "                                 numbitsRangeQuantParam);\n",
        "    quants.emplace_back(quantParam);\n",
        "    vecOutputQuantParam->emplace_back(quants);\n",
        "  }\n",
        "}\n",
        "schema::PrimitiveT *PrimitiveC::GetPrimitiveT() const { return this->primitive_; }\n",
        "\n",
        "void PrimitiveC::ClearPrimitiveT() { this->primitive_ = nullptr; }\n",
        "\n",
        "void PrimitiveC::SetInputQuantParam(const std::vector<std::vector<schema::QuantParamT>> &input_quant_param) {\n",
        "  this->input_quant_param_ = input_quant_param;\n",
        "}\n",
        "\n",
        "void PrimitiveC::SetOutputQuantParam(const std::vector<std::vector<schema::QuantParamT>> &output_quant_param) {\n",
        "  this->output_quant_param_ = output_quant_param;\n",
        "}\n",
        "\n",
        "void PrimitiveC::ClearInputOutputQuantParam() {\n",
        "  input_quant_param_.clear();\n",
        "  output_quant_param_.clear();\n",
        "}\n",
        "\n",
        "void PrimitiveC::AddInputQuantParam(std::vector<schema::QuantParamT> quant_param) {\n",
        "  this->input_quant_param_.emplace_back(quant_param);\n",
        "}\n",
        "std::vector<std::vector<schema::QuantParamT>> PrimitiveC::GetInputQuantParams() const { return input_quant_param_; }\n",
        "\n",
        "void PrimitiveC::AddOutputQuantParam(std::vector<schema::QuantParamT> quant_param) {\n",
        "  this->output_quant_param_.emplace_back(quant_param);\n",
        "}\n",
        "std::vector<std::vector<schema::QuantParamT>> PrimitiveC::GetOutputQuantParams() const { return output_quant_param_; }\n",
        "\n",
        "void PrimitiveC::SetQuantType(const schema::QuantType &quant_type) { this->quant_type_ = quant_type; }\n",
        "\n",
        "schema::QuantType PrimitiveC::GetQuantType() const { return quant_type_; }\n",
        "\n",
        "std::shared_ptr<PrimitiveC> GetReturnPrim() {\n",
        "  auto return_primitiveT = new (std::nothrow) schema::PrimitiveT;\n",
        "  if (return_primitiveT == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new PrimitiveT failed\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  return_primitiveT->value.type = schema::PrimitiveType_Return;\n",
        "  return_primitiveT->value.value = new schema::ReturnT;\n",
        "  if (return_primitiveT->value.value == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new ReturnT failed\";\n",
        "    delete (return_primitiveT);\n",
        "    return nullptr;\n",
        "  }\n",
        "  return std::make_shared<Return>(return_primitiveT);\n",
        "}\n",
        "\n",
        "std::shared_ptr<PrimitiveC> GetMakeTuplePrim() {\n",
        "  auto make_tuple_primitiveT = new schema::PrimitiveT;\n",
        "  if (make_tuple_primitiveT == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new PrimitiveT failed\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  make_tuple_primitiveT->value.type = schema::PrimitiveType_MakeTuple;\n",
        "  make_tuple_primitiveT->value.value = new schema::MakeTupleT;\n",
        "  if (make_tuple_primitiveT->value.value == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new MakeTupleT failed\";\n",
        "    delete (make_tuple_primitiveT);\n",
        "    return nullptr;\n",
        "  }\n",
        "  return std::make_shared<MakeTuple>(make_tuple_primitiveT);\n",
        "}\n",
        "\n",
        "std::shared_ptr<PrimitiveC> GetTupleGetItemPrim() {\n",
        "  auto tuple_get_item_primitiveT = new schema::PrimitiveT();\n",
        "  if (tuple_get_item_primitiveT == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new PrimitiveT failed\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  tuple_get_item_primitiveT->value.type = schema::PrimitiveType_TupleGetItem;\n",
        "  tuple_get_item_primitiveT->value.value = new schema::TupleGetItemT;\n",
        "  if (tuple_get_item_primitiveT->value.value == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new TupleGetItemT failed\";\n",
        "    delete (tuple_get_item_primitiveT);\n",
        "    return nullptr;\n",
        "  }\n",
        "  return std::make_shared<TupleGetItem>(tuple_get_item_primitiveT);\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if<std::is_base_of<PrimitiveC, T>::value>>\n",
        "std::shared_ptr<PrimitiveC> NewPrimitiveC(const Primitive &prim, const std::vector<AnfNodePtr> &inputs,\n",
        "                                          const schema::QuantType &quantType) {\n",
        "  auto primc = std::make_shared<T>();\n",
        "  if (primc == nullptr) {\n",
        "    MS_LOG(ERROR) << \"make_shared PrimitiveC failed\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  primc->SetQuantType(quantType);\n",
        "  auto ret = primc->UnPackAttr(prim, inputs);\n",
        "  if (ret != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"UnPackAttr failed\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  return primc;\n",
        "}\n",
        "\n",
        "std::shared_ptr<PrimitiveC> PrimitiveC::UnPackFromPrimitive(const Primitive &prim,\n",
        "                                                            const std::vector<AnfNodePtr> &inputs,\n",
        "                                                            const schema::QuantType &quantType) {\n",
        "  const auto &op_type = prim.name();\n",
        "  if (op_type == \"ReLU\" || op_type == \"ReLU6\" || op_type == \"Sigmoid\") {\n",
        "    return NewPrimitiveC<Activation>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"BatchNorm\") {\n",
        "    return NewPrimitiveC<BatchNorm>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"BiasAdd\") {\n",
        "    return NewPrimitiveC<BiasAdd>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Concat\") {\n",
        "    return NewPrimitiveC<Concat>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Conv2D\") {\n",
        "    return NewPrimitiveC<Conv2D>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"DepthwiseConv2dNative\" || op_type == \"DepthwiseConv2D\") {\n",
        "    return NewPrimitiveC<DepthwiseConv2D>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Dequant\") {\n",
        "    return NewPrimitiveC<Dequant>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Flatten\") {\n",
        "    return NewPrimitiveC<Flatten>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"make_tuple\") {\n",
        "    return NewPrimitiveC<MakeTuple>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"MatMul\") {\n",
        "    return NewPrimitiveC<MatMul>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Mul\") {\n",
        "    return NewPrimitiveC<Mul>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"MaxPool\") {\n",
        "    return NewPrimitiveC<Pooling>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Quant\") {\n",
        "    return NewPrimitiveC<Quant>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"ReduceMean\") {\n",
        "    return NewPrimitiveC<Reduce>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Reshape\") {\n",
        "    return NewPrimitiveC<Reshape>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"TensorAdd\") {\n",
        "    return NewPrimitiveC<Add>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Transpose\") {\n",
        "    return NewPrimitiveC<Transpose>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Elu\") {\n",
        "    return NewPrimitiveC<Elu>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Log\") {\n",
        "    return NewPrimitiveC<Log>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Conv2DBackpropInput\") {\n",
        "    return NewPrimitiveC<DeConv2D>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"tuple_getitem\") {\n",
        "    return NewPrimitiveC<TupleGetItem>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Softmax\") {\n",
        "    return NewPrimitiveC<SoftMax>(prim, inputs, quantType);\n",
        "#ifdef SUPPORT_TRAIN0\n",
        "  } else if ((op_type == \"ReluGrad\" || op_type == \"Relu6Grad\" || op_type == \"SigmoidGrad\")) {\n",
        "    return NewPrimitiveC<ActivationGrad>(prim, inputs, quantType);\n",
        "  } else if ((op_type == \"MaxPoolGrad\") || (op_type == \"MeanPoolGrad\")) {\n",
        "    return NewPrimitiveC<PoolingGrad>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Conv2DBackpropFilter\") {\n",
        "    return NewPrimitiveC<Conv2DGradFilter>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"BiasAddGrad\") {\n",
        "    return NewPrimitiveC<BiasGrad>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"ApplyMomentum\") {\n",
        "    return NewPrimitiveC<ApplyMomentum>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"BatchNormGrad\") {\n",
        "    return NewPrimitiveC<BNGrad>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Conv2DGradInput\") {\n",
        "    return NewPrimitiveC<Conv2DGradInput>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"Conv2DGradFilter\") {\n",
        "    return NewPrimitiveC<Conv2DGradFilter>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"BiasGrad\") {\n",
        "    return NewPrimitiveC<BiasGrad>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"ActivationGrad\") {\n",
        "    return NewPrimitiveC<ActivationGrad>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"PoolingGrad\") {\n",
        "    return NewPrimitiveC<PoolingGrad>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"BNGradInput\") {\n",
        "    return NewPrimitiveC<BNGradInput>(prim, inputs, quantType);\n",
        "  } else if (op_type == \"PowerGrad\") {\n",
        "    return NewPrimitiveC<PowerGrad>(prim, inputs, quantType);\n",
        "#endif\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"Unsupported primitive type in UnPackFromPrimitive : \" << op_type;\n",
        "    return nullptr;\n",
        "  }\n",
        "}\n",
        "\n",
        "PrimitiveC *PrimitiveC::UnPackFromSchemaPrimitiveT(mindspore::schema::PrimitiveT *primitive) {\n",
        "  MS_ASSERT(primitive != nullptr);\n",
        "  auto op_type = primitive->value.type;\n",
        "  switch (op_type) {\n",
        "    case schema::PrimitiveType_SoftMax:\n",
        "      return new SoftMax(primitive);\n",
        "    case schema::PrimitiveType_Activation:\n",
        "      return new Activation(primitive);\n",
        "    case schema::PrimitiveType_Conv2D:\n",
        "      return new Conv2D(primitive);\n",
        "    case schema::PrimitiveType_DeConv2D:\n",
        "      return new DeConv2D(primitive);\n",
        "    case schema::PrimitiveType_Reduce:\n",
        "      return new Reduce(primitive);\n",
        "    case schema::PrimitiveType_Pooling:\n",
        "      return new Pooling(primitive);\n",
        "    case schema::PrimitiveType_ROIPooling:\n",
        "      return new ROIPooling(primitive);\n",
        "    case schema::PrimitiveType_DepthwiseConv2D:\n",
        "      return new DepthwiseConv2D(primitive);\n",
        "    case schema::PrimitiveType_FusedBatchNorm:\n",
        "      return new FusedBatchNorm(primitive);\n",
        "    case schema::PrimitiveType_BatchNorm:\n",
        "      return new BatchNorm(primitive);\n",
        "    case schema::PrimitiveType_FullConnection:\n",
        "      return new FullConnection(primitive);\n",
        "    case schema::PrimitiveType_Power:\n",
        "      return new Power(primitive);\n",
        "    case schema::PrimitiveType_Pad:\n",
        "      return new Pad(primitive);\n",
        "    case schema::PrimitiveType_Range:\n",
        "      return new Range(primitive);\n",
        "    case schema::PrimitiveType_Mul:\n",
        "      return new Mul(primitive);\n",
        "    case schema::PrimitiveType_Add:\n",
        "      return new Add(primitive);\n",
        "    case schema::PrimitiveType_Sub:\n",
        "      return new Sub(primitive);\n",
        "    case schema::PrimitiveType_Div:\n",
        "      return new Div(primitive);\n",
        "    case schema::PrimitiveType_BiasAdd:\n",
        "      return new BiasAdd(primitive);\n",
        "    case schema::PrimitiveType_ExpandDims:\n",
        "      return new ExpandDims(primitive);\n",
        "    case schema::PrimitiveType_ArgMax:\n",
        "      return new ArgMax(primitive);\n",
        "    case schema::PrimitiveType_ArgMin:\n",
        "      return new ArgMin(primitive);\n",
        "    case schema::PrimitiveType_Cast:\n",
        "      return new Cast(primitive);\n",
        "    case schema::PrimitiveType_Reshape:\n",
        "      return new Reshape(primitive);\n",
        "    case schema::PrimitiveType_Scale:\n",
        "      return new Scale(primitive);\n",
        "    case schema::PrimitiveType_Eltwise:\n",
        "      return new Eltwise(primitive);\n",
        "    case schema::PrimitiveType_Ceil:\n",
        "      return new Ceil(primitive);\n",
        "    case schema::PrimitiveType_Concat:\n",
        "      return new Concat(primitive);\n",
        "    case schema::PrimitiveType_Fill:\n",
        "      return new Fill(primitive);\n",
        "    case schema::PrimitiveType_Nhwc2Nchw:\n",
        "      return new Nhwc2Nchw(primitive);\n",
        "    case schema::PrimitiveType_Nchw2Nhwc:\n",
        "      return new Nchw2Nhwc(primitive);\n",
        "    case schema::PrimitiveType_Transpose:\n",
        "      return new Transpose(primitive);\n",
        "    case schema::PrimitiveType_Slice:\n",
        "      return new Slice(primitive);\n",
        "    case schema::PrimitiveType_Squeeze:\n",
        "      return new Squeeze(primitive);\n",
        "    case schema::PrimitiveType_Flatten:\n",
        "      return new Flatten(primitive);\n",
        "    case schema::PrimitiveType_Mean:\n",
        "      return new Mean(primitive);\n",
        "    case schema::PrimitiveType_Stack:\n",
        "      return new Stack(primitive);\n",
        "    case schema::PrimitiveType_Crop:\n",
        "      return new Crop(primitive);\n",
        "    case schema::PrimitiveType_SquaredDifference:\n",
        "      return new SquaredDifference(primitive);\n",
        "    case schema::PrimitiveType_AddN:\n",
        "      return new AddN(primitive);\n",
        "    case schema::PrimitiveType_Abs:\n",
        "      return new Abs(primitive);\n",
        "    case schema::PrimitiveType_Sin:\n",
        "      return new Sin(primitive);\n",
        "    case schema::PrimitiveType_Cos:\n",
        "      return new Cos(primitive);\n",
        "    case schema::PrimitiveType_Log:\n",
        "      return new Log(primitive);\n",
        "    case schema::PrimitiveType_Sqrt:\n",
        "      return new Sqrt(primitive);\n",
        "    case schema::PrimitiveType_Rsqrt:\n",
        "      return new Rsqrt(primitive);\n",
        "    case schema::PrimitiveType_Square:\n",
        "      return new Square(primitive);\n",
        "    case schema::PrimitiveType_Exp:\n",
        "      return new Exp(primitive);\n",
        "    case schema::PrimitiveType_Gather:\n",
        "      return new Gather(primitive);\n",
        "    case schema::PrimitiveType_GatherNd:\n",
        "      return new GatherNd(primitive);\n",
        "    case schema::PrimitiveType_LocalResponseNormalization:\n",
        "      return new LocalResponseNormalization(primitive);\n",
        "    case schema::PrimitiveType_Maximum:\n",
        "      return new Maximum(primitive);\n",
        "    case schema::PrimitiveType_Minimum:\n",
        "      return new Minimum(primitive);\n",
        "    case schema::PrimitiveType_StridedSlice:\n",
        "      return new StridedSlice(primitive);\n",
        "    case schema::PrimitiveType_LeakyReLU:\n",
        "      return new (std::nothrow) LeakyReLU(primitive);\n",
        "    case schema::PrimitiveType_PReLU:\n",
        "      return new (std::nothrow) PReLU(primitive);\n",
        "    case schema::PrimitiveType_Round:\n",
        "      return new Round(primitive);\n",
        "    case schema::PrimitiveType_Reverse:\n",
        "      return new Reverse(primitive);\n",
        "    case schema::PrimitiveType_ReverseSequence:\n",
        "      return new ReverseSequence(primitive);\n",
        "    case schema::PrimitiveType_LogicalAnd:\n",
        "      return new LogicalAnd(primitive);\n",
        "    case schema::PrimitiveType_LogicalOr:\n",
        "      return new LogicalOr(primitive);\n",
        "    case schema::PrimitiveType_LogicalNot:\n",
        "      return new LogicalNot(primitive);\n",
        "    case schema::PrimitiveType_FloorDiv:\n",
        "      return new FloorDiv(primitive);\n",
        "    case schema::PrimitiveType_FloorMod:\n",
        "      return new FloorMod(primitive);\n",
        "    case schema::PrimitiveType_Equal:\n",
        "      return new Equal(primitive);\n",
        "    case schema::PrimitiveType_NotEqual:\n",
        "      return new NotEqual(primitive);\n",
        "    case schema::PrimitiveType_Less:\n",
        "      return new Less(primitive);\n",
        "    case schema::PrimitiveType_LessEqual:\n",
        "      return new LessEqual(primitive);\n",
        "    case schema::PrimitiveType_Greater:\n",
        "      return new Greater(primitive);\n",
        "    case schema::PrimitiveType_GreaterEqual:\n",
        "      return new GreaterEqual(primitive);\n",
        "    case schema::PrimitiveType_Floor:\n",
        "      return new Floor(primitive);\n",
        "    case schema::PrimitiveType_Split:\n",
        "      return new Split(primitive);\n",
        "    case schema::PrimitiveType_OneHot:\n",
        "      return new OneHot(primitive);\n",
        "    case schema::PrimitiveType_PriorBox:\n",
        "      return new PriorBox(primitive);\n",
        "    case schema::PrimitiveType_SpaceToDepth:\n",
        "      return new SpaceToDepth(primitive);\n",
        "    case schema::PrimitiveType_Tile:\n",
        "      return new Tile(primitive);\n",
        "    case schema::PrimitiveType_Resize:\n",
        "      return new Resize(primitive);\n",
        "    case schema::PrimitiveType_Unstack:\n",
        "      return new Unstack(primitive);\n",
        "    case schema::PrimitiveType_Unique:\n",
        "      return new Unique(primitive);\n",
        "    case schema::PrimitiveType_TopK:\n",
        "      return new TopK(primitive);\n",
        "    case schema::PrimitiveType_MatMul:\n",
        "      return new MatMul(primitive);\n",
        "    case schema::PrimitiveType_QuantDTypeCast:\n",
        "      return new QuantDTypeCast(primitive);\n",
        "    case schema::PrimitiveType_EmbeddingLookup:\n",
        "      return new EmbeddingLookup(primitive);\n",
        "    case schema::PrimitiveType_Elu:\n",
        "      return new Elu(primitive);\n",
        "    case schema::PrimitiveType_DeDepthwiseConv2D:\n",
        "      return new DeDepthwiseConv2D(primitive);\n",
        "    case schema::PrimitiveType_Shape:\n",
        "      return new Shape(primitive);\n",
        "    case schema::PrimitiveType_Unsqueeze:\n",
        "      return new Unsqueeze(primitive);\n",
        "    case schema::PrimitiveType_BatchToSpace:\n",
        "      return new BatchToSpace(primitive);\n",
        "    case schema::PrimitiveType_SpaceToBatch:\n",
        "      return new SpaceToBatch(primitive);\n",
        "    case schema::PrimitiveType_SpaceToBatchND:\n",
        "      return new SpaceToBatchND(primitive);\n",
        "    case schema::PrimitiveType_BroadcastTo:\n",
        "      return new BroadcastTo(primitive);\n",
        "    case schema::PrimitiveType_DepthToSpace:\n",
        "      return new DepthToSpace(primitive);\n",
        "    case schema::PrimitiveType_Lstm:\n",
        "      return new Lstm(primitive);\n",
        "    case schema::PrimitiveType_ZerosLike:\n",
        "      return new ZerosLike(primitive);\n",
        "    case schema::PrimitiveType_MakeTuple:\n",
        "      return new MakeTuple(primitive);\n",
        "    case schema::PrimitiveType_Where:\n",
        "      return new Where(primitive);\n",
        "    case schema::PrimitiveType_ScatterND:\n",
        "      return new ScatterND(primitive);\n",
        "    case schema::PrimitiveType_ConstantOfShape:\n",
        "      return new ConstantOfShape(primitive);\n",
        "    case schema::PrimitiveType_L2Norm:\n",
        "      return new L2Norm(primitive);\n",
        "    case schema::PrimitiveType_SparseToDense:\n",
        "      return new SparseToDense(primitive);\n",
        "    case schema::PrimitiveType_DetectionPostProcess:\n",
        "      return new DetectionPostProcess(primitive);\n",
        "\n",
        "#ifdef SUPPORT_TRAIN\n",
        "    case schema::PrimitiveType_ActivationGrad:\n",
        "      return new ActivationGrad(primitive);\n",
        "    case schema::PrimitiveType_PoolingGrad:\n",
        "      return new PoolingGrad(primitive);\n",
        "    case schema::PrimitiveType_Conv2DGradFilter:\n",
        "      return new Conv2DGradFilter(primitive);\n",
        "    case schema::PrimitiveType_Conv2DGradInput:\n",
        "      return new Conv2DGradInput(primitive);\n",
        "    case schema::PrimitiveType_BiasGrad:\n",
        "      return new BiasGrad(primitive);\n",
        "    case schema::PrimitiveType_ApplyMomentum:\n",
        "      return new ApplyMomentum(primitive);\n",
        "    case schema::PrimitiveType_BNGrad:\n",
        "      return new BNGrad(primitive);\n",
        "    case schema::PrimitiveType_AddGrad:\n",
        "      return new ArithmeticGrad(primitive);\n",
        "    case schema::PrimitiveType_SubGrad:\n",
        "      return new ArithmeticGrad(primitive);\n",
        "    case schema::PrimitiveType_MulGrad:\n",
        "      return new ArithmeticGrad(primitive);\n",
        "    case schema::PrimitiveType_DivGrad:\n",
        "      return new ArithmeticGrad(primitive);\n",
        "    case schema::PrimitiveType_PowerGrad:\n",
        "      return new PowerGrad(primitive);\n",
        "    case schema::PrimitiveType_BNGradInput:\n",
        "      return new BNGradInput(primitive);\n",
        "#endif\n",
        "\n",
        "    default:\n",
        "      MS_LOG(ERROR) << \"Unsupported primitive type in UnPackFromSchemaPrimitiveT : \"\n",
        "                    << schema::EnumNamePrimitiveType(op_type);\n",
        "      break;\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "#else\n",
        "PrimitiveC *PrimitiveC::UnPackFromSchemaPrimitive(const schema::Primitive *primitive) {\n",
        "  MS_ASSERT(primitive);\n",
        "  auto op_type = primitive->value_type();\n",
        "  switch (op_type) {\n",
        "    case schema::PrimitiveType_SoftMax:\n",
        "      return NewPrimitiveC<SoftMax>(primitive);\n",
        "    case schema::PrimitiveType_Activation:\n",
        "      return NewPrimitiveC<Activation>(primitive);\n",
        "    case schema::PrimitiveType_Conv2D:\n",
        "      return NewPrimitiveC<Conv2D>(primitive);\n",
        "    case schema::PrimitiveType_DeConv2D:\n",
        "      return NewPrimitiveC<DeConv2D>(primitive);\n",
        "    case schema::PrimitiveType_Reduce:\n",
        "      return NewPrimitiveC<Reduce>(primitive);\n",
        "    case schema::PrimitiveType_Pooling:\n",
        "      return NewPrimitiveC<Pooling>(primitive);\n",
        "    case schema::PrimitiveType_ROIPooling:\n",
        "      return NewPrimitiveC<ROIPooling>(primitive);\n",
        "    case schema::PrimitiveType_DepthwiseConv2D:\n",
        "      return NewPrimitiveC<DepthwiseConv2D>(primitive);\n",
        "    case schema::PrimitiveType_FusedBatchNorm:\n",
        "      return NewPrimitiveC<FusedBatchNorm>(primitive);\n",
        "    case schema::PrimitiveType_BatchNorm:\n",
        "      return NewPrimitiveC<BatchNorm>(primitive);\n",
        "    case schema::PrimitiveType_FullConnection:\n",
        "      return NewPrimitiveC<FullConnection>(primitive);\n",
        "    case schema::PrimitiveType_Power:\n",
        "      return NewPrimitiveC<Power>(primitive);\n",
        "    case schema::PrimitiveType_Pad:\n",
        "      return NewPrimitiveC<Pad>(primitive);\n",
        "    case schema::PrimitiveType_Range:\n",
        "      return NewPrimitiveC<Range>(primitive);\n",
        "    case schema::PrimitiveType_Mul:\n",
        "      return NewPrimitiveC<Mul>(primitive);\n",
        "    case schema::PrimitiveType_Add:\n",
        "      return NewPrimitiveC<Add>(primitive);\n",
        "    case schema::PrimitiveType_Sub:\n",
        "      return NewPrimitiveC<Sub>(primitive);\n",
        "    case schema::PrimitiveType_Div:\n",
        "      return NewPrimitiveC<Div>(primitive);\n",
        "    case schema::PrimitiveType_BiasAdd:\n",
        "      return NewPrimitiveC<BiasAdd>(primitive);\n",
        "    case schema::PrimitiveType_ExpandDims:\n",
        "      return NewPrimitiveC<ExpandDims>(primitive);\n",
        "    case schema::PrimitiveType_ArgMax:\n",
        "      return NewPrimitiveC<ArgMax>(primitive);\n",
        "    case schema::PrimitiveType_ArgMin:\n",
        "      return NewPrimitiveC<ArgMin>(primitive);\n",
        "    case schema::PrimitiveType_Cast:\n",
        "      return NewPrimitiveC<Cast>(primitive);\n",
        "    case schema::PrimitiveType_Reshape:\n",
        "      return NewPrimitiveC<Reshape>(primitive);\n",
        "    case schema::PrimitiveType_Scale:\n",
        "      return NewPrimitiveC<Scale>(primitive);\n",
        "    case schema::PrimitiveType_Eltwise:\n",
        "      return NewPrimitiveC<Eltwise>(primitive);\n",
        "    case schema::PrimitiveType_Ceil:\n",
        "      return NewPrimitiveC<Ceil>(primitive);\n",
        "    case schema::PrimitiveType_Concat:\n",
        "      return NewPrimitiveC<Concat>(primitive);\n",
        "    case schema::PrimitiveType_Fill:\n",
        "      return NewPrimitiveC<Fill>(primitive);\n",
        "    case schema::PrimitiveType_Nhwc2Nchw:\n",
        "      return NewPrimitiveC<Nhwc2Nchw>(primitive);\n",
        "    case schema::PrimitiveType_Nchw2Nhwc:\n",
        "      return NewPrimitiveC<Nchw2Nhwc>(primitive);\n",
        "    case schema::PrimitiveType_Transpose:\n",
        "      return NewPrimitiveC<Transpose>(primitive);\n",
        "    case schema::PrimitiveType_Slice:\n",
        "      return NewPrimitiveC<Slice>(primitive);\n",
        "    case schema::PrimitiveType_Squeeze:\n",
        "      return NewPrimitiveC<Squeeze>(primitive);\n",
        "    case schema::PrimitiveType_Flatten:\n",
        "      return NewPrimitiveC<Flatten>(primitive);\n",
        "    case schema::PrimitiveType_Mean:\n",
        "      return NewPrimitiveC<Mean>(primitive);\n",
        "    case schema::PrimitiveType_Stack:\n",
        "      return NewPrimitiveC<Stack>(primitive);\n",
        "    case schema::PrimitiveType_Crop:\n",
        "      return NewPrimitiveC<Crop>(primitive);\n",
        "    case schema::PrimitiveType_SquaredDifference:\n",
        "      return NewPrimitiveC<SquaredDifference>(primitive);\n",
        "    case schema::PrimitiveType_AddN:\n",
        "      return NewPrimitiveC<AddN>(primitive);\n",
        "    case schema::PrimitiveType_Abs:\n",
        "      return NewPrimitiveC<Abs>(primitive);\n",
        "    case schema::PrimitiveType_Sin:\n",
        "      return NewPrimitiveC<Sin>(primitive);\n",
        "    case schema::PrimitiveType_Cos:\n",
        "      return NewPrimitiveC<Cos>(primitive);\n",
        "    case schema::PrimitiveType_Log:\n",
        "      return NewPrimitiveC<Log>(primitive);\n",
        "    case schema::PrimitiveType_Sqrt:\n",
        "      return NewPrimitiveC<Sqrt>(primitive);\n",
        "    case schema::PrimitiveType_Rsqrt:\n",
        "      return NewPrimitiveC<Rsqrt>(primitive);\n",
        "    case schema::PrimitiveType_Square:\n",
        "      return NewPrimitiveC<Square>(primitive);\n",
        "    case schema::PrimitiveType_Exp:\n",
        "      return NewPrimitiveC<Exp>(primitive);\n",
        "    case schema::PrimitiveType_Gather:\n",
        "      return NewPrimitiveC<Gather>(primitive);\n",
        "    case schema::PrimitiveType_GatherNd:\n",
        "      return NewPrimitiveC<GatherNd>(primitive);\n",
        "    case schema::PrimitiveType_LocalResponseNormalization:\n",
        "      return NewPrimitiveC<LocalResponseNormalization>(primitive);\n",
        "    case schema::PrimitiveType_Maximum:\n",
        "      return NewPrimitiveC<Maximum>(primitive);\n",
        "    case schema::PrimitiveType_Minimum:\n",
        "      return NewPrimitiveC<Minimum>(primitive);\n",
        "    case schema::PrimitiveType_StridedSlice:\n",
        "      return NewPrimitiveC<StridedSlice>(primitive);\n",
        "    case schema::PrimitiveType_LeakyReLU:\n",
        "      return NewPrimitiveC<LeakyReLU>(primitive);\n",
        "    case schema::PrimitiveType_PReLU:\n",
        "      return NewPrimitiveC<PReLU>(primitive);\n",
        "    case schema::PrimitiveType_Round:\n",
        "      return NewPrimitiveC<Round>(primitive);\n",
        "    case schema::PrimitiveType_Reverse:\n",
        "      return NewPrimitiveC<Reverse>(primitive);\n",
        "    case schema::PrimitiveType_ReverseSequence:\n",
        "      return NewPrimitiveC<ReverseSequence>(primitive);\n",
        "    case schema::PrimitiveType_LogicalAnd:\n",
        "      return NewPrimitiveC<LogicalAnd>(primitive);\n",
        "    case schema::PrimitiveType_LogicalOr:\n",
        "      return NewPrimitiveC<LogicalOr>(primitive);\n",
        "    case schema::PrimitiveType_LogicalNot:\n",
        "      return NewPrimitiveC<LogicalNot>(primitive);\n",
        "    case schema::PrimitiveType_FloorDiv:\n",
        "      return NewPrimitiveC<FloorDiv>(primitive);\n",
        "    case schema::PrimitiveType_FloorMod:\n",
        "      return NewPrimitiveC<FloorMod>(primitive);\n",
        "    case schema::PrimitiveType_Equal:\n",
        "      return NewPrimitiveC<Equal>(primitive);\n",
        "    case schema::PrimitiveType_NotEqual:\n",
        "      return NewPrimitiveC<NotEqual>(primitive);\n",
        "    case schema::PrimitiveType_Less:\n",
        "      return NewPrimitiveC<Less>(primitive);\n",
        "    case schema::PrimitiveType_LessEqual:\n",
        "      return NewPrimitiveC<LessEqual>(primitive);\n",
        "    case schema::PrimitiveType_Greater:\n",
        "      return NewPrimitiveC<Greater>(primitive);\n",
        "    case schema::PrimitiveType_GreaterEqual:\n",
        "      return NewPrimitiveC<GreaterEqual>(primitive);\n",
        "    case schema::PrimitiveType_Floor:\n",
        "      return NewPrimitiveC<Floor>(primitive);\n",
        "    case schema::PrimitiveType_Split:\n",
        "      return NewPrimitiveC<Split>(primitive);\n",
        "    case schema::PrimitiveType_OneHot:\n",
        "      return NewPrimitiveC<OneHot>(primitive);\n",
        "    case schema::PrimitiveType_PriorBox:\n",
        "      return NewPrimitiveC<PriorBox>(primitive);\n",
        "    case schema::PrimitiveType_SpaceToDepth:\n",
        "      return NewPrimitiveC<SpaceToDepth>(primitive);\n",
        "    case schema::PrimitiveType_Tile:\n",
        "      return NewPrimitiveC<Tile>(primitive);\n",
        "    case schema::PrimitiveType_Resize:\n",
        "      return NewPrimitiveC<Resize>(primitive);\n",
        "    case schema::PrimitiveType_Unstack:\n",
        "      return NewPrimitiveC<Unstack>(primitive);\n",
        "    case schema::PrimitiveType_Unique:\n",
        "      return NewPrimitiveC<Unique>(primitive);\n",
        "    case schema::PrimitiveType_TopK:\n",
        "      return NewPrimitiveC<TopK>(primitive);\n",
        "    case schema::PrimitiveType_MatMul:\n",
        "      return NewPrimitiveC<MatMul>(primitive);\n",
        "    case schema::PrimitiveType_QuantDTypeCast:\n",
        "      return NewPrimitiveC<QuantDTypeCast>(primitive);\n",
        "    case schema::PrimitiveType_EmbeddingLookup:\n",
        "      return NewPrimitiveC<EmbeddingLookup>(primitive);\n",
        "    case schema::PrimitiveType_Elu:\n",
        "      return NewPrimitiveC<Elu>(primitive);\n",
        "    case schema::PrimitiveType_DeDepthwiseConv2D:\n",
        "      return NewPrimitiveC<DeDepthwiseConv2D>(primitive);\n",
        "    case schema::PrimitiveType_Shape:\n",
        "      return NewPrimitiveC<Shape>(primitive);\n",
        "    case schema::PrimitiveType_Unsqueeze:\n",
        "      return NewPrimitiveC<Unsqueeze>(primitive);\n",
        "    case schema::PrimitiveType_BatchToSpace:\n",
        "      return NewPrimitiveC<BatchToSpace>(primitive);\n",
        "    case schema::PrimitiveType_SpaceToBatch:\n",
        "      return NewPrimitiveC<SpaceToBatch>(primitive);\n",
        "    case schema::PrimitiveType_SpaceToBatchND:\n",
        "      return NewPrimitiveC<SpaceToBatchND>(primitive);\n",
        "    case schema::PrimitiveType_BroadcastTo:\n",
        "      return NewPrimitiveC<BroadcastTo>(primitive);\n",
        "    case schema::PrimitiveType_DepthToSpace:\n",
        "      return NewPrimitiveC<DepthToSpace>(primitive);\n",
        "    case schema::PrimitiveType_Lstm:\n",
        "      return NewPrimitiveC<Lstm>(primitive);\n",
        "    case schema::PrimitiveType_ZerosLike:\n",
        "      return NewPrimitiveC<ZerosLike>(primitive);\n",
        "    case schema::PrimitiveType_MakeTuple:\n",
        "      return NewPrimitiveC<MakeTuple>(primitive);\n",
        "    case schema::PrimitiveType_Where:\n",
        "      return NewPrimitiveC<Where>(primitive);\n",
        "    case schema::PrimitiveType_ScatterND:\n",
        "      return NewPrimitiveC<ScatterND>(primitive);\n",
        "    case schema::PrimitiveType_ConstantOfShape:\n",
        "      return NewPrimitiveC<ConstantOfShape>(primitive);\n",
        "    case schema::PrimitiveType_L2Norm:\n",
        "      return NewPrimitiveC<L2Norm>(primitive);\n",
        "    case schema::PrimitiveType_SparseToDense:\n",
        "      return NewPrimitiveC<SparseToDense>(primitive);\n",
        "    case schema::PrimitiveType_DetectionPostProcess:\n",
        "      return NewPrimitiveC<DetectionPostProcess>(primitive);\n",
        "\n",
        "#ifdef SUPPORT_TRAIN\n",
        "    case schema::PrimitiveType_ActivationGrad:\n",
        "      return NewPrimitiveC<ActivationGrad>(primitive);\n",
        "    case schema::PrimitiveType_PoolingGrad:\n",
        "      return NewPrimitiveC<PoolingGrad>(primitive);\n",
        "    case schema::PrimitiveType_Conv2DGradFilter:\n",
        "      return NewPrimitiveC<Conv2DGradFilter>(primitive);\n",
        "    case schema::PrimitiveType_Conv2DGradInput:\n",
        "      return NewPrimitiveC<Conv2DGradInput>(primitive);\n",
        "    case schema::PrimitiveType_BiasGrad:\n",
        "      return NewPrimitiveC<BiasGrad>(primitive);\n",
        "    case schema::PrimitiveType_ApplyMomentum:\n",
        "      return NewPrimitiveC<ApplyMomentum>(primitive);\n",
        "    case schema::PrimitiveType_BNGrad:\n",
        "      return NewPrimitiveC<BNGrad>(primitive);\n",
        "    case schema::PrimitiveType_AddGrad:\n",
        "      return NewPrimitiveC<ArithmeticGrad>(primitive);\n",
        "    case schema::PrimitiveType_SubGrad:\n",
        "      return NewPrimitiveC<ArithmeticGrad>(primitive);\n",
        "    case schema::PrimitiveType_MulGrad:\n",
        "      return NewPrimitiveC<ArithmeticGrad>(primitive);\n",
        "    case schema::PrimitiveType_DivGrad:\n",
        "     return NewPrimitiveC<ArithmeticGrad>(primitive);\n",
        "#endif\n",
        "    default:\n",
        "      MS_LOG(ERROR) << \"Unsupported primitive type in UnPackFromSchemaPrimitive : \"\n",
        "                    << schema::EnumNamePrimitiveType(op_type);\n",
        "      break;\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "void PrimitiveC::SetQuantType(schema::QuantType quant_type) {\n",
        "  this->quant_type_ = quant_type;\n",
        "}\n",
        "schema::QuantType PrimitiveC::GetQuantType() const { return quant_type_;}\n",
        "#endif\n",
        "\n",
        "int PrimitiveC::Type() const {\n",
        "  if (this->primitive_ == nullptr) {\n",
        "    return schema::PrimitiveType_NONE;\n",
        "  }\n",
        "#ifdef PRIMITIVE_WRITEABLE\n",
        "  return this->primitive_->value.type;\n",
        "#else\n",
        "  return this->primitive_->value_type();\n",
        "#endif\n",
        "}\n",
        "bool PrimitiveC::GetInferFlag() const { return this->infer_flag_; }\n",
        "\n",
        "void PrimitiveC::SetInferFlag(bool flag) { this->infer_flag_ = flag; }\n",
        "\n",
        "int PrimitiveC::InferShape(std::vector<lite::tensor::Tensor *> inputs_, std::vector<lite::tensor::Tensor *> outputs_) {\n",
        "  auto input = inputs_.front();\n",
        "  MS_ASSERT(input != nullptr);\n",
        "  auto output = outputs_.front();\n",
        "  MS_ASSERT(output != nullptr);\n",
        "  output->set_shape(input->shape());\n",
        "  output->set_data_type(input->data_type());\n",
        "  output->SetFormat(input->GetFormat());\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIWO5XRfhbXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include <string>\n",
        "#include <memory>\n",
        "#include <utility>\n",
        "#include \"tools/converter/legacy_optimizer/graph/format_trans_pass.h\"\n",
        "#include \"tools/common/converter_op_utils.h\"\n",
        "#include \"tools/common/node_util.h\"\n",
        "#include \"utils/log_adapter.h\"\n",
        "#include \"src/common/common.h\"\n",
        "#include \"src/common/utils.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "#define kMinInputNum 1\n",
        "#define kOutputNum 1\n",
        "\n",
        "STATUS FormatTransPass::Run(schema::MetaGraphT *graph) {\n",
        "  if (fmkType == converter::FmkType_TF) {\n",
        "    return RET_OK;\n",
        "  }\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  auto status = DoModelInputFormatTrans(graph);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"DoModelInputFormatTrans failed : \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  status = DoNodeInoutFormatTrans(graph);\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"DoNodeInoutFormatTrans failed : \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "STATUS FormatTransPass::DoModelInputFormatTrans(schema::MetaGraphT *graph) {\n",
        "  if (fmkType == converter::FmkType_TF || fmkType == converter::FmkType_TFLITE) {\n",
        "    return RET_OK;\n",
        "  }\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  // insert trans node in model input tensor\n",
        "  if (graph->nodes.empty()) {\n",
        "    return RET_OK;\n",
        "  }\n",
        "  auto graphInputIdxes = graph->inputIndex;\n",
        "  for (size_t i = 0; i < graphInputIdxes.size(); i++) {\n",
        "    bool transed = false;\n",
        "    auto inputIdx = graphInputIdxes.at(i);\n",
        "    MS_ASSERT(inputIdx < subGraph->allTensors.size());\n",
        "    auto &tensor = graph->allTensors.at(inputIdx);\n",
        "    if (tensor->dims.size() != kNCHWDimNumber) {\n",
        "      continue;\n",
        "    }\n",
        "\n",
        "    for (auto iter = graph->nodes.begin(); iter != graph->nodes.end(); iter++) {\n",
        "      auto &node = *iter;\n",
        "      for (size_t inputIndexIdx = 0; inputIndexIdx < node->inputIndex.size(); inputIndexIdx++) {\n",
        "        if (node->inputIndex.at(inputIndexIdx) == inputIdx) {\n",
        "          STATUS status = RET_OK;\n",
        "          iter = InsertFormatTransNode(graph, iter, kBefore, inputIndexIdx, kNHWC2NCHW, &status);\n",
        "          if (status != RET_OK) {\n",
        "            MS_LOG(ERROR) << \"InsertNhwc2NchwNode before \" << (*iter)->name << \" failed\";\n",
        "            return status;\n",
        "          }\n",
        "          // set first tensor format to nhwc\n",
        "          auto &transNode = *(iter - 1);\n",
        "          MS_ASSERT(transNode != nullptr);\n",
        "          MS_ASSERT(transNode->inputIndex.size() == 1);\n",
        "          MS_ASSERT(subGraph->allTensors.size() > transNode->inputIndex.front());\n",
        "          auto &graphInTensor = graph->allTensors.at(transNode->inputIndex.front());\n",
        "          graphInTensor->format = schema::Format_NHWC;\n",
        "          // assume parser not reformat shape\n",
        "          auto oldDims = graphInTensor->dims;\n",
        "          if (!transed) {\n",
        "            graphInTensor->dims = {oldDims[NCHW_N], oldDims[NCHW_H], oldDims[NCHW_W], oldDims[NCHW_C]};\n",
        "            transed = true;\n",
        "          }\n",
        "          break;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "// inference needed inputFormat:\n",
        "//           conv     deconv     depth     dedepth\n",
        "// fp32      NCHW     NCHW       NCHW      NCHW\n",
        "// uint8     NCHW      ?         NCHW        ?\n",
        "STATUS FormatTransPass::DoNodeInoutFormatTrans(schema::MetaGraphT *graph) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  // insert before and after the op cal by nchw/nc4hw4\n",
        "  for (auto iter = graph->nodes.begin(); iter != graph->nodes.end(); iter++) {\n",
        "    FormatTransNodeType beforeNodeType, afterNodeType;\n",
        "    if (fmkType == converter::FmkType_TFLITE) {  // inference by nhwc\n",
        "      continue;\n",
        "    } else if (fmkType == converter::FmkType_CAFFE) {  // inference by nchw\n",
        "      if (!IsContain(GetNhwcOpList(), GetCNodeTType(**iter))) {\n",
        "        continue;\n",
        "      }\n",
        "      beforeNodeType = kNCHW2NHWC;\n",
        "      afterNodeType = kNHWC2NCHW;\n",
        "    } else if (fmkType == converter::FmkType_MS) {\n",
        "      if (!IsContain(GetNhwcOpList(), GetCNodeTType(**iter))) {\n",
        "        continue;\n",
        "      }\n",
        "      beforeNodeType = kNCHW2NHWC;\n",
        "      afterNodeType = kNHWC2NCHW;\n",
        "    } else if (fmkType == converter::FmkType_ONNX) {\n",
        "      if (!IsContain(GetNhwcOpList(), GetCNodeTType(**iter))) {\n",
        "        continue;\n",
        "      }\n",
        "      beforeNodeType = kNCHW2NHWC;\n",
        "      afterNodeType = kNHWC2NCHW;\n",
        "    } else {\n",
        "      MS_LOG(ERROR) << \"Unsupported fmk: \" << fmkType;\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    auto &node = *iter;\n",
        "    auto nodeName = node->name;\n",
        "    if (node->inputIndex.size() < kMinInputNum) {\n",
        "      MS_LOG(ERROR) << \"Op should have \" << kMinInputNum << \" input tensor at least\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    if (node->outputIndex.size() != kOutputNum) {\n",
        "      MS_LOG(ERROR) << \"Op should have \" << kOutputNum << \" output tensor\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    STATUS status;\n",
        "    iter = InsertFormatTransNode(graph, iter, kBefore, 0, beforeNodeType, &status);\n",
        "    if (status != RET_OK) {\n",
        "      MS_LOG(ERROR) << \"InsertNhwc2NchwNode before \" << nodeName << \"failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "\n",
        "    iter = InsertFormatTransNode(graph, iter, kAfter, 0, afterNodeType, &status);\n",
        "    if (status != RET_OK) {\n",
        "      MS_LOG(ERROR) << \"InsertNhwc2NchwNode after \" << nodeName << \"failed\";\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "NodeIter FormatTransPass::InsertFormatTransNode(schema::MetaGraphT *graph, NodeIter existNodeIter, InsertPlace place,\n",
        "                                                size_t inoutIdx, FormatTransNodeType nodeType, STATUS *errorCode) {\n",
        "  MS_ASSERT((*existNodeIter) != nullptr);\n",
        "  auto existNodeName = (*existNodeIter)->name;\n",
        "  std::string tileName;\n",
        "  if (place == kBefore) {\n",
        "    tileName = existNodeName + \"_pre\";\n",
        "  } else {\n",
        "    tileName = existNodeName + \"_post\";\n",
        "  }\n",
        "  auto transNode = std::make_unique<schema::CNodeT>();\n",
        "  transNode->primitive = std::make_unique<schema::PrimitiveT>();\n",
        "\n",
        "  if (nodeType == kNCHW2NHWC) {\n",
        "    transNode->name = \"nchw2nhwc_\" + tileName + std::to_string(id++);\n",
        "    transNode->primitive->value.type = schema::PrimitiveType_Nchw2Nhwc;\n",
        "  } else {\n",
        "    transNode->name = \"nhwc2nchw_\" + tileName + std::to_string(id++);\n",
        "    transNode->primitive->value.type = schema::PrimitiveType_Nhwc2Nchw;\n",
        "  }\n",
        "  return InsertNode(graph, existNodeIter, place, inoutIdx, std::move(transNode), errorCode);\n",
        "}\n",
        "\n",
        "void FormatTransPass::SetQuantType(QuantType quantType) { this->quantType = quantType; }\n",
        "\n",
        "void FormatTransPass::SetFmk(converter::FmkType fmkType) { this->fmkType = fmkType; }\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVuWr1Zqq_Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/converter/legacy_optimizer/graph/infershape_pass.h\"\n",
        "#include <vector>\n",
        "#include \"utils/log_adapter.h\"\n",
        "#include \"include/errorcode.h\"\n",
        "#include \"src/ir/tensor.h\"\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "\n",
        "using mindspore::lite::PrimitiveC;\n",
        "using mindspore::lite::tensor::Tensor;\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "namespace {\n",
        "std::vector<tensor::Tensor *> ConvertTensorToLiteTensor(MetaGraphT *graph, const std::vector<uint32_t> &tensor_indexs,\n",
        "                                                        const schema::PrimitiveType node_type) {\n",
        "  std::vector<tensor::Tensor *> lite_tensors;\n",
        "  for (size_t i = 0; i < tensor_indexs.size(); i++) {\n",
        "    auto &tensorT = graph->allTensors.at(tensor_indexs[i]);\n",
        "    auto tensor_shape = tensorT->dims;\n",
        "    auto lite_tensor =\n",
        "        std::make_unique<tensor::Tensor>(TypeId(tensorT->dataType), tensor_shape, tensorT->format, tensorT->nodeType);\n",
        "    if (lite_tensor == nullptr) {\n",
        "      MS_LOG(ERROR) << \"lite tensor is nullptr\";\n",
        "      return std::vector<tensor::Tensor *>();\n",
        "    }\n",
        "    // reshape op must get tensor data to infershape\n",
        "    if (node_type == schema::PrimitiveType_Reshape && i == 1 && tensorT->nodeType == NodeType_ValueNode) {\n",
        "      auto lite_tensor_size = tensorT->data.size() * sizeof(uint8_t);\n",
        "      // when tensorT as param input\n",
        "      if (lite_tensor_size == 0) {\n",
        "        return std::vector<tensor::Tensor *>();\n",
        "      }\n",
        "      auto tensor_data = std::unique_ptr<char[]>(new(std::nothrow) char[lite_tensor_size / sizeof(char)]);\n",
        "      if (tensor_data == nullptr) {\n",
        "        MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "        return std::vector<tensor::Tensor *>();\n",
        "      }\n",
        "      auto ret = memcpy_s(tensor_data.get(), lite_tensor_size, tensorT->data.data(), lite_tensor_size);\n",
        "      if (ret != EOK) {\n",
        "        MS_LOG(ERROR) << \"memcpy error: \" << ret;\n",
        "        return std::vector<tensor::Tensor *>();\n",
        "      }\n",
        "      lite_tensor->SetData(tensor_data.release());\n",
        "      lite_tensors.emplace_back(lite_tensor.release());\n",
        "      continue;\n",
        "    }\n",
        "    lite_tensors.emplace_back(lite_tensor.release());\n",
        "  }\n",
        "  return lite_tensors;\n",
        "}\n",
        "void PrintTensorShape(std::vector<tensor::Tensor *> &input_tensors, std::vector<tensor::Tensor *> &output_tensors) {\n",
        "  int i = 0;\n",
        "  for (auto input_tensor : input_tensors) {\n",
        "    std::ostringstream oss;\n",
        "    for (auto &dim : input_tensor->shape()) {\n",
        "      oss << \" \" << dim;\n",
        "    }\n",
        "    MS_LOG(DEBUG) << \"input shape \" << i++ << \":\" << oss.str();\n",
        "  }\n",
        "  i = 0;\n",
        "  for (auto output_tensor : output_tensors) {\n",
        "    std::ostringstream oss;\n",
        "    for (auto &dim : output_tensor->shape()) {\n",
        "      oss << \" \" << dim;\n",
        "    }\n",
        "    MS_LOG(DEBUG) << \"output shape\" << i++ << \":\" << oss.str();\n",
        "  }\n",
        "}\n",
        "void FreeTensors(std::vector<tensor::Tensor *> input_tensors, std::vector<tensor::Tensor *> output_tensors) {\n",
        "  input_tensors.clear();\n",
        "  input_tensors.shrink_to_fit();\n",
        "  output_tensors.clear();\n",
        "  output_tensors.shrink_to_fit();\n",
        "}\n",
        "}  // namespace\n",
        "STATUS InferShapePass::Run(MetaGraphT *graph) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  for (auto iter = graph->nodes.begin(); iter != graph->nodes.end(); iter++) {\n",
        "    auto &node = *iter;\n",
        "    auto input_tensors = ConvertTensorToLiteTensor(graph, node->inputIndex, node->primitive->value.type);\n",
        "    std::vector<tensor::Tensor *> output_tensors;\n",
        "    if (input_tensors.empty() || input_tensors.size() != node->inputIndex.size()) {\n",
        "      MS_LOG(ERROR) << \"convert input lite tensor error\";\n",
        "      FreeTensors(input_tensors, output_tensors);\n",
        "      return RET_INFER_ERR;\n",
        "    }\n",
        "    output_tensors = ConvertTensorToLiteTensor(graph, node->outputIndex, node->primitive->value.type);\n",
        "    if (output_tensors.empty() || output_tensors.size() != node->outputIndex.size()) {\n",
        "      MS_LOG(ERROR) << \"convert output lite tensor error\";\n",
        "      FreeTensors(input_tensors, output_tensors);\n",
        "      return RET_INFER_ERR;\n",
        "    }\n",
        "    std::unique_ptr<PrimitiveT> primitiveT(new(std::nothrow) PrimitiveT(*node->primitive));\n",
        "    if (primitiveT == nullptr) {\n",
        "      MS_LOG(ERROR) << \"copy primitiveT error\";\n",
        "      FreeTensors(input_tensors, output_tensors);\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    auto primitiveC = std::shared_ptr<PrimitiveC>(PrimitiveC::UnPackFromSchemaPrimitiveT(primitiveT.release()));\n",
        "    if (primitiveC == nullptr) {\n",
        "      MS_LOG(ERROR) << \"unpack primitiveT error\";\n",
        "      FreeTensors(input_tensors, output_tensors);\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    auto ret = primitiveC->InferShape(input_tensors, output_tensors);\n",
        "    MS_LOG(DEBUG) << \"cur node:\" << node->name;\n",
        "    if (ret == RET_INFER_INVALID) {\n",
        "      MS_LOG(INFO) << \"InferShape shouldn't be done before runtime, name: \" << node->name\n",
        "                   << \", type: \" << schema::EnumNamePrimitiveType(node->primitive->value.type) << \"flag set to false.\";\n",
        "    } else if (ret != RET_OK) {\n",
        "      MS_LOG(WARNING) << \"InferShape failed, name: \" << node->name\n",
        "                      << \", type: \" << schema::EnumNamePrimitiveType(node->primitive->value.type);\n",
        "      FreeTensors(input_tensors, output_tensors);\n",
        "      return RET_INFER_ERR;\n",
        "    }\n",
        "    PrintTensorShape(input_tensors, output_tensors);\n",
        "    // copy output shape to tensorT\n",
        "    for (size_t i = 0; i < output_tensors.size(); i++) {\n",
        "      auto output_dims = output_tensors[i]->shape();\n",
        "      auto &output_tensor = graph->allTensors.at(node->outputIndex[i]);\n",
        "      output_tensor->dims.swap(output_dims);\n",
        "      output_tensor->format = output_tensors[i]->GetFormat();\n",
        "      output_tensor->dataType = output_tensors[i]->data_type();\n",
        "    }\n",
        "    FreeTensors(input_tensors, output_tensors);\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGU4PDAsNQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B24P1lcrlmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWGS_5Sfsenj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk",
        "colab_type": "text"
      },
      "source": [
        "#Data process"
      ]
    }
  ]
}