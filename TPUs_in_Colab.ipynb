{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18a4343c030b4371b899e99d4f8c513c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97212f9b24d549d08e65ef631c788e5e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c9b0b798db64f1298760c85a57f097c",
              "IPY_MODEL_e3fe3c2345634b088bff27a09a10b640"
            ]
          }
        },
        "97212f9b24d549d08e65ef631c788e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c9b0b798db64f1298760c85a57f097c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33c8818e4b764cdcba3e4b6f52f1dbea",
            "_dom_classes": [],
            "description": "Training:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 171,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94e37f508ab34e898ec927961919de34"
          }
        },
        "e3fe3c2345634b088bff27a09a10b640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49f8aa1ae9f84e4b9a2c5177bc74ea4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/171 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c95bbed6e4ba46b18898146970ea9de6"
          }
        },
        "33c8818e4b764cdcba3e4b6f52f1dbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94e37f508ab34e898ec927961919de34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49f8aa1ae9f84e4b9a2c5177bc74ea4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c95bbed6e4ba46b18898146970ea9de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdBPxeSSlAF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/optimizer/fusion/constant_folding_fusion.h\"\n",
        "#include <memory>\n",
        "#include <set>\n",
        "#include <vector>\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include \"tools/anf_exporter/anf_exporter.h\"\n",
        "#include \"src/kernel_registry.h\"\n",
        "#include \"include/context.h\"\n",
        "#include \"src/populate_parameter.h\"\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "\n",
        "using mindspore::lite::KernelRegistry;\n",
        "using mindspore::lite::PrimitiveC;\n",
        "using mindspore::lite::tensor::Tensor;\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "std::vector<Tensor *> GetCNodeInputTensors(const CNodePtr &CNode) {\n",
        "  MS_ASSERT(CNode != nullptr);\n",
        "  auto tmp_meta_graph = std::make_unique<schema::MetaGraphT>();\n",
        "  auto tmp_fb_node = std::make_unique<schema::CNodeT>();\n",
        "  lite::AnfExporter anfExporter;\n",
        "  anfExporter.SetOpInputNode(CNode, tmp_meta_graph, tmp_fb_node.get());\n",
        "  std::vector<Tensor *> input_tensors;\n",
        "  for (auto input_index : tmp_fb_node->inputIndex) {\n",
        "    auto tensorT = tmp_meta_graph->allTensors.at(input_index).get();\n",
        "    auto tensor_shape = tensorT->dims;\n",
        "    auto lite_tensor =\n",
        "        new (std::nothrow) Tensor(TypeId(tensorT->dataType), tensor_shape, tensorT->format, tensorT->nodeType);\n",
        "    if (lite_tensor == nullptr) {\n",
        "      MS_LOG(ERROR) << \"lite tensor is nullptr\";\n",
        "      return input_tensors;\n",
        "    }\n",
        "    auto lite_tensor_size = tensorT->data.size() * sizeof(uint8_t);\n",
        "    // when tensorT as graph input\n",
        "    if (lite_tensor_size <= 0) {\n",
        "      delete lite_tensor;\n",
        "      return input_tensors;\n",
        "    }\n",
        "    auto tensor_data = new (std::nothrow) uint8_t[lite_tensor_size / sizeof(char)];\n",
        "    if (tensor_data == nullptr) {\n",
        "      MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "      delete lite_tensor;\n",
        "      return input_tensors;\n",
        "    }\n",
        "    auto ret = memcpy_s(tensor_data, lite_tensor_size, tensorT->data.data(), lite_tensor_size);\n",
        "    if (ret != EOK) {\n",
        "      delete lite_tensor;\n",
        "      delete[](tensor_data);\n",
        "      MS_LOG(EXCEPTION) << \"memcpy error: \" << ret;\n",
        "    }\n",
        "    lite_tensor->SetData(tensor_data);\n",
        "    input_tensors.emplace_back(lite_tensor);\n",
        "  }\n",
        "  return input_tensors;\n",
        "}\n",
        "\n",
        "ParameterPtr CreateNewParamter(const FuncGraphPtr &func_graph, Tensor *tensor) {\n",
        "  auto parameter = func_graph->add_parameter();\n",
        "  std::vector<int> shape(tensor->shape());\n",
        "  auto type_id = static_cast<TypeId>(tensor->data_type());\n",
        "  auto type_ptr = TypeIdToType(type_id);\n",
        "  auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(type_ptr, shape);\n",
        "  parameter->set_abstract(abstract_tensor);\n",
        "\n",
        "  ParamValueLitePtr param_value = std::make_shared<ParamValueLite>();\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  param_value->set_tensor_shape(shape);\n",
        "  param_value->set_tensor_type(type_id);\n",
        "  param_value->set_format(tensor->GetFormat());\n",
        "  if (tensor->Data() != nullptr) {\n",
        "    auto size = tensor->ElementsNum();\n",
        "    auto tensor_data = new (std::nothrow) float[size];\n",
        "    if (tensor_data == nullptr) {\n",
        "      MS_LOG(ERROR) << \"tensor_data is nullptr\";\n",
        "      return nullptr;\n",
        "    }\n",
        "    auto ret = memcpy_s(tensor_data, size * sizeof(float), tensor->Data(), size * sizeof(float));\n",
        "    if (ret != EOK) {\n",
        "      delete[] tensor_data;\n",
        "      MS_LOG(ERROR) << \"memcpy error: \" << ret;\n",
        "      return nullptr;\n",
        "    }\n",
        "    param_value->set_tensor_addr(tensor_data);\n",
        "    param_value->set_tensor_size(size * sizeof(float) / sizeof(uint8_t));\n",
        "  }\n",
        "  parameter->set_default_param(param_value);\n",
        "  return parameter;\n",
        "}\n",
        "kernel::LiteKernel *GetLiteKernel(std::vector<Tensor *> inputs, std::vector<Tensor *> outputs, OpParameter *parameter,\n",
        "                                  mindspore::lite::PrimitiveC *primitive) {\n",
        "  MS_ASSERT(nullptr != lite_primitive);\n",
        "  auto data_type = inputs.front()->data_type();\n",
        "  kernel::KernelKey desc{kernel::KERNEL_ARCH::kCPU, data_type, (schema::PrimitiveType) primitive->Type()};\n",
        "  lite::Context context;\n",
        "  auto creator = lite::KernelRegistry::GetInstance()->GetCreator(desc);\n",
        "  if (creator != nullptr) {\n",
        "    auto lite_kernel = creator(inputs, outputs, parameter, &context, desc, primitive);\n",
        "    return lite_kernel;\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "lite::STATUS ReplaceCNode(const FuncGraphPtr &func_graph, const CNodePtr &any_node, const AnfNodePtr &input_node,\n",
        "                          std::vector<Tensor *> output_tensors, size_t replace_index) {\n",
        "  MS_ASSERT(func_graph != nullptr);\n",
        "  auto manager = func_graph->manager();\n",
        "  MS_ASSERT(manager != nullptr);\n",
        "  if (output_tensors.size() != 1) {\n",
        "    for (size_t k = 0; k < output_tensors.size(); k++) {\n",
        "      auto used_node_list = GetRealNodeUsedListByOutputIdx(func_graph, input_node, k);\n",
        "      if (used_node_list->size() != 1) {\n",
        "        MS_LOG(ERROR) << \" output must tuple_getitem\";\n",
        "        return lite::RET_ERROR;\n",
        "      }\n",
        "      auto tuple_node = used_node_list->at(0).first;\n",
        "      if (GetCNodeType(tuple_node) == schema::PrimitiveType_TupleGetItem) {\n",
        "        auto new_parameter = CreateNewParamter(func_graph, output_tensors.at(k));\n",
        "        if (new_parameter == nullptr) {\n",
        "          MS_LOG(ERROR) << \"CreateNewParamter failed, name: \" << input_node->fullname_with_scope();\n",
        "          return lite::RET_ERROR;\n",
        "        }\n",
        "        new_parameter->set_name(input_node->fullname_with_scope() + \"_const_\" + std::to_string(k));\n",
        "        manager->Replace(tuple_node, new_parameter);\n",
        "      } else {\n",
        "        MS_LOG(ERROR) << \" multi out tensor must connect tuple-getitem: \" << input_node->fullname_with_scope();\n",
        "        return lite::RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "  } else {\n",
        "    auto new_parameter = CreateNewParamter(func_graph, output_tensors.front());\n",
        "    if (new_parameter == nullptr) {\n",
        "      MS_LOG(ERROR) << \"CreateNewParamter failed, name: \" << input_node->fullname_with_scope();\n",
        "      return lite::RET_ERROR;\n",
        "    }\n",
        "    new_parameter->set_name(input_node->fullname_with_scope());\n",
        "    any_node->set_input(replace_index, new_parameter);\n",
        "  }\n",
        "  return lite::RET_OK;\n",
        "}\n",
        "}  //  namespace\n",
        "void FreeTensors(std::vector<Tensor *> *input_tensor, std::vector<Tensor *> *output_tensor) {\n",
        "  if (input_tensor != nullptr) {\n",
        "    for (size_t i = 0; i < input_tensor->size(); i++) {\n",
        "      delete (*input_tensor)[i];\n",
        "      (*input_tensor)[i] = nullptr;\n",
        "    }\n",
        "  }\n",
        "  if (output_tensor != nullptr) {\n",
        "    for (size_t i = 0; i < output_tensor->size(); i++) {\n",
        "      delete (*output_tensor)[i];\n",
        "      (*output_tensor)[i] = nullptr;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "const AnfNodePtr ConstFoldPass::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                        const EquivPtr &) const {\n",
        "  CheckIfFuncGraphIsNull(func_graph);\n",
        "  CheckIfAnfNodeIsNull(node);\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return node;\n",
        "  }\n",
        "  auto any_node = node->cast<CNodePtr>();\n",
        "  CheckIfCNodeIsNull(any_node);\n",
        "  bool changed = false;\n",
        "  for (size_t i = 1; i < any_node->inputs().size(); i++) {\n",
        "    auto input_node = any_node->input(i);\n",
        "    if (!input_node->isa<CNode>() || !CheckIsAllInputsParam(input_node)) {\n",
        "      continue;\n",
        "    }\n",
        "    auto input_cnode = input_node->cast<CNodePtr>();\n",
        "    auto input_tensors = GetCNodeInputTensors(input_cnode);\n",
        "    if (input_tensors.empty() || input_tensors.size() != input_cnode->inputs().size() - 1) {\n",
        "      FreeTensors(&input_tensors, nullptr);\n",
        "      continue;\n",
        "    }\n",
        "    changed = true;\n",
        "    MS_LOG(INFO) << \"Begin fold node:\" << input_node->fullname_with_scope();\n",
        "    auto output_nums = GetOutputTensorNum(input_cnode);\n",
        "    std::vector<Tensor *> output_tensors{output_nums, new Tensor()};\n",
        "    auto lite_primitive = GetValueNode<std::shared_ptr<PrimitiveC>>(input_cnode->input(0));\n",
        "    if (lite_primitive == nullptr) {\n",
        "      MS_LOG(ERROR) << \"lite_primitive is nullptr\";\n",
        "      FreeTensors(&input_tensors, &output_tensors);\n",
        "      return nullptr;\n",
        "    }\n",
        "    // here, input_tensor's format need to be transposed nhwc according to fmkType,\n",
        "    // but for the time being, we only transpose the tensor with 0/1/2/3D.\n",
        "    // Others should be added in future.\n",
        "    for (size_t j = 0; j < input_tensors.size(); ++j) {\n",
        "      input_tensors[j]->SetFormat(schema::Format_NHWC);\n",
        "      if (input_tensors[j]->shape().size() == 4) {\n",
        "        MS_LOG(INFO) << \"init input_tensor format to nhwc\";\n",
        "      }\n",
        "    }\n",
        "    lite_primitive->InferShape(input_tensors, output_tensors);\n",
        "    auto parameter = kernel::PopulateParameter(lite_primitive.get());\n",
        "    if (parameter == nullptr) {\n",
        "      MS_LOG(ERROR) << \"PopulateParameter return nullptr, type: \"\n",
        "                    << schema::EnumNamePrimitiveType((schema::PrimitiveType) (lite_primitive->Type()));\n",
        "      return nullptr;\n",
        "    }\n",
        "    auto lite_kernel = GetLiteKernel(input_tensors, output_tensors, parameter, lite_primitive.get());\n",
        "    if (lite_kernel == nullptr) {\n",
        "      MS_LOG(ERROR) << \"constant_folding schedule node lite kernel nullptr\";\n",
        "      FreeTensors(&input_tensors, &output_tensors);\n",
        "      return nullptr;\n",
        "    }\n",
        "    auto ret = lite_kernel->Run();\n",
        "    if (0 != ret) {\n",
        "      FreeTensors(&input_tensors, &output_tensors);\n",
        "      MS_LOG(ERROR) << \"run kernel failed, name: \" << lite_kernel->name();\n",
        "      return nullptr;\n",
        "    }\n",
        "    // replace cnode by new param\n",
        "    if (ReplaceCNode(func_graph, any_node, input_node, output_tensors, i) != lite::RET_OK) {\n",
        "      FreeTensors(&input_tensors, &output_tensors);\n",
        "      delete (lite_kernel);\n",
        "      MS_LOG(ERROR) << \"constant_folding replace cnode failed\";\n",
        "      return nullptr;\n",
        "    }\n",
        "    FreeTensors(&input_tensors, &output_tensors);\n",
        "    delete (lite_kernel);\n",
        "  }\n",
        "  return changed ? any_node : nullptr;\n",
        "}\n",
        "}  // namespace mindspore::opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qnitkvFlJzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include <vector>\n",
        "#include <algorithm>\n",
        "#include <utility>\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "#include \"frontend/operator/ops.h\"\n",
        "#include \"backend/optimizer/common/helper.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace opt {\n",
        "namespace {\n",
        "constexpr auto kAnfPrimitiveIndex = 0;\n",
        "bool CheckPrimitiveType(const AnfNodePtr &node, const PrimitivePtr &primitive_type) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return false;\n",
        "  }\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  MS_EXCEPTION_IF_NULL(cnode);\n",
        "  return IsPrimitive(cnode->input(kAnfPrimitiveIndex), primitive_type);\n",
        "}\n",
        "\n",
        "bool IsRealKernel(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  // parameter and value node is not a real kernel too\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return true;\n",
        "  }\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  MS_EXCEPTION_IF_NULL(cnode);\n",
        "  if (cnode->inputs().empty()) {\n",
        "    MS_LOG(EXCEPTION) << \"Illegal null input of cnode(%s)\" << node->DebugString();\n",
        "  }\n",
        "  auto input = cnode->inputs()[0];\n",
        "  bool is_virtual_node = IsPrimitive(input, prim::kPrimImageSummary) || IsPrimitive(input, prim::kPrimScalarSummary) ||\n",
        "      IsPrimitive(input, prim::kPrimTensorSummary) ||\n",
        "      IsPrimitive(input, prim::kPrimHistogramSummary) || IsPrimitive(input, prim::kPrimMakeTuple) ||\n",
        "      IsPrimitive(input, prim::kPrimStateSetItem) || IsPrimitive(input, prim::kPrimDepend) ||\n",
        "      IsPrimitive(input, prim::kPrimTupleGetItem) || IsPrimitive(input, prim::kPrimControlDepend) ||\n",
        "      IsPrimitive(input, prim::kPrimReturn) || IsPrimitive(input, prim::kPrimPartial);\n",
        "  return !is_virtual_node;\n",
        "}\n",
        "\n",
        "ValueNodePtr CreateValueNodeWithSexp(const BaseRef &sexp) {\n",
        "  if (utils::isa<int>(sexp)) {\n",
        "    return NewValueNode(utils::cast<int>(sexp));\n",
        "  }\n",
        "  if (utils::isa<float>(sexp)) {\n",
        "    return NewValueNode(utils::cast<float>(sexp));\n",
        "  }\n",
        "  if (utils::isa<bool>(sexp)) {\n",
        "    return NewValueNode(utils::cast<bool>(sexp));\n",
        "  }\n",
        "  if (utils::isa<ValuePtr>(sexp)) {\n",
        "    return NewValueNode(utils::cast<ValuePtr>(sexp));\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "CNodePtr CreateCNodeWithGraph(const std::vector<AnfNodePtr> &input_nodes, const BaseRef &graph) {\n",
        "  if (utils::isa<FuncGraphPtr>(graph)) {\n",
        "    return std::make_shared<CNode>(input_nodes, utils::cast<FuncGraphPtr>(graph));\n",
        "  }\n",
        "  if (utils::isa<VarPtr>(graph)) {\n",
        "    return std::make_shared<CNode>(input_nodes, utils::cast<VarPtr>(graph));\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "VarNodePtr CreateVarNodeWithSexp(const BaseRef &sexp, const BaseRef &graph) {\n",
        "  if (utils::isa<VarPtr>(graph)) {\n",
        "    MS_LOG(DEBUG) << \"make VarPtr \" + graph.ToString();\n",
        "    return std::make_shared<VarNode>(utils::cast<VarPtr>(sexp), nullptr);\n",
        "  }\n",
        "  if (utils::isa<FuncGraphPtr>(graph)) {\n",
        "    MS_LOG(DEBUG) << \"VarNode, should input a Var in graph. It's GraphPtr: \" + graph.ToString();\n",
        "    return std::make_shared<VarNode>(utils::cast<VarPtr>(sexp), utils::cast<FuncGraphPtr>(graph));\n",
        "  }\n",
        "  MS_LOG(ERROR) << \"VarNode, should input a Var in graph. It's \" + graph.ToString();\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "AnfNodePtr HandleSexpVector(const BaseRef &sexp, const BaseRef &graph, PrimitiveVarMap *primitive_vars,\n",
        "                            bool multigraph) {\n",
        "  MS_LOG(DEBUG) << \"HandleSexpVector sexp: \" + sexp.ToString() + \", graph \" + graph.ToString();\n",
        "  std::vector<AnfNodePtr> input_nodes;\n",
        "  const auto &tuple = utils::cast<VectorRef>(sexp);\n",
        "  if (multigraph && utils::isa<VarPtr>(graph)) {\n",
        "    for (auto &x : tuple) {\n",
        "      AnfNodePtr node = SexpToNode(x, std::make_shared<Var>(\"G\"), primitive_vars, true);\n",
        "      input_nodes.push_back(node);\n",
        "    }\n",
        "    VarPtr var_ptr = utils::cast<VarPtr>(graph);\n",
        "    return std::make_shared<CNode>(input_nodes, var_ptr);\n",
        "  }\n",
        "\n",
        "  for (auto &x : tuple) {\n",
        "    AnfNodePtr node = SexpToNode(x, graph, primitive_vars, multigraph);\n",
        "    input_nodes.push_back(node);\n",
        "  }\n",
        "  return CreateCNodeWithGraph(input_nodes, graph);\n",
        "}\n",
        "}  // namespace\n",
        "\n",
        "bool AnfEqual(const BaseRef &a, const BaseRef &b) {\n",
        "  if (utils::isa<AnfNodePtr>(a) && utils::isa<AnfNodePtr>(b)) {\n",
        "    auto a_node = utils::cast<AnfNodePtr>(a);\n",
        "    auto b_node = utils::cast<AnfNodePtr>(b);\n",
        "    MS_EXCEPTION_IF_NULL(a_node);\n",
        "    MS_EXCEPTION_IF_NULL(b_node);\n",
        "    if (IsValueNode<Primitive>(a_node) && IsValueNode<Primitive>(b_node)) {\n",
        "      auto a_value_node = a_node->cast<ValueNodePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(a_value_node);\n",
        "      auto a_value = a_value_node->value();\n",
        "      MS_EXCEPTION_IF_NULL(a_value);\n",
        "      auto a_prim = a_value->cast<PrimitivePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(a_prim);\n",
        "\n",
        "      auto b_value_node = b_node->cast<ValueNodePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(b_value_node);\n",
        "      auto b_value = b_value_node->value();\n",
        "      MS_EXCEPTION_IF_NULL(b_value);\n",
        "      auto b_prim = b_value->cast<PrimitivePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(b_prim);\n",
        "\n",
        "      return a_prim->cast<PrimitiveCPtr>()->Type() == b_prim->cast<PrimitiveCPtr>()->Type();\n",
        "    } else if (a_node->isa<ValueNode>() && b_node->isa<ValueNode>()) {\n",
        "      auto a_value_node_ptr = a_node->cast<ValueNodePtr>();\n",
        "      if (a_value_node_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"cast value node ptr fail\";\n",
        "      }\n",
        "      auto a_value_ptr = a_value_node_ptr->value();\n",
        "      if (a_value_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"value ptr is nullptr\";\n",
        "      }\n",
        "\n",
        "      auto b_value_node_ptr = b_node->cast<ValueNodePtr>();\n",
        "      if (b_value_node_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"cast value node ptr fail\";\n",
        "      }\n",
        "      auto b_value_ptr = b_value_node_ptr->value();\n",
        "      if (b_value_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"value ptr is nullptr\";\n",
        "      }\n",
        "\n",
        "      if (utils::isa<lite::PrimitiveC>(a_value_ptr) && utils::isa<lite::PrimitiveC>(b_value_ptr)) {\n",
        "        auto a_obj = (lite::PrimitiveC *) (a_value_ptr.get());\n",
        "        auto b_obj = (lite::PrimitiveC *) (b_value_ptr.get());\n",
        "        return (*a_obj) == (*b_obj);\n",
        "      } else {\n",
        "        return (*a_value_ptr) == (*b_value_ptr);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  if (a.m_ptr->isa<lite::PrimitiveC>() && b.m_ptr->isa<lite::PrimitiveC>()) {\n",
        "    auto a_value_node_ptr = a.m_ptr->cast<PrimitiveCPtr>();\n",
        "    auto b_value_node_ptr = b.m_ptr->cast<PrimitiveCPtr>();\n",
        "    return a_value_node_ptr->Type() == b_value_node_ptr->Type();\n",
        "  }\n",
        "\n",
        "  return a == b;\n",
        "}\n",
        "\n",
        "bool CNodeTypeEqual(const BaseRef &a, const BaseRef &b) {\n",
        "  // To matchCNode and Kernel's type\n",
        "  if (utils::isa<CNode>(a) && utils::isa<CNode>(b)) {\n",
        "    return true;\n",
        "  }\n",
        "  return a.type() == b.type();\n",
        "}\n",
        "\n",
        "AnfNodePtr SexpToNode(const BaseRef &sexp, const BaseRef &graph, PrimitiveVarMap *primitive_vars, bool multigraph) {\n",
        "  MS_LOG(DEBUG) << \"SexpToNode sexp: \" + sexp.ToString() + \", graph \" + graph.ToString();\n",
        "  MS_EXCEPTION_IF_NULL(primitive_vars);\n",
        "  if (utils::isa<VectorRef>(sexp)) {\n",
        "    return HandleSexpVector(sexp, graph, primitive_vars, multigraph);\n",
        "  }\n",
        "  if (utils::isa<VarPtr>(sexp)) {\n",
        "    auto var_ptr = utils::cast<VarPtr>(sexp);\n",
        "    MS_EXCEPTION_IF_NULL(var_ptr);\n",
        "    if (var_ptr->primitive()) {\n",
        "      (*primitive_vars)[var_ptr->primitive()] = var_ptr;\n",
        "      return NewValueNode(var_ptr->primitive());\n",
        "    }\n",
        "    return CreateVarNodeWithSexp(sexp, graph);\n",
        "  }\n",
        "  if (utils::isa<AnfNodePtr>(sexp)) {\n",
        "    return utils::cast<AnfNodePtr>(sexp);\n",
        "  }\n",
        "  auto value_node = CreateValueNodeWithSexp(sexp);\n",
        "  if (value_node == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"sexp cannot converted. sexp: \" + sexp.ToString();\n",
        "  }\n",
        "  return value_node;\n",
        "}\n",
        "\n",
        "bool IsRealCNodeKernel(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  // parameter and value node is not a real cnode kernel\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return false;\n",
        "  }\n",
        "  // return considered as a real node\n",
        "  if (CheckPrimitiveType(node, prim::kPrimReturn)) {\n",
        "    return true;\n",
        "  }\n",
        "  return IsRealKernel(node);\n",
        "}\n",
        "bool IsGraphKernel(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  // graph kernel should be a real cnode kernel.\n",
        "  if (!IsRealCNodeKernel(node)) {\n",
        "    return false;\n",
        "  }\n",
        "\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  MS_EXCEPTION_IF_NULL(cnode);\n",
        "  auto input = cnode->input(kAnfPrimitiveIndex);\n",
        "  // graph kernel should has func_graph as first input.\n",
        "  if (!IsValueNode<FuncGraph>(input)) {\n",
        "    return false;\n",
        "  }\n",
        "\n",
        "  auto func_graph = GetValueNode<FuncGraphPtr>(input);\n",
        "  MS_EXCEPTION_IF_NULL(func_graph);\n",
        "  return func_graph->has_attr(FUNC_GRAPH_ATTR_GRAPH_KERNEL);\n",
        "}\n",
        "\n",
        "void CheckIfFuncGraphIsNull(const FuncGraphPtr &graph) {\n",
        "  if (graph == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The graph is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfAnfNodeIsNull(const AnfNodePtr &node) {\n",
        "  if (node == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The AnfNode is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfCNodeIsNull(const CNodePtr &node) {\n",
        "  if (node == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The CNode is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfVarIsNull(const VarPtr &var) {\n",
        "  if (var == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The Var is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfNodeIsParam(const AnfNodePtr &node) {\n",
        "  if (node != nullptr && !utils::isa<ParameterPtr>(node)) {\n",
        "    MS_LOG(EXCEPTION) << \"The Node is not param.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckInputSize(const CNodePtr &node, const int size) {\n",
        "  if (static_cast<int>(node->inputs().size()) != size) {\n",
        "    MS_LOG(EXCEPTION) << \"The input size of node must be \" << size << \", but it is\" << node->inputs().size();\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckLeastInputSize(const CNodePtr &node, const int size) {\n",
        "  if (static_cast<int>(node->inputs().size()) < size) {\n",
        "    MS_LOG(EXCEPTION) << \"The input size of node must be \" << size << \", but it is\" << node->inputs().size();\n",
        "  }\n",
        "}\n",
        "\n",
        "ParameterPtr AddNewBiasNode(float *bias_data, const FuncGraphPtr &func_graph, int kernel_num,\n",
        "                            const ParamValueLitePtr &weight_tensor) {\n",
        "  auto bias_parameter = func_graph->add_parameter();\n",
        "  MS_ASSERT(bias_parameter != nullptr);\n",
        "  std::vector<int> shape = {kernel_num};\n",
        "  auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(TypeIdToType(weight_tensor->tensor_type()), shape);\n",
        "  bias_parameter->set_abstract(abstract_tensor);\n",
        "\n",
        "  ParamValueLitePtr param_value = std::make_shared<ParamValueLite>();\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  param_value->set_tensor_addr(bias_data);\n",
        "  param_value->set_tensor_size(kernel_num * sizeof(float) / sizeof(uint8_t));\n",
        "  param_value->set_format(weight_tensor->format());\n",
        "  param_value->set_tensor_type(weight_tensor->tensor_type());\n",
        "  param_value->set_tensor_shape(shape);\n",
        "  bias_parameter->set_default_param(param_value);\n",
        "  return bias_parameter;\n",
        "}\n",
        "\n",
        "schema::PrimitiveType GetCNodeType(const BaseRef &n) {\n",
        "  ValueNodePtr value_node;\n",
        "  if (utils::isa<CNodePtr>(n)) {\n",
        "    auto in = utils::cast<CNodePtr>(n);\n",
        "    value_node = in->input(0)->cast<ValueNodePtr>();\n",
        "  } else if (utils::isa<ValueNodePtr>(n)) {\n",
        "    value_node = utils::cast<ValueNodePtr>(n);\n",
        "  } else {\n",
        "    MS_LOG(EXCEPTION) << \"only value node or cnode has type\";\n",
        "    return schema::PrimitiveType_NONE;\n",
        "  }\n",
        "  MS_EXCEPTION_IF_NULL(value_node);\n",
        "  auto value = value_node->value();\n",
        "  MS_ASSERT(value != nullptr);\n",
        "  if (utils::isa<PrimitiveCPtr>(value)) {\n",
        "    auto primitive = value->cast<PrimitiveCPtr>();\n",
        "    MS_ASSERT(primitive != nullptr);\n",
        "    return (schema::PrimitiveType) primitive->Type();\n",
        "  } else if (utils::isa<Primitive>(value)) {\n",
        "    auto primitive = value->cast<PrimitivePtr>();\n",
        "    MS_ASSERT(primitive != nullptr);\n",
        "    MS_LOG(INFO) << \"anf primitive node type:\" << primitive->name();\n",
        "    return schema::PrimitiveType_NONE;\n",
        "  }\n",
        "  return schema::PrimitiveType_NONE;\n",
        "}\n",
        "\n",
        "bool IsParamNode(const BaseRef &n) {\n",
        "  if (!utils::isa<ParameterPtr>(n)) {\n",
        "    return false;\n",
        "  }\n",
        "  auto param = utils::cast<ParameterPtr>(n)->default_param();\n",
        "  auto tensor = std::dynamic_pointer_cast<ParamValueLite>(param);\n",
        "  if (tensor == nullptr) {\n",
        "    return false;\n",
        "  }\n",
        "  return tensor->tensor_addr() != nullptr;\n",
        "}\n",
        "\n",
        "bool IsConvNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Conv2D || type == schema::PrimitiveType_DepthwiseConv2D;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "bool IsPoolingNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Pooling;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "bool CheckIsAllInputsParam(const AnfNodePtr &node) {\n",
        "  if (utils::isa<CNode>(node)) {\n",
        "    auto cnode = node->cast<CNodePtr>();\n",
        "    for (size_t i = 1; i < cnode->inputs().size(); i++) {\n",
        "      if (!utils::isa<Parameter>(cnode->input(i))) {\n",
        "        return false;\n",
        "      }\n",
        "    }\n",
        "    return true;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "size_t GetOutputTensorNum(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  auto type = node->Type();\n",
        "  if (type == nullptr) {\n",
        "    return 1;\n",
        "  }\n",
        "  if (type->isa<Tuple>()) {\n",
        "    auto tuple_type = type->cast<TuplePtr>();\n",
        "    MS_EXCEPTION_IF_NULL(tuple_type);\n",
        "    return tuple_type->size();\n",
        "  } else if (type->isa<TensorType>() || type->isa<Number>()) {\n",
        "    return 1;\n",
        "  } else if (type->isa<TypeNone>()) {\n",
        "    return 0;\n",
        "  } else {\n",
        "    return 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "bool IsMultiOutputTensors(const FuncGraphPtr &graph, const AnfNodePtr &node) {\n",
        "  auto output_node_list = GetRealNodeUsedList(graph, node);\n",
        "  if (output_node_list->size() != 1) {\n",
        "    MS_LOG(DEBUG) << \"fusion node has multi output nodes\";\n",
        "    return true;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "std::shared_ptr<std::vector<std::pair<AnfNodePtr, int>>> GetRealNodeUsedList(const FuncGraphPtr &graph,\n",
        "                                                                             const AnfNodePtr &node) {\n",
        "  auto output_node_list = std::make_shared<std::vector<std::pair<AnfNodePtr, int>>>();\n",
        "  MS_EXCEPTION_IF_NULL(graph);\n",
        "  auto manager = graph->manager();\n",
        "  MS_EXCEPTION_IF_NULL(manager);\n",
        "  auto iter = manager->node_users().find(node);\n",
        "  if (iter == manager->node_users().end()) {\n",
        "    MS_LOG(EXCEPTION) << \"node has no output in manager\";\n",
        "  }\n",
        "  auto output_info_list = iter->second;\n",
        "  std::copy(output_info_list.begin(), output_info_list.end(), std::back_inserter(*output_node_list));\n",
        "  return output_node_list;\n",
        "}\n",
        "size_t GetTupleGetItemOutIndex(const CNodePtr &tuple_get_item) {\n",
        "  MS_ASSERT(tuple_get_item != nullptr);\n",
        "  if (tuple_get_item->size() != kTupleGetItemInputSize) {\n",
        "    MS_LOG(ERROR) << \"The node tuple_get_item must have 2 inputs!\";\n",
        "    return -1;\n",
        "  }\n",
        "  auto output_index_value_node = tuple_get_item->input(kInputNodeOutputIndexInTupleGetItem);\n",
        "  MS_ASSERT(output_index_value_node != nullptr);\n",
        "  auto value_node = output_index_value_node->cast<ValueNodePtr>();\n",
        "  MS_ASSERT(value_node != nullptr);\n",
        "  return IntToSize(GetValue<int>(value_node->value()));\n",
        "}\n",
        "std::shared_ptr<std::vector<std::pair<AnfNodePtr, int>>> GetRealNodeUsedListByOutputIdx(const FuncGraphPtr &graph,\n",
        "                                                                                        const AnfNodePtr &node,\n",
        "                                                                                        size_t output_index) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  MS_ASSERT(output_index >= 0);\n",
        "  auto output_node_list = std::make_shared<std::vector<std::pair<AnfNodePtr, int>>>();\n",
        "  auto manager = graph->manager();\n",
        "  MS_ASSERT(manager != nullptr);\n",
        "  auto iter = manager->node_users().find(node);\n",
        "  if (iter == manager->node_users().end()) {\n",
        "    MS_LOG(ERROR) << \"node has no output in manager\";\n",
        "    return output_node_list;\n",
        "  }\n",
        "  auto output_info_list = iter->second;\n",
        "  for (const auto &output_info : output_info_list) {\n",
        "    size_t used_output_index;\n",
        "    if (GetCNodeType(output_info.first) == schema::PrimitiveType_TupleGetItem) {\n",
        "      used_output_index = GetTupleGetItemOutIndex(utils::cast<CNodePtr>(output_info.first));\n",
        "    } else if (GetCNodeType(node) == schema::PrimitiveType_TupleGetItem) {\n",
        "      used_output_index = output_index;\n",
        "    } else {\n",
        "      if (output_index != 0) {\n",
        "        MS_LOG(ERROR) << \"node has no output in manager\";\n",
        "        return output_node_list;\n",
        "      }\n",
        "      return output_node_list;\n",
        "    }\n",
        "    if (used_output_index == output_index) {\n",
        "      output_node_list->push_back(output_info);\n",
        "    }\n",
        "  }\n",
        "  return output_node_list;\n",
        "}\n",
        "}  // namespace opt\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HORtT0tvpzX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_LITE_SRC_PASS_COMMON_GLLO_UTILS_H_\n",
        "#define MINDSPORE_LITE_SRC_PASS_COMMON_GLLO_UTILS_H_\n",
        "\n",
        "#include <memory>\n",
        "#include \"src/ops//primitive_c.h\"\n",
        "#include \"ir/anf.h\"\n",
        "#include \"ir/func_graph.h\"\n",
        "#include \"src/common/utils.h\"\n",
        "#include \"backend/optimizer/common/pattern_engine.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"src/param_value_lite.h\"\n",
        "\n",
        "using PrimitiveCPtr = std::shared_ptr<mindspore::lite::PrimitiveC>;\n",
        "namespace mindspore {\n",
        "namespace opt {\n",
        "bool IsRealCNodeKernel(const AnfNodePtr &node);\n",
        "\n",
        "bool IsGraphKernel(const AnfNodePtr &node);\n",
        "\n",
        "void CheckIfFuncGraphIsNull(const FuncGraphPtr &graph);\n",
        "\n",
        "void CheckIfAnfNodeIsNull(const AnfNodePtr &node);\n",
        "\n",
        "void CheckIfCNodeIsNull(const CNodePtr &node);\n",
        "\n",
        "void CheckIfVarIsNull(const VarPtr &var);\n",
        "\n",
        "void CheckInputSize(const CNodePtr &node, int size);\n",
        "\n",
        "void CheckIfNodeIsParam(const AnfNodePtr &node);\n",
        "\n",
        "void CheckLeastInputSize(const CNodePtr &node, int size);\n",
        "\n",
        "ParameterPtr AddNewBiasNode(float *bias_data, const FuncGraphPtr &func_graph, int kernel_num,\n",
        "                            const ParamValueLitePtr &weight_tensor);\n",
        "\n",
        "schema::PrimitiveType GetCNodeType(const BaseRef &node);\n",
        "\n",
        "bool IsParamNode(const BaseRef &n);\n",
        "\n",
        "bool IsConvNode(const BaseRef &n);\n",
        "\n",
        "bool IsPoolingNode(const BaseRef &n);\n",
        "\n",
        "bool CheckIsAllInputsParam(const AnfNodePtr &node);\n",
        "\n",
        "size_t GetOutputTensorNum(const AnfNodePtr &node);\n",
        "\n",
        "bool IsMultiOutputTensors(const FuncGraphPtr &graph, const AnfNodePtr &node);\n",
        "\n",
        "size_t GetTupleGetItemOutIndex(const CNodePtr &tuple_get_item);\n",
        "}  // namespace opt\n",
        "}  // namespace mindspore\n",
        "#endif  // MINDSPORE_LITE_SRC_PASS_COMMON_GLLO_UTILS_H_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNEpd7uIdMIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSGqPkiLq2eK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.Inf\n",
        "        else:\n",
        "            self.val_score = -np.Inf\n",
        "\n",
        "    def __call__(self, epoch_score, model, model_path):\n",
        "\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        self.val_score = epoch_score\n",
        "\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIWO5XRfhbXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals) / len(vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVuWr1Zqq_Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *\n",
        "import tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGU4PDAsNQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ./input/roberta-base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B24P1lcrlmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = './input/roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "config = RobertaConfig.from_pretrained('roberta-base')\n",
        "tokenizer.save_vocabulary(save_path)\n",
        "model.save_pretrained(save_path)\n",
        "config.save_pretrained(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWGS_5Sfsenj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class config:\n",
        "    FOLD = 0\n",
        "    LEARNING_RATE = 0.2 * 3e-5\n",
        "    MAX_LEN = 192\n",
        "    TRAIN_BATCH_SIZE = 16\n",
        "    VALID_BATCH_SIZE = 8\n",
        "    EPOCHS = 3\n",
        "    TRAINING_FILE = \"./tweet-sentiment/train_folds.csv\"\n",
        "    ROBERTA_PATH = \"./input/roberta-base\"\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
        "        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
        "        lowercase=True,\n",
        "        add_prefix_space=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk",
        "colab_type": "text"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nl8SqDItPsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "    \n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "    \n",
        "    targets_start = target_idx[0]\n",
        "    targets_end = target_idx[-1]\n",
        "\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1XkJ_uOtk5U",
        "colab_type": "text"
      },
      "source": [
        "#Data loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOidlXY8uO8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item], \n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1tiaim0uWB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n",
        "        self.drop_out = nn.Dropout(0.1)\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "        out = self.drop_out(out)\n",
        "        logits = self.l0(out)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_D9iCOPueGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EJr5IHPuji1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n",
        "    model.train()\n",
        "    tk0 = tqdm(data_loader, total=num_batches, desc=\"Training\", disable=not xm.is_master_ordinal())\n",
        "    for bi, d in enumerate(tk0):\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        sentiment = d[\"sentiment\"]\n",
        "        orig_selected = d[\"orig_selected\"]\n",
        "        orig_tweet = d[\"orig_tweet\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        offsets = d[\"offsets\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets_start = targets_start.to(device, dtype=torch.long)\n",
        "        targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs_start, outputs_end = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        scheduler.step()\n",
        "        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n",
        "        tk0.set_postfix(loss=print_loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDxCSB91unsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    jaccards = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for bi, d in enumerate(data_loader):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            sentiment = d[\"sentiment\"]\n",
        "            orig_selected = d[\"orig_selected\"]\n",
        "            orig_tweet = d[\"orig_tweet\"]\n",
        "            targets_start = d[\"targets_start\"]\n",
        "            targets_end = d[\"targets_end\"]\n",
        "            offsets = d[\"offsets\"].cpu().numpy()\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets_start = targets_start.to(device, dtype=torch.long)\n",
        "            targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                tweet_sentiment = sentiment[px]\n",
        "                jaccard_score, _ = calculate_jaccard_score(\n",
        "                    original_tweet=tweet,\n",
        "                    target_string=selected_tweet,\n",
        "                    sentiment_val=tweet_sentiment,\n",
        "                    idx_start=np.argmax(outputs_start[px, :]),\n",
        "                    idx_end=np.argmax(outputs_end[px, :]),\n",
        "                    offsets=offsets[px]\n",
        "                )\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            losses.update(loss.item(), ids.size(0))\n",
        "\n",
        "    return jaccards.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb7KafbguwPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n",
        "model_config.output_hidden_states = True\n",
        "MX = TweetModel(conf=model_config)\n",
        "\n",
        "dfx = pd.read_csv(config.TRAINING_FILE)\n",
        "\n",
        "df_train = dfx[dfx.kfold != config.FOLD].reset_index(drop=True)\n",
        "df_valid = dfx[dfx.kfold == config.FOLD].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehNC95DRu3EL",
        "colab_type": "text"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqDrsrE2u1oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run():\n",
        "    device = xm.xla_device()\n",
        "    model = MX.to(device)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      train_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=True\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      valid_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=False\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=False,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\n",
        "        \"bias\",\n",
        "        \"LayerNorm.bias\",\n",
        "        \"LayerNorm.weight\"\n",
        "    ]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if not any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "         'weight_decay': 0.001\n",
        "        },\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "            'weight_decay': 0.0\n",
        "        },\n",
        "    ]\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE / xm.xrt_world_size() * config.EPOCHS\n",
        "    )\n",
        "    optimizer = AdamW(\n",
        "        optimizer_parameters, \n",
        "        lr=config.LEARNING_RATE * xm.xrt_world_size()\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_jac = 0\n",
        "    es = EarlyStopping(patience=2, mode=\"max\")\n",
        "    num_batches = int(len(df_train) / (config.TRAIN_BATCH_SIZE * xm.xrt_world_size()))\n",
        "    \n",
        "    xm.master_print(\"Training is Starting....\")\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "        train_fn(\n",
        "            para_loader.per_device_loader(device), \n",
        "            model, \n",
        "            optimizer, \n",
        "            device,\n",
        "            num_batches,\n",
        "            scheduler\n",
        "        )\n",
        "\n",
        "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "        jac = eval_fn(\n",
        "            para_loader.per_device_loader(device), \n",
        "            model, \n",
        "            device\n",
        "        )\n",
        "        jac = xm.mesh_reduce('jac_reduce', jac, reduce_fn)\n",
        "        xm.master_print(f'Epoch={epoch}, Jaccard={jac}')\n",
        "        if jac > best_jac:\n",
        "            xm.master_print(\"Model Improved!!! Saving Model\")\n",
        "            xm.save(model.state_dict(), f\"model_{config.FOLD}.bin\")\n",
        "            best_jac = jac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHlyHRTVvEwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    a = run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99O6yUgLvHgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "18a4343c030b4371b899e99d4f8c513c",
            "97212f9b24d549d08e65ef631c788e5e",
            "2c9b0b798db64f1298760c85a57f097c",
            "e3fe3c2345634b088bff27a09a10b640",
            "33c8818e4b764cdcba3e4b6f52f1dbea",
            "94e37f508ab34e898ec927961919de34",
            "49f8aa1ae9f84e4b9a2c5177bc74ea4b",
            "c95bbed6e4ba46b18898146970ea9de6"
          ]
        },
        "outputId": "30bc90da-0640-45d1-8873-5724f16c8935"
      },
      "source": [
        "FLAGS={}\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training is Starting....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18a4343c030b4371b899e99d4f8c513c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Training', max=171.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}