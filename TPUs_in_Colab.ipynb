{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3nFGKv3fHDm"
      },
      "source": [
        "diff --git a/mindspore/lite/examples/export_models/models/densenet_train_export.py b/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "index df785e8..3ebc019 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "@@ -23,10 +23,6 @@ from mindspore import context, Tensor, nn\n",
        " from mindspore.train.serialization import export\n",
        " from official.cv.densenet121.src.network.densenet import DenseNet121\n",
        " sys.path.append(os.environ['CLOUD_MODEL_ZOO'] + 'official/cv/densenet121/')\n",
        "-#pylint: disable=wrong-import-position\n",
        "-\n",
        "-\n",
        "-\n",
        " \n",
        " \n",
        " context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\", save_graphs=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/train_utils.py b/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "index 5017b8f..ee5ed2e 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "@@ -38,6 +38,10 @@ def save_t(t, file):\n",
        "     x = t.asnumpy()\n",
        "     x.tofile(file)\n",
        " \n",
        "+def save_txt(t,file):\n",
        "+    with os.fdopen(file, 'w') as f:\n",
        "+        for j in t.asnumpy().flatten():\n",
        "+            f.write(str(j)+' ')\n",
        " \n",
        " def save_inout(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "     \"\"\"save_inout\"\"\"\n",
        "@@ -64,9 +68,8 @@ def save_inout(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "     if isinstance(y, tuple):\n",
        "         i = 1\n",
        "         for t in y:\n",
        "-            with os.fdopen(name + \"_output\" + str(i) + \".bin\", 'w') as f:\n",
        "-                for j in t.asnumpy().flatten():\n",
        "-                    f.write(str(j)+' ')\n",
        "+            y_name = name + \"_output\" + str(i) + \".bin\"\n",
        "+            save_txt(y, y_name)\n",
        "             i = i + 1\n",
        "     else:\n",
        "         y_name = name + \"_output1.bin\"\n",
        "diff --git a/mindspore/lite/examples/train_lenet/src/net_runner.cc b/mindspore/lite/examples/train_lenet/src/net_runner.cc\n",
        "index f2ede8f..2b88407 100644\n",
        "--- a/mindspore/lite/examples/train_lenet/src/net_runner.cc\n",
        "+++ b/mindspore/lite/examples/train_lenet/src/net_runner.cc\n",
        "@@ -91,7 +91,7 @@ bool after_callback(const std::vector<mindspore::tensor::MSTensor *> &after_inpu\n",
        "     auto d = reinterpret_cast<float *>(after_outputs.at(i)->MutableData());\n",
        "     int num2p = (after_outputs.at(i)->ElementsNum());\n",
        "     printf(\"ou%zu(%d): \", i, num2p);\n",
        "-    if (num2p > 10) num2p = 10;\n",
        "+    if (num2p > kPrintNum) num2p = kPrintNum;\n",
        "     for (int j = 0; j < num2p; j++) printf(\"%f, \", d[j]);\n",
        "     printf(\"\\n\");\n",
        "   }\n",
        "diff --git a/mindspore/lite/examples/transfer_learning/model/train_utils.py b/mindspore/lite/examples/transfer_learning/model/train_utils.py\n",
        "index 53585c8..1dc01a0 100644\n",
        "--- a/mindspore/lite/examples/transfer_learning/model/train_utils.py\n",
        "+++ b/mindspore/lite/examples/transfer_learning/model/train_utils.py\n",
        "@@ -17,6 +17,7 @@\n",
        " import mindspore.nn as nn\n",
        " from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        "+\n",
        " def train_wrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "     \"\"\"\n",
        "     train_wrap\n",
        "diff --git a/mindspore/lite/src/train/classification_train_accuracy_monitor.cc b/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "index b734427..d3da48b 100644\n",
        "--- a/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "+++ b/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "@@ -49,7 +49,7 @@ void ClassificationTrainAccuracyMonitor::EpochBegin(const session::TrainLoopCall\n",
        " int ClassificationTrainAccuracyMonitor::EpochEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "   if (cb_data.step_ > 0) accuracies_.at(cb_data.epoch_).second /= static_cast<float>(cb_data.step_ + 1);\n",
        "   if ((cb_data.epoch_ + 1) % print_every_n_ == 0) {\n",
        "-    std::cout << \"Epoch (\" << cb_data.epoch_ + 1 << \"):\\tTraining Accuracy is \" << accuracies_.at(cb_data.epoch_).second\n",
        "+    std::cout << \"Epoch (\" << (cb_data.epoch_ + 1) << \"):\\tTraining Accuracy is \" << accuracies_.at(cb_data.epoch_).second\n",
        "               << std::endl;\n",
        "   }\n",
        "   return mindspore::session::RET_CONTINUE;\n",
        "diff --git a/mindspore/lite/src/train/train_populate_parameter.cc b/mindspore/lite/src/train/train_populate_parameter.cc\n",
        "index 7d7f155..e2cf808 100644\n",
        "--- a/mindspore/lite/src/train/train_populate_parameter.cc\n",
        "+++ b/mindspore/lite/src/train/train_populate_parameter.cc\n",
        "@@ -184,6 +184,20 @@ void SetConvParam(ConvParameter *param, const flatbuffers::Vector<int64_t> *kern\n",
        "   param->pad_r_ = pad_list->Get(kNHWCCDim);\n",
        " }\n",
        " \n",
        "+void SetConvActivation(ConvParameter *param, schema::ActivationType activation_type) {\n",
        "+  switch (activation_type) {\n",
        "+    case schema::ActivationType_RELU:\n",
        "+      param->act_type_ = ActType_Relu;\n",
        "+      break;\n",
        "+    case schema::ActivationType_RELU6:\n",
        "+      param->act_type_ = ActType_Relu6;\n",
        "+      break;\n",
        "+    default:\n",
        "+      param->act_type_ = ActType_No;\n",
        "+      break;\n",
        "+  }\n",
        "+}\n",
        "+\n",
        " OpParameter *PopulateConvolutionGradFilterParameter(const void *prim) {\n",
        "   ConvParameter *param = reinterpret_cast<ConvParameter *>(malloc(sizeof(ConvParameter)));\n",
        "   if (param == nullptr) {\n",
        "@@ -196,17 +210,7 @@ OpParameter *PopulateConvolutionGradFilterParameter(const void *prim) {\n",
        "   param->op_parameter_.type_ = primitive->value_type();\n",
        "   SetConvParam(param, value->kernel_size(), value->stride(), value->dilation(), value->pad_list());\n",
        "   param->group_ = value->group();\n",
        "-  param->act_type_ = ActType_No;\n",
        "-  switch (value->activation_type()) {\n",
        "-    case schema::ActivationType_RELU:\n",
        "-      param->act_type_ = ActType_Relu;\n",
        "-      break;\n",
        "-    case schema::ActivationType_RELU6:\n",
        "-      param->act_type_ = ActType_Relu6;\n",
        "-      break;\n",
        "-    default:\n",
        "-      break;\n",
        "-  }\n",
        "+  SetConvActivation(param,value->activation_type());\n",
        "   return reinterpret_cast<OpParameter *>(param);\n",
        " }\n",
        " \n",
        "@@ -220,19 +224,8 @@ OpParameter *PopulateConvolutionGradInputParameter(const void *prim) {\n",
        "   auto value = primitive->value_as_Conv2DBackpropInputFusion();\n",
        "   param->op_parameter_.type_ = primitive->value_type();\n",
        "   SetConvParam(param, value->kernel_size(), value->stride(), value->dilation(), value->pad_list());\n",
        "+  SetConvActivation(param,value->activation_type());\n",
        "   param->group_ = value->group();\n",
        "-  param->act_type_ = ActType_No;\n",
        "-  switch (value->activation_type()) {\n",
        "-    case schema::ActivationType_RELU:\n",
        "-      param->act_type_ = ActType_Relu;\n",
        "-      break;\n",
        "-    case schema::ActivationType_RELU6:\n",
        "-      param->act_type_ = ActType_Relu6;\n",
        "-      break;\n",
        "-    default:\n",
        "-      break;\n",
        "-  }\n",
        "-\n",
        "   return reinterpret_cast<OpParameter *>(param);\n",
        " }\n",
        " \n",
        "diff --git a/mindspore/lite/src/train/train_populate_parameter_v0.cc b/mindspore/lite/src/train/train_populate_parameter_v0.cc\n",
        "index 8f52614..c9fbaa4 100644\n",
        "--- a/mindspore/lite/src/train/train_populate_parameter_v0.cc\n",
        "+++ b/mindspore/lite/src/train/train_populate_parameter_v0.cc\n",
        "@@ -89,8 +89,7 @@ OpParameter *DefaultPopulateParameter(const void *primitive) {\n",
        " \n",
        "   return param;\n",
        " }\n",
        "-\n",
        "-OpParameter *PopulateSmoothL1LossParameter(const void *primitive) {\n",
        "+SmoothL1LossParameter * MallocSmoothL1LossParam(const void *primitive) {\n",
        "   if (primitive == nullptr) {\n",
        "     MS_LOG(ERROR) << \"Primitive is nullptr when populating parameter for op.\";\n",
        "     return nullptr;\n",
        "@@ -101,8 +100,12 @@ OpParameter *PopulateSmoothL1LossParameter(const void *primitive) {\n",
        "     MS_LOG(ERROR) << \"malloc SmoothL1LossParameter failed.\";\n",
        "     return nullptr;\n",
        "   }\n",
        "+  return p;\n",
        "+}\n",
        "+OpParameter *PopulateSmoothL1LossParameter(const void *primitive) {\n",
        "+  auto p = MallocSmoothL1LossParam(primitive);\n",
        "   p->op_parameter_.type_ = schema::PrimitiveType_SmoothL1Loss;\n",
        "-\n",
        "+  auto *prim = static_cast<const schema::v0::Primitive *>(primitive);\n",
        "   auto smoothL1Loss_prim = prim->value_as_SmoothL1Loss();\n",
        " \n",
        "   p->beta_ = smoothL1Loss_prim->beta();\n",
        "@@ -110,18 +113,9 @@ OpParameter *PopulateSmoothL1LossParameter(const void *primitive) {\n",
        " }\n",
        " \n",
        " OpParameter *PopulateSmoothL1LossGradParameter(const void *primitive) {\n",
        "-  if (primitive == nullptr) {\n",
        "-    MS_LOG(ERROR) << \"Primitive is nullptr when populating parameter for op.\";\n",
        "-    return nullptr;\n",
        "-  }\n",
        "-  auto *prim = static_cast<const schema::v0::Primitive *>(primitive);\n",
        "-  SmoothL1LossParameter *p = reinterpret_cast<SmoothL1LossParameter *>(malloc(sizeof(SmoothL1LossParameter)));\n",
        "-  if (p == nullptr) {\n",
        "-    MS_LOG(ERROR) << \"malloc SmoothL1LossParameter failed.\";\n",
        "-    return nullptr;\n",
        "-  }\n",
        "+  auto p = MallocSmoothL1LossParam(primitive);\n",
        "   p->op_parameter_.type_ = schema::PrimitiveType_SmoothL1LossGrad;\n",
        "-\n",
        "+  auto *prim = static_cast<const schema::v0::Primitive *>(primitive);\n",
        "   auto smoothL1LossGrad_prim = prim->value_as_SmoothL1LossGrad();\n",
        " \n",
        "   p->beta_ = smoothL1LossGrad_prim->beta();\n",
        "@@ -327,9 +321,23 @@ OpParameter *PopulateActivationGradParameter(const void *primitive) {\n",
        "   return reinterpret_cast<OpParameter *>(act_param);\n",
        " }\n",
        " \n",
        "+void SetConvActivation(ConvParameter *param, schema::v0::ActivationType activation_type) {\n",
        "+  switch (activation_type) {\n",
        "+    case schema::v0::ActivationType_RELU:\n",
        "+      param->act_type_ = ActType_Relu;\n",
        "+      break;\n",
        "+    case schema::v0::ActivationType_RELU6:\n",
        "+      param->act_type_ = ActType_Relu6;\n",
        "+      break;\n",
        "+    default:\n",
        "+      param->act_type_ = ActType_No;\n",
        "+      break;\n",
        "+  }\n",
        "+}\n",
        "+\n",
        " OpParameter *PopulateConvolutionGradFilterParameter(const void *primitive) {\n",
        "   if (primitive == nullptr) {\n",
        "-    MS_LOG(ERROR) << \"Primitive is nullptr when populating parameter for op.\";\n",
        "+    MS_LOG(ERROR) << \"Primitive is nullptr when populating conv grad filter parameter for op.\";\n",
        "     return nullptr;\n",
        "   }\n",
        "   auto *prim = static_cast<const schema::v0::Primitive *>(primitive);\n",
        "@@ -360,31 +368,21 @@ OpParameter *PopulateConvolutionGradFilterParameter(const void *primitive) {\n",
        "   param->pad_l_ = convolutionGradFilter_prim->padLeft();\n",
        "   param->pad_r_ = convolutionGradFilter_prim->padRight();\n",
        "   param->group_ = convolutionGradFilter_prim->group();\n",
        "+  SetConvActivation(param, convolutionGradFilter_prim->activationType());\n",
        "   param->act_type_ = ActType_No;\n",
        "-  switch (convolutionGradFilter_prim->activationType()) {\n",
        "-    case schema::v0::ActivationType_RELU:\n",
        "-      param->act_type_ = ActType_Relu;\n",
        "-      break;\n",
        "-    case schema::v0::ActivationType_RELU6:\n",
        "-      param->act_type_ = ActType_Relu6;\n",
        "-      break;\n",
        "-    default:\n",
        "-      break;\n",
        "-  }\n",
        "-\n",
        "   return reinterpret_cast<OpParameter *>(param);\n",
        " }\n",
        " \n",
        " OpParameter *PopulateConvolutionGradInputParameter(const void *primitive) {\n",
        "   if (primitive == nullptr) {\n",
        "-    MS_LOG(ERROR) << \"Primitive is nullptr when populating parameter for op.\";\n",
        "+    MS_LOG(ERROR) << \"Primitive is nullptr when populating conv grad input parameter for op.\";\n",
        "     return nullptr;\n",
        "   }\n",
        "   auto *prim = static_cast<const schema::v0::Primitive *>(primitive);\n",
        " \n",
        "   ConvParameter *param = reinterpret_cast<ConvParameter *>(malloc(sizeof(ConvParameter)));\n",
        "   if (param == nullptr) {\n",
        "-    MS_LOG(ERROR) << \"malloc Param for conv grad filter failed.\";\n",
        "+    MS_LOG(ERROR) << \"malloc Param for conv grad input failed.\";\n",
        "     return nullptr;\n",
        "   }\n",
        "   param->op_parameter_.type_ = schema::PrimitiveType_Conv2DBackpropInputFusion;\n",
        "@@ -409,16 +407,7 @@ OpParameter *PopulateConvolutionGradInputParameter(const void *primitive) {\n",
        "   param->pad_r_ = convolutionGradInput_prim->padRight();\n",
        "   param->group_ = convolutionGradInput_prim->group();\n",
        "   param->act_type_ = ActType_No;\n",
        "-  switch (convolutionGradInput_prim->activationType()) {\n",
        "-    case schema::v0::ActivationType_RELU:\n",
        "-      param->act_type_ = ActType_Relu;\n",
        "-      break;\n",
        "-    case schema::v0::ActivationType_RELU6:\n",
        "-      param->act_type_ = ActType_Relu6;\n",
        "-      break;\n",
        "-    default:\n",
        "-      break;\n",
        "-  }\n",
        "+  SetConvActivation(param, convolutionGradInput_prim->activationType());\n",
        " \n",
        "   return reinterpret_cast<OpParameter *>(param);\n",
        " }\n",
        "@@ -456,17 +445,7 @@ OpParameter *PopulateGroupConvolutionGradInputParameter(const void *primitive) {\n",
        "   param->pad_l_ = groupConvolutionGradInput_prim->padLeft();\n",
        "   param->pad_r_ = groupConvolutionGradInput_prim->padRight();\n",
        "   param->group_ = groupConvolutionGradInput_prim->group();\n",
        "-  param->act_type_ = ActType_No;\n",
        "-  switch (groupConvolutionGradInput_prim->activationType()) {\n",
        "-    case schema::v0::ActivationType_RELU:\n",
        "-      param->act_type_ = ActType_Relu;\n",
        "-      break;\n",
        "-    case schema::v0::ActivationType_RELU6:\n",
        "-      param->act_type_ = ActType_Relu6;\n",
        "-      break;\n",
        "-    default:\n",
        "-      break;\n",
        "-  }\n",
        "+  SetConvActivation(param, groupConvolutionGradInput_prim->activationType());\n",
        " \n",
        "   return reinterpret_cast<OpParameter *>(param);\n",
        " }\n",
        "diff --git a/mindspore/lite/src/train/train_session.cc b/mindspore/lite/src/train/train_session.cc\n",
        "index ead56e7..bd6df04 100644\n",
        "--- a/mindspore/lite/src/train/train_session.cc\n",
        "+++ b/mindspore/lite/src/train/train_session.cc\n",
        "@@ -23,9 +23,8 @@\n",
        " #include <fstream>\n",
        " #include <memory>\n",
        " #include \"include/errorcode.h\"\n",
        "-#include \"src/common/utils.h\"\n",
        " #include \"src/tensor.h\"\n",
        "-#include \"src/train/loss_kernel.h\"\n",
        "+#include \"src/common/utils.h\"\n",
        " #include \"src/train/optimizer_kernel.h\"\n",
        " #include \"src/sub_graph_kernel.h\"\n",
        " #include \"src/train/train_populate_parameter.h\"\n",
        "@@ -58,7 +57,7 @@ std::unique_ptr<char[]> ReadFileToBuf(const std::string &filename, size_t *size)\n",
        "   }\n",
        "   size_t fsize = static_cast<size_t>(tellg_ret);\n",
        " \n",
        "-  std::unique_ptr<char[]> buf(new (std::nothrow) char[fsize]);\n",
        "+  auto buf = std::make_unique<char[]> (fsize);\n",
        "   if (buf == nullptr) {\n",
        "     MS_LOG(ERROR) << \"malloc buf failed, file: \" << filename;\n",
        "     ifs.close();\n",
        "@@ -237,7 +236,7 @@ int TrainSession::Train() {\n",
        "   train_mode_ = true;\n",
        "   virtual_batch_idx_ = 0;\n",
        "   for (auto kernel : this->train_kernels_) {\n",
        "-    MS_ASSERT(nullptr != kernel);\n",
        "+    MS_ASSERT( kernel != nullptr);\n",
        "     auto ret = kernel->Train();\n",
        "     if (ret != RET_OK) {\n",
        "       MS_LOG(ERROR) << kernel->name() << \" failed to set train mode\";\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}