{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIWO5XRfhbXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/fusion/weight_format_hardcode_pass.h\"\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "\n",
        "using mindspore::lite::converter::FmkType_CAFFE;\n",
        "using mindspore::lite::converter::FmkType_TFLITE;\n",
        "using mindspore::lite::converter::FmkType_ONNX;\n",
        "using mindspore::lite::converter::FmkType_MS;\n",
        "using mindspore::schema::QuantType_WeightQuant;\n",
        "using mindspore::schema::QuantType_QUANT_NONE;\n",
        "using mindspore::schema::QuantType_AwareTraining;\n",
        "using mindspore::schema::QuantType_PostTraining;\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "constexpr size_t kConvWeightIndex = 2;\n",
        "bool IsConvExtendNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Conv2D || type == schema::PrimitiveType_DepthwiseConv2D ||\n",
        "        type == schema::PrimitiveType_DeConv2D || type == schema::PrimitiveType_DeDepthwiseConv2D;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "}  // namespace\n",
        "void WeightFormatHardCodePass::SetQuantType(QuantType type) {\n",
        "  this->quant_type = type;\n",
        "}\n",
        "void WeightFormatHardCodePass::SetFmkType(FmkType type) {\n",
        "  this->fmk_type = type;\n",
        "}\n",
        "lite::STATUS WeightFormatHardCodePass::HardCodeCAFFE(const AnfNodePtr &conv_node,\n",
        "                                                     const ParamValueLitePtr &param_value) const {\n",
        "  MS_ASSERT(conv_cnode != nullptr);\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  switch (quant_type) {\n",
        "    case QuantType_WeightQuant:\n",
        "    case QuantType_QUANT_NONE:param_value->set_format(schema::Format::Format_KCHW);\n",
        "      break;\n",
        "    default: {\n",
        "      MS_LOG(ERROR) << \"Unsupported quantType: \" << EnumNameQuantType(quant_type) << \", node: \"\n",
        "                    << conv_node->fullname_with_scope();\n",
        "      return lite::RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  return lite::RET_OK;\n",
        "}\n",
        "\n",
        "lite::STATUS WeightFormatHardCodePass::HardCodeONNX(const AnfNodePtr &conv_node,\n",
        "                                                    const ParamValueLitePtr &param_value) const {\n",
        "  MS_ASSERT(conv_cnode != nullptr);\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  auto op_type = GetCNodeType(conv_node);\n",
        "  switch (this->quant_type) {\n",
        "    case QuantType_AwareTraining: {\n",
        "      // sum up from current onnx quant models\n",
        "      if (op_type == schema::PrimitiveType_Conv2D) {\n",
        "        param_value->set_format(schema::Format::Format_KHWC);\n",
        "      } else if (op_type == schema::PrimitiveType_DepthwiseConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CHWK);\n",
        "      } else if (op_type == schema::PrimitiveType_DeConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CKHW);\n",
        "      } else {\n",
        "        MS_LOG(ERROR) << \"Unsupported opType: \" << EnumNamePrimitiveType(op_type) << \", node: \"\n",
        "                      << conv_node->fullname_with_scope();\n",
        "        return lite::RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case QuantType_WeightQuant:\n",
        "    case QuantType_QUANT_NONE: {\n",
        "      // conv (K x C/group x kH x kW) group = 1\n",
        "      // depth (K x C/group x kH x kW) group = channelOut ==> (K, multiplier, H, W)\n",
        "      // deconv (C x K/group x kH x kW) group = 1\n",
        "      // dedepth (C x K/group x kH x kW) group = channelIn ==> (C, multiplier, H, W)\n",
        "      if (op_type == schema::PrimitiveType_Conv2D || op_type == schema::PrimitiveType_DepthwiseConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_KCHW);\n",
        "      } else if (op_type == schema::PrimitiveType_DeConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CKHW);\n",
        "      } else {\n",
        "        MS_LOG(ERROR) << \"Unsupported opType: \" << EnumNamePrimitiveType(op_type) << \", node: \"\n",
        "                      << conv_node->fullname_with_scope();\n",
        "        return lite::RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    default: {\n",
        "      MS_LOG(ERROR) << \"Unsupported quantType: \" << EnumNameQuantType(quant_type) << \", node: \"\n",
        "                    << conv_node->fullname_with_scope();\n",
        "      return lite::RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  return lite::RET_OK;\n",
        "}\n",
        "\n",
        "lite::STATUS WeightFormatHardCodePass::HardCodeMS(const AnfNodePtr &conv_node,\n",
        "                                                  const ParamValueLitePtr &param_value) const {\n",
        "  MS_ASSERT(conv_cnode != nullptr);\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  auto op_type = GetCNodeType(conv_node);\n",
        "  switch (this->quant_type) {\n",
        "    case QuantType_AwareTraining: {\n",
        "      if (op_type == schema::PrimitiveType_Conv2D) {\n",
        "        param_value->set_format(schema::Format::Format_KCHW);\n",
        "      } else if (op_type == schema::PrimitiveType_DepthwiseConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CKHW);\n",
        "      } else {\n",
        "        param_value->set_format(schema::Format::Format_KCHW);\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case QuantType_WeightQuant:\n",
        "    case QuantType_QUANT_NONE: {\n",
        "      // sum up from current ms quant models\n",
        "      if (op_type == schema::PrimitiveType_Conv2D) {\n",
        "        param_value->set_format(schema::Format::Format_KCHW);\n",
        "      } else if (op_type == schema::PrimitiveType_DepthwiseConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CKHW);\n",
        "      } else if (op_type == schema::PrimitiveType_DeConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_KCHW);\n",
        "      } else {\n",
        "        MS_LOG(ERROR) << \"Unsupported opType: \" << EnumNamePrimitiveType(op_type) << \", node: \"\n",
        "                      << conv_node->fullname_with_scope();\n",
        "        return lite::RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    default: {\n",
        "      MS_LOG(ERROR) << \"Unsupported quantType: \" << EnumNameQuantType(quant_type) << \", node: \"\n",
        "                    << conv_node->fullname_with_scope();\n",
        "      return lite::RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  return lite::RET_OK;\n",
        "}\n",
        "\n",
        "lite::STATUS WeightFormatHardCodePass::HardCodeTFLITE(const AnfNodePtr &conv_node,\n",
        "                                                      const ParamValueLitePtr &param_value) const {\n",
        "  MS_ASSERT(conv_cnode != nullptr);\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  auto op_type = GetCNodeType(conv_node);\n",
        "  switch (this->quant_type) {\n",
        "    case QuantType_AwareTraining:\n",
        "    case QuantType_PostTraining:\n",
        "    case QuantType_WeightQuant:\n",
        "    case QuantType_QUANT_NONE: {\n",
        "      if (op_type == schema::PrimitiveType_Conv2D) {\n",
        "        param_value->set_format(schema::Format::Format_KHWC);\n",
        "      } else if (op_type == schema::PrimitiveType_DepthwiseConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CHWK);\n",
        "      } else if (op_type == schema::PrimitiveType_DeConv2D) {\n",
        "        param_value->set_format(schema::Format::Format_CHWK);\n",
        "      } else {\n",
        "        MS_LOG(ERROR) << \"Unsupported opType: \" << EnumNamePrimitiveType(op_type) << \", node: \"\n",
        "                      << conv_node->fullname_with_scope();\n",
        "        return lite::RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    default: {\n",
        "      MS_LOG(ERROR) << \"Unsupported opType: \" << EnumNamePrimitiveType(op_type) << \", node: \"\n",
        "                    << conv_node->fullname_with_scope();\n",
        "      return lite::RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "  return lite::RET_OK;\n",
        "}\n",
        "\n",
        "const BaseRef WeightFormatHardCodePass::DefinePattern() const {\n",
        "  auto conv_var = std::make_shared<CondVar>(IsConvExtendNode);\n",
        "  auto weight_var = std::make_shared<CondVar>(IsParamNode);\n",
        "  auto other_var = std::make_shared<SeqVar>();\n",
        "  return VectorRef({conv_var, conv_var, weight_var, other_var});\n",
        "}\n",
        "\n",
        "const AnfNodePtr WeightFormatHardCodePass::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                                   const EquivPtr &) const {\n",
        "  MS_ASSERT(func_graph != nullptr);\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  if (!utils::isa<CNode>(node)) {\n",
        "    MS_LOG(ERROR) << \"weight format hardcode pass only support cnode\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto conv_cnode = node->cast<CNodePtr>();\n",
        "  MS_ASSERT(conv_cnode->inputs().size() > kConvWeightIndex);\n",
        "  auto param_value = GetLiteParamValue(conv_cnode->input(kConvWeightIndex));\n",
        "  lite::STATUS status;\n",
        "  switch (fmk_type) {\n",
        "    case FmkType_CAFFE:status = HardCodeCAFFE(node, param_value);\n",
        "      break;\n",
        "    case FmkType_TFLITE:status = HardCodeTFLITE(node, param_value);\n",
        "      break;\n",
        "    case FmkType_ONNX:status = HardCodeONNX(node, param_value);\n",
        "      break;\n",
        "    case FmkType_MS:status = HardCodeMS(node, param_value);\n",
        "      break;\n",
        "    default:MS_LOG(ERROR) << \"Unsupported fmkType: \" << fmk_type << \", node: \" << node->fullname_with_scope();\n",
        "      return nullptr;\n",
        "  }\n",
        "  if (status != lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"schema::Format hardCode faild: \" << status << \", node: \" << node->fullname_with_scope();\n",
        "    return nullptr;\n",
        "  }\n",
        "  return node;\n",
        "}\n",
        "}  // namespace mindspore::opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVuWr1Zqq_Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_LITE_SRC_PASS_FUSION_WEIGHT_FORMAT_HARDCODE_PASS_H_\n",
        "#define MINDSPORE_LITE_SRC_PASS_FUSION_WEIGHT_FORMAT_HARDCODE_PASS_H_\n",
        "#include <string>\n",
        "#include \"backend/optimizer/common/optimizer.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"tools/converter/converter_flags.h\"\n",
        "#include \"src/param_value_lite.h\"\n",
        "\n",
        "using mindspore::lite::converter::FmkType;\n",
        "using mindspore::schema::QuantType;\n",
        "namespace mindspore::opt {\n",
        "class WeightFormatHardCodePass : public PatternProcessPass {\n",
        " public:\n",
        "  explicit WeightFormatHardCodePass(bool multigraph = true) : PatternProcessPass(\"weight_format_hardcode_pass\",\n",
        "                                                                                 multigraph) {}\n",
        "  ~WeightFormatHardCodePass() override = default;\n",
        "  void SetQuantType(QuantType type);\n",
        "  void SetFmkType(FmkType fmkType);\n",
        "  const BaseRef DefinePattern() const override;\n",
        "  const AnfNodePtr Process(const FuncGraphPtr &, const AnfNodePtr &, const EquivPtr &) const override;\n",
        "\n",
        " private:\n",
        "  lite::STATUS HardCodeCAFFE(const AnfNodePtr &node, const ParamValueLitePtr &param_value) const;\n",
        "  lite::STATUS HardCodeONNX(const AnfNodePtr &node, const ParamValueLitePtr &param_value) const;\n",
        "  lite::STATUS HardCodeMS(const AnfNodePtr &node, const ParamValueLitePtr &param_value) const;\n",
        "  lite::STATUS HardCodeTFLITE(const AnfNodePtr &node, const ParamValueLitePtr &param_value) const;\n",
        "\n",
        " private:\n",
        "  QuantType quant_type = schema::QuantType_QUANT_NONE;\n",
        "  FmkType fmk_type = lite::converter::FmkType_TF;\n",
        "};\n",
        "}  // namespace mindspore::opt\n",
        "#endif  // MINDSPORE_LITE_SRC_PASS_FUSION_WEIGHT_FORMAT_HARDCODE_PASS_H_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGU4PDAsNQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/fusion/weight_format_transform_pass.h\"\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "\n",
        "using mindspore::lite::converter::FmkType_CAFFE;\n",
        "using mindspore::lite::converter::FmkType_TFLITE;\n",
        "using mindspore::lite::converter::FmkType_ONNX;\n",
        "using mindspore::lite::converter::FmkType_MS;\n",
        "using mindspore::schema::QuantType_WeightQuant;\n",
        "using mindspore::schema::QuantType_QUANT_NONE;\n",
        "using mindspore::schema::QuantType_AwareTraining;\n",
        "using mindspore::schema::QuantType_PostTraining;\n",
        "\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "constexpr size_t kConvWeightIndex = 2;\n",
        "}  // namespace\n",
        "void WeightFormatTransformPass::SetQuantType(QuantType type) {\n",
        "  this->quant_type = type;\n",
        "}\n",
        "void WeightFormatTransformPass::SetFmkType(FmkType type) {\n",
        "  this->fmk_type = type;\n",
        "}\n",
        "void WeightFormatTransformPass::SetDstFormat(schema::Format format) {\n",
        "  this->dst_format = format;\n",
        "}\n",
        "lite::STATUS WeightFormatTransformPass::ConvWeightFormatTrans(const FuncGraphPtr &graph, bool quant_flag) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  std::vector<AnfNodePtr> node_list = TopoSort(graph->get_return());\n",
        "  for (auto &node : node_list) {\n",
        "    if (!utils::isa<CNodePtr>(node)) {\n",
        "      continue;\n",
        "    }\n",
        "    auto type = opt::GetCNodeType(node);\n",
        "    if (quant_flag) {\n",
        "      if (type != schema::PrimitiveType_Conv2D && type != schema::PrimitiveType_DepthwiseConv2D) {\n",
        "        continue;\n",
        "      }\n",
        "    } else {\n",
        "      if (type != schema::PrimitiveType_Conv2D && type != schema::PrimitiveType_DepthwiseConv2D\n",
        "          && type != schema::PrimitiveType_DeConv2D && type != schema::PrimitiveType_DeDepthwiseConv2D) {\n",
        "        continue;\n",
        "      }\n",
        "    }\n",
        "    auto conv_cnode = node->cast<CNodePtr>();\n",
        "    MS_ASSERT(conv_cnode->inputs().size() > kConvWeightIndex);\n",
        "    auto weight_node = conv_cnode->input(kConvWeightIndex);\n",
        "    auto weight_value = GetLiteParamValue(weight_node);\n",
        "    MS_ASSERT(weight_value->tensor_type() == TypeId::kNumberTypeFloat32\n",
        "                  || weight_value->tensor_type() == TypeId::kNumberTypeUInt8);\n",
        "    lite::STATUS status;\n",
        "    schema::Format weight_dst_format = schema::Format::Format_KHWC;\n",
        "    if (dst_format != schema::Format::Format_NUM_OF_FORMAT) {\n",
        "      weight_dst_format = dst_format;\n",
        "    }\n",
        "    status = TransFilterFormat(weight_value, weight_dst_format);\n",
        "    if (status == RET_OK) {\n",
        "      weight_value->set_format(weight_dst_format);\n",
        "    } else {\n",
        "      MS_LOG(ERROR) << \"TransFilter \" << EnumNameFormat(schema::EnumValuesFormat()[weight_value->format()]) << \"To\"\n",
        "                    << EnumNameFormat(weight_dst_format) << \" failed, node : \" << node->fullname_with_scope()\n",
        "                    << \"quant type:\" << quant_type;\n",
        "      return ERROR;\n",
        "    }\n",
        "    auto type_id = static_cast<TypeId>(weight_value->tensor_type());\n",
        "    auto type_ptr = TypeIdToType(type_id);\n",
        "    auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(type_ptr, weight_value->tensor_shape());\n",
        "    weight_node->set_abstract(abstract_tensor);\n",
        "  }\n",
        "  return\n",
        "      RET_OK;\n",
        "}\n",
        "\n",
        "bool WeightFormatTransformPass::Run(const FuncGraphPtr &func_graph) {\n",
        "  MS_ASSERT(func_graph != nullptr);\n",
        "  bool quant_flag = false;\n",
        "  if (this->quant_type == QuantType_AwareTraining || this->quant_type == QuantType_WeightQuant) {\n",
        "    quant_flag = true;\n",
        "  }\n",
        "  auto status = ConvWeightFormatTrans(func_graph, quant_flag);\n",
        "  if (status != lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"NonQuantDataFormatTrans failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "}  // namespace mindspore::opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B24P1lcrlmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_LITE_SRC_PASS_FUSION_WEIGHT_FORMAT_TRANSFORM_PASS_H_\n",
        "#define MINDSPORE_LITE_SRC_PASS_FUSION_WEIGHT_FORMAT_TRANSFORM_PASS_H_\n",
        "#include <string>\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"tools/converter/converter_flags.h\"\n",
        "#include \"backend/optimizer/common/pass.h\"\n",
        "\n",
        "using mindspore::lite::converter::FmkType;\n",
        "using mindspore::schema::QuantType;\n",
        "namespace mindspore::opt {\n",
        "class WeightFormatTransformPass : public Pass {\n",
        " public:\n",
        "  explicit WeightFormatTransformPass() : Pass(\"weight_format_transform_pass\") {}\n",
        "  ~WeightFormatTransformPass() override = default;\n",
        "  void SetQuantType(QuantType type);\n",
        "  void SetFmkType(FmkType fmkType);\n",
        "  void SetDstFormat(schema::Format format);\n",
        "  bool Run(const FuncGraphPtr &graph) override;\n",
        "\n",
        " private:\n",
        "  lite::STATUS ConvWeightFormatTrans(const FuncGraphPtr &graph, bool quant_flag);\n",
        "\n",
        " private:\n",
        "  QuantType quant_type = schema::QuantType_QUANT_NONE;\n",
        "  FmkType fmk_type = lite::converter::FmkType_TF;\n",
        "  schema::Format dst_format = schema::Format::Format_NUM_OF_FORMAT;\n",
        "};\n",
        "}  // namespace mindspore::opt\n",
        "#endif  // MINDSPORE_LITE_SRC_PASS_FUSION_WEIGHT_FORMAT_TRANSFORM_PASS_H_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWGS_5Sfsenj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"tools/optimizer/common/gllo_utils.h\"\n",
        "#include <vector>\n",
        "#include <algorithm>\n",
        "#include <utility>\n",
        "#include \"src/ops/primitive_c.h\"\n",
        "#include \"src/common/common.h\"\n",
        "#include \"frontend/operator/ops.h\"\n",
        "#include \"backend/optimizer/common/helper.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace opt {\n",
        "namespace {\n",
        "constexpr auto kAnfPrimitiveIndex = 0;\n",
        "bool CheckPrimitiveType(const AnfNodePtr &node, const PrimitivePtr &primitive_type) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return false;\n",
        "  }\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  MS_EXCEPTION_IF_NULL(cnode);\n",
        "  return IsPrimitive(cnode->input(kAnfPrimitiveIndex), primitive_type);\n",
        "}\n",
        "\n",
        "bool IsRealKernel(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  // parameter and value node is not a real kernel too\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return true;\n",
        "  }\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  MS_EXCEPTION_IF_NULL(cnode);\n",
        "  if (cnode->inputs().empty()) {\n",
        "    MS_LOG(EXCEPTION) << \"Illegal null input of cnode(%s)\" << node->DebugString();\n",
        "  }\n",
        "  auto input = cnode->inputs()[0];\n",
        "  bool is_virtual_node = IsPrimitive(input, prim::kPrimImageSummary) || IsPrimitive(input, prim::kPrimScalarSummary) ||\n",
        "                         IsPrimitive(input, prim::kPrimTensorSummary) ||\n",
        "                         IsPrimitive(input, prim::kPrimHistogramSummary) || IsPrimitive(input, prim::kPrimMakeTuple) ||\n",
        "                         IsPrimitive(input, prim::kPrimStateSetItem) || IsPrimitive(input, prim::kPrimDepend) ||\n",
        "                         IsPrimitive(input, prim::kPrimTupleGetItem) || IsPrimitive(input, prim::kPrimControlDepend) ||\n",
        "                         IsPrimitive(input, prim::kPrimReturn) || IsPrimitive(input, prim::kPrimPartial);\n",
        "  return !is_virtual_node;\n",
        "}\n",
        "\n",
        "ValueNodePtr CreateValueNodeWithSexp(const BaseRef &sexp) {\n",
        "  if (utils::isa<int>(sexp)) {\n",
        "    return NewValueNode(utils::cast<int>(sexp));\n",
        "  }\n",
        "  if (utils::isa<float>(sexp)) {\n",
        "    return NewValueNode(utils::cast<float>(sexp));\n",
        "  }\n",
        "  if (utils::isa<bool>(sexp)) {\n",
        "    return NewValueNode(utils::cast<bool>(sexp));\n",
        "  }\n",
        "  if (utils::isa<ValuePtr>(sexp)) {\n",
        "    return NewValueNode(utils::cast<ValuePtr>(sexp));\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "CNodePtr CreateCNodeWithGraph(const std::vector<AnfNodePtr> &input_nodes, const BaseRef &graph) {\n",
        "  if (utils::isa<FuncGraphPtr>(graph)) {\n",
        "    return std::make_shared<CNode>(input_nodes, utils::cast<FuncGraphPtr>(graph));\n",
        "  }\n",
        "  if (utils::isa<VarPtr>(graph)) {\n",
        "    return std::make_shared<CNode>(input_nodes, utils::cast<VarPtr>(graph));\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "VarNodePtr CreateVarNodeWithSexp(const BaseRef &sexp, const BaseRef &graph) {\n",
        "  if (utils::isa<VarPtr>(graph)) {\n",
        "    MS_LOG(DEBUG) << \"make VarPtr \" + graph.ToString();\n",
        "    return std::make_shared<VarNode>(utils::cast<VarPtr>(sexp), nullptr);\n",
        "  }\n",
        "  if (utils::isa<FuncGraphPtr>(graph)) {\n",
        "    MS_LOG(DEBUG) << \"VarNode, should input a Var in graph. It's GraphPtr: \" + graph.ToString();\n",
        "    return std::make_shared<VarNode>(utils::cast<VarPtr>(sexp), utils::cast<FuncGraphPtr>(graph));\n",
        "  }\n",
        "  MS_LOG(ERROR) << \"VarNode, should input a Var in graph. It's \" + graph.ToString();\n",
        "  return nullptr;\n",
        "}\n",
        "\n",
        "AnfNodePtr HandleSexpVector(const BaseRef &sexp, const BaseRef &graph, PrimitiveVarMap *primitive_vars,\n",
        "                            bool multigraph) {\n",
        "  MS_LOG(DEBUG) << \"HandleSexpVector sexp: \" + sexp.ToString() + \", graph \" + graph.ToString();\n",
        "  std::vector<AnfNodePtr> input_nodes;\n",
        "  const auto &tuple = utils::cast<VectorRef>(sexp);\n",
        "  if (multigraph && utils::isa<VarPtr>(graph)) {\n",
        "    for (auto &x : tuple) {\n",
        "      AnfNodePtr node = SexpToNode(x, std::make_shared<Var>(\"G\"), primitive_vars, true);\n",
        "      input_nodes.push_back(node);\n",
        "    }\n",
        "    VarPtr var_ptr = utils::cast<VarPtr>(graph);\n",
        "    return std::make_shared<CNode>(input_nodes, var_ptr);\n",
        "  }\n",
        "\n",
        "  for (auto &x : tuple) {\n",
        "    AnfNodePtr node = SexpToNode(x, graph, primitive_vars, multigraph);\n",
        "    input_nodes.push_back(node);\n",
        "  }\n",
        "  return CreateCNodeWithGraph(input_nodes, graph);\n",
        "}\n",
        "}  // namespace\n",
        "\n",
        "bool AnfEqual(const BaseRef &a, const BaseRef &b) {\n",
        "  if (utils::isa<AnfNodePtr>(a) && utils::isa<AnfNodePtr>(b)) {\n",
        "    auto a_node = utils::cast<AnfNodePtr>(a);\n",
        "    auto b_node = utils::cast<AnfNodePtr>(b);\n",
        "    MS_EXCEPTION_IF_NULL(a_node);\n",
        "    MS_EXCEPTION_IF_NULL(b_node);\n",
        "    if (IsValueNode<Primitive>(a_node) && IsValueNode<Primitive>(b_node)) {\n",
        "      auto a_value_node = a_node->cast<ValueNodePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(a_value_node);\n",
        "      auto a_value = a_value_node->value();\n",
        "      MS_EXCEPTION_IF_NULL(a_value);\n",
        "      auto a_prim = a_value->cast<PrimitivePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(a_prim);\n",
        "\n",
        "      auto b_value_node = b_node->cast<ValueNodePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(b_value_node);\n",
        "      auto b_value = b_value_node->value();\n",
        "      MS_EXCEPTION_IF_NULL(b_value);\n",
        "      auto b_prim = b_value->cast<PrimitivePtr>();\n",
        "      MS_EXCEPTION_IF_NULL(b_prim);\n",
        "\n",
        "      return a_prim->cast<PrimitiveCPtr>()->Type() == b_prim->cast<PrimitiveCPtr>()->Type();\n",
        "    } else if (a_node->isa<ValueNode>() && b_node->isa<ValueNode>()) {\n",
        "      auto a_value_node_ptr = a_node->cast<ValueNodePtr>();\n",
        "      if (a_value_node_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"cast value node ptr fail\";\n",
        "      }\n",
        "      auto a_value_ptr = a_value_node_ptr->value();\n",
        "      if (a_value_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"value ptr is nullptr\";\n",
        "      }\n",
        "\n",
        "      auto b_value_node_ptr = b_node->cast<ValueNodePtr>();\n",
        "      if (b_value_node_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"cast value node ptr fail\";\n",
        "      }\n",
        "      auto b_value_ptr = b_value_node_ptr->value();\n",
        "      if (b_value_ptr == nullptr) {\n",
        "        MS_LOG(EXCEPTION) << \"value ptr is nullptr\";\n",
        "      }\n",
        "\n",
        "      if (utils::isa<lite::PrimitiveC>(a_value_ptr) && utils::isa<lite::PrimitiveC>(b_value_ptr)) {\n",
        "        auto a_obj = (lite::PrimitiveC *)(a_value_ptr.get());\n",
        "        auto b_obj = (lite::PrimitiveC *)(b_value_ptr.get());\n",
        "        return (*a_obj) == (*b_obj);\n",
        "      } else {\n",
        "        return (*a_value_ptr) == (*b_value_ptr);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  if (a.m_ptr->isa<lite::PrimitiveC>() && b.m_ptr->isa<lite::PrimitiveC>()) {\n",
        "    auto a_value_node_ptr = a.m_ptr->cast<PrimitiveCPtr>();\n",
        "    auto b_value_node_ptr = b.m_ptr->cast<PrimitiveCPtr>();\n",
        "    return a_value_node_ptr->Type() == b_value_node_ptr->Type();\n",
        "  }\n",
        "\n",
        "  return a == b;\n",
        "}\n",
        "\n",
        "bool CNodeTypeEqual(const BaseRef &a, const BaseRef &b) {\n",
        "  // To matchCNode and Kernel's type\n",
        "  if (utils::isa<CNode>(a) && utils::isa<CNode>(b)) {\n",
        "    return true;\n",
        "  }\n",
        "  return a.type() == b.type();\n",
        "}\n",
        "\n",
        "AnfNodePtr SexpToNode(const BaseRef &sexp, const BaseRef &graph, PrimitiveVarMap *primitive_vars, bool multigraph) {\n",
        "  MS_LOG(DEBUG) << \"SexpToNode sexp: \" + sexp.ToString() + \", graph \" + graph.ToString();\n",
        "  MS_EXCEPTION_IF_NULL(primitive_vars);\n",
        "  if (utils::isa<VectorRef>(sexp)) {\n",
        "    return HandleSexpVector(sexp, graph, primitive_vars, multigraph);\n",
        "  }\n",
        "  if (utils::isa<VarPtr>(sexp)) {\n",
        "    auto var_ptr = utils::cast<VarPtr>(sexp);\n",
        "    MS_EXCEPTION_IF_NULL(var_ptr);\n",
        "    if (var_ptr->primitive()) {\n",
        "      (*primitive_vars)[var_ptr->primitive()] = var_ptr;\n",
        "      return NewValueNode(var_ptr->primitive());\n",
        "    }\n",
        "    return CreateVarNodeWithSexp(sexp, graph);\n",
        "  }\n",
        "  if (utils::isa<AnfNodePtr>(sexp)) {\n",
        "    return utils::cast<AnfNodePtr>(sexp);\n",
        "  }\n",
        "  auto value_node = CreateValueNodeWithSexp(sexp);\n",
        "  if (value_node == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"sexp cannot converted. sexp: \" + sexp.ToString();\n",
        "  }\n",
        "  return value_node;\n",
        "}\n",
        "\n",
        "bool IsRealCNodeKernel(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  // parameter and value node is not a real cnode kernel\n",
        "  if (!node->isa<CNode>()) {\n",
        "    return false;\n",
        "  }\n",
        "  // return considered as a real node\n",
        "  if (CheckPrimitiveType(node, prim::kPrimReturn)) {\n",
        "    return true;\n",
        "  }\n",
        "  return IsRealKernel(node);\n",
        "}\n",
        "bool IsGraphKernel(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  // graph kernel should be a real cnode kernel.\n",
        "  if (!IsRealCNodeKernel(node)) {\n",
        "    return false;\n",
        "  }\n",
        "\n",
        "  auto cnode = node->cast<CNodePtr>();\n",
        "  MS_EXCEPTION_IF_NULL(cnode);\n",
        "  auto input = cnode->input(kAnfPrimitiveIndex);\n",
        "  // graph kernel should has func_graph as first input.\n",
        "  if (!IsValueNode<FuncGraph>(input)) {\n",
        "    return false;\n",
        "  }\n",
        "\n",
        "  auto func_graph = GetValueNode<FuncGraphPtr>(input);\n",
        "  MS_EXCEPTION_IF_NULL(func_graph);\n",
        "  return func_graph->has_attr(FUNC_GRAPH_ATTR_GRAPH_KERNEL);\n",
        "}\n",
        "\n",
        "void CheckIfFuncGraphIsNull(const FuncGraphPtr &graph) {\n",
        "  if (graph == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The graph is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfAnfNodeIsNull(const AnfNodePtr &node) {\n",
        "  if (node == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The AnfNode is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfCNodeIsNull(const CNodePtr &node) {\n",
        "  if (node == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The CNode is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfVarIsNull(const VarPtr &var) {\n",
        "  if (var == nullptr) {\n",
        "    MS_LOG(EXCEPTION) << \"The Var is null.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckIfNodeIsParam(const AnfNodePtr &node) {\n",
        "  if (node != nullptr && !utils::isa<ParameterPtr>(node)) {\n",
        "    MS_LOG(EXCEPTION) << \"The Node is not param.\";\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckInputSize(const CNodePtr &node, const int size) {\n",
        "  if (static_cast<int>(node->inputs().size()) != size) {\n",
        "    MS_LOG(EXCEPTION) << \"The input size of node must be \" << size << \", but it is\" << node->inputs().size();\n",
        "  }\n",
        "}\n",
        "\n",
        "void CheckLeastInputSize(const CNodePtr &node, const int size) {\n",
        "  if (static_cast<int>(node->inputs().size()) < size) {\n",
        "    MS_LOG(EXCEPTION) << \"The input size of node must be \" << size << \", but it is\" << node->inputs().size();\n",
        "  }\n",
        "}\n",
        "\n",
        "ParameterPtr AddNewBiasNode(float *bias_data, const FuncGraphPtr &func_graph, int kernel_num,\n",
        "                            const ParamValueLitePtr &weight_tensor) {\n",
        "  auto bias_parameter = func_graph->add_parameter();\n",
        "  MS_ASSERT(bias_parameter != nullptr);\n",
        "  std::vector<int> shape = {kernel_num};\n",
        "  auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(TypeIdToType(weight_tensor->tensor_type()), shape);\n",
        "  bias_parameter->set_abstract(abstract_tensor);\n",
        "\n",
        "  ParamValueLitePtr param_value = std::make_shared<ParamValueLite>();\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  param_value->set_tensor_addr(bias_data);\n",
        "  param_value->set_tensor_size(kernel_num * sizeof(float) / sizeof(uint8_t));\n",
        "  param_value->set_format(weight_tensor->format());\n",
        "  param_value->set_tensor_type(weight_tensor->tensor_type());\n",
        "  param_value->set_tensor_shape(shape);\n",
        "  bias_parameter->set_default_param(param_value);\n",
        "  return bias_parameter;\n",
        "}\n",
        "\n",
        "schema::PrimitiveType GetCNodeType(const BaseRef &n) {\n",
        "  ValueNodePtr value_node;\n",
        "  if (utils::isa<CNodePtr>(n)) {\n",
        "    auto in = utils::cast<CNodePtr>(n);\n",
        "    value_node = in->input(0)->cast<ValueNodePtr>();\n",
        "  } else if (utils::isa<ValueNodePtr>(n)) {\n",
        "    value_node = utils::cast<ValueNodePtr>(n);\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"only value node or cnode has type\";\n",
        "    return schema::PrimitiveType_NONE;\n",
        "  }\n",
        "  MS_EXCEPTION_IF_NULL(value_node);\n",
        "  auto value = value_node->value();\n",
        "  MS_ASSERT(value != nullptr);\n",
        "  if (utils::isa<PrimitiveCPtr>(value)) {\n",
        "    auto primitive = value->cast<PrimitiveCPtr>();\n",
        "    MS_ASSERT(primitive != nullptr);\n",
        "    return (schema::PrimitiveType)primitive->Type();\n",
        "  } else if (utils::isa<Primitive>(value)) {\n",
        "    auto primitive = value->cast<PrimitivePtr>();\n",
        "    MS_ASSERT(primitive != nullptr);\n",
        "    MS_LOG(INFO) << \"anf primitive node type:\" << primitive->name();\n",
        "    return schema::PrimitiveType_NONE;\n",
        "  }\n",
        "  return schema::PrimitiveType_NONE;\n",
        "}\n",
        "ParamValueLitePtr  GetLiteParamValue(const AnfNodePtr &node) {\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  if (!utils::isa<ParameterPtr>(node)) {\n",
        "    MS_LOG(ERROR) << \"cur node not parameter\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  auto param = node->cast<ParameterPtr>();\n",
        "  MS_ASSERT(param != nullptr);\n",
        "  auto param_value = std::dynamic_pointer_cast<ParamValueLite>(param->default_param());\n",
        "  MS_ASSERT(param_value != nullptr);\n",
        "  return param_value;\n",
        "}\n",
        "bool IsParamNode(const BaseRef &n) {\n",
        "  if (!utils::isa<ParameterPtr>(n)) {\n",
        "    return false;\n",
        "  }\n",
        "  auto param = utils::cast<ParameterPtr>(n)->default_param();\n",
        "  auto tensor = std::dynamic_pointer_cast<ParamValueLite>(param);\n",
        "  if (tensor == nullptr) {\n",
        "    return false;\n",
        "  }\n",
        "  return tensor->tensor_addr() != nullptr;\n",
        "}\n",
        "\n",
        "bool IsConvNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Conv2D || type == schema::PrimitiveType_DepthwiseConv2D;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "bool IsPoolingNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_Pooling;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "bool IsQuantNode(const BaseRef &n) {\n",
        "  if (utils::isa<CNodePtr>(n) || utils::isa<ValueNodePtr>(n)) {\n",
        "    auto type = opt::GetCNodeType(n);\n",
        "    return type == schema::PrimitiveType_QuantDTypeCast;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "bool CheckIsAllInputsParam(const AnfNodePtr &node) {\n",
        "  if (utils::isa<CNode>(node)) {\n",
        "    auto cnode = node->cast<CNodePtr>();\n",
        "    for (size_t i = 1; i < cnode->inputs().size(); i++) {\n",
        "      if (!utils::isa<Parameter>(cnode->input(i))) {\n",
        "        return false;\n",
        "      }\n",
        "    }\n",
        "    return true;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "size_t GetOutputTensorNum(const AnfNodePtr &node) {\n",
        "  MS_EXCEPTION_IF_NULL(node);\n",
        "  auto type = node->Type();\n",
        "  if (type == nullptr) {\n",
        "    return 1;\n",
        "  }\n",
        "  if (type->isa<Tuple>()) {\n",
        "    auto tuple_type = type->cast<TuplePtr>();\n",
        "    MS_EXCEPTION_IF_NULL(tuple_type);\n",
        "    return tuple_type->size();\n",
        "  } else if (type->isa<TensorType>() || type->isa<Number>()) {\n",
        "    return 1;\n",
        "  } else if (type->isa<TypeNone>()) {\n",
        "    return 0;\n",
        "  } else {\n",
        "    return 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "bool IsMultiOutputTensors(const FuncGraphPtr &graph, const AnfNodePtr &node) {\n",
        "  auto output_node_list = GetRealNodeUsedList(graph, node);\n",
        "  if (output_node_list->size() != 1) {\n",
        "    MS_LOG(DEBUG) << \"fusion node has multi output nodes\";\n",
        "    return true;\n",
        "  }\n",
        "  return false;\n",
        "}\n",
        "\n",
        "std::shared_ptr<std::vector<std::pair<AnfNodePtr, int>>> GetRealNodeUsedList(const FuncGraphPtr &graph,\n",
        "                                                                             const AnfNodePtr &node) {\n",
        "  auto output_node_list = std::make_shared<std::vector<std::pair<AnfNodePtr, int>>>();\n",
        "  MS_EXCEPTION_IF_NULL(graph);\n",
        "  auto manager = graph->manager();\n",
        "  MS_EXCEPTION_IF_NULL(manager);\n",
        "  auto iter = manager->node_users().find(node);\n",
        "  if (iter == manager->node_users().end()) {\n",
        "    MS_LOG(EXCEPTION) << \"node has no output in manager\";\n",
        "  }\n",
        "  auto output_info_list = iter->second;\n",
        "  std::copy(output_info_list.begin(), output_info_list.end(), std::back_inserter(*output_node_list));\n",
        "  return output_node_list;\n",
        "}\n",
        "size_t GetTupleGetItemOutIndex(const CNodePtr &tuple_get_item) {\n",
        "  MS_ASSERT(tuple_get_item != nullptr);\n",
        "  if (tuple_get_item->size() != kTupleGetItemInputSize) {\n",
        "    MS_LOG(ERROR) << \"The node tuple_get_item must have 2 inputs!\";\n",
        "    return -1;\n",
        "  }\n",
        "  auto output_index_value_node = tuple_get_item->input(kInputNodeOutputIndexInTupleGetItem);\n",
        "  MS_ASSERT(output_index_value_node != nullptr);\n",
        "  auto value_node = output_index_value_node->cast<ValueNodePtr>();\n",
        "  MS_ASSERT(value_node != nullptr);\n",
        "  return IntToSize(GetValue<int>(value_node->value()));\n",
        "}\n",
        "std::shared_ptr<std::vector<std::pair<AnfNodePtr, int>>> GetRealNodeUsedListByOutputIdx(const FuncGraphPtr &graph,\n",
        "                                                                                        const AnfNodePtr &node,\n",
        "                                                                                        size_t output_index) {\n",
        "  MS_ASSERT(graph != nullptr);\n",
        "  MS_ASSERT(node != nullptr);\n",
        "  auto output_node_list = std::make_shared<std::vector<std::pair<AnfNodePtr, int>>>();\n",
        "  auto manager = graph->manager();\n",
        "  MS_ASSERT(manager != nullptr);\n",
        "  auto iter = manager->node_users().find(node);\n",
        "  if (iter == manager->node_users().end()) {\n",
        "    MS_LOG(ERROR) << \"node has no output in manager\";\n",
        "    return output_node_list;\n",
        "  }\n",
        "  auto output_info_list = iter->second;\n",
        "  for (const auto &output_info : output_info_list) {\n",
        "    size_t used_output_index;\n",
        "    if (GetCNodeType(output_info.first) == schema::PrimitiveType_TupleGetItem) {\n",
        "      used_output_index = GetTupleGetItemOutIndex(utils::cast<CNodePtr>(output_info.first));\n",
        "    } else if (GetCNodeType(node) == schema::PrimitiveType_TupleGetItem) {\n",
        "      used_output_index = output_index;\n",
        "    } else {\n",
        "      if (output_index != 0) {\n",
        "        MS_LOG(ERROR) << \"node has no output in manager\";\n",
        "        return output_node_list;\n",
        "      }\n",
        "      return output_node_list;\n",
        "    }\n",
        "    if (used_output_index == output_index) {\n",
        "      output_node_list->push_back(output_info);\n",
        "    }\n",
        "  }\n",
        "  return output_node_list;\n",
        "}\n",
        "STATUS GetFilterDim(const std::vector<int32_t> &oriDims, kTransFilterType type, int32_t *filterK, int32_t *filterC,\n",
        "                    int32_t *filterH, int32_t *filterW) {\n",
        "  MS_ASSERT(oriDims.size() == 4);\n",
        "  if (type == kKCHW2HWCK || type == kKCHW2HWKC || type == kKCHW2KHWC || type == kKCHW2CKHW) {\n",
        "    *filterK = oriDims.at(lite::KCHW_K);\n",
        "    *filterC = oriDims.at(lite::KCHW_C);\n",
        "    *filterH = oriDims.at(lite::KCHW_H);\n",
        "    *filterW = oriDims.at(lite::KCHW_W);\n",
        "  } else if (type == kCKHW2HWCK || type == kCKHW2HWKC || type == kCKHW2KHWC) {\n",
        "    *filterC = oriDims.at(lite::CKHW_C);\n",
        "    *filterK = oriDims.at(lite::CKHW_K);\n",
        "    *filterH = oriDims.at(lite::CKHW_H);\n",
        "    *filterW = oriDims.at(lite::CKHW_W);\n",
        "  } else if (type == kHWCK2KCHW || type == kHWCK2CKHW) {\n",
        "    *filterH = oriDims.at(lite::HWCK_H);\n",
        "    *filterW = oriDims.at(lite::HWCK_W);\n",
        "    *filterC = oriDims.at(lite::HWCK_C);\n",
        "    *filterK = oriDims.at(lite::HWCK_K);\n",
        "  } else if (type == kHWKC2KCHW || type == kHWKC2CKHW) {\n",
        "    *filterH = oriDims.at(lite::HWKC_H);\n",
        "    *filterW = oriDims.at(lite::HWKC_W);\n",
        "    *filterK = oriDims.at(lite::HWKC_K);\n",
        "    *filterC = oriDims.at(lite::HWKC_C);\n",
        "  } else if (type == kNHWC2KCHW || type == kNHWC2HWCK || type == kNHWC2CKHW) {\n",
        "    *filterK = oriDims.at(lite::NHWC_N);\n",
        "    *filterH = oriDims.at(lite::NHWC_H);\n",
        "    *filterW = oriDims.at(lite::NHWC_W);\n",
        "    *filterC = oriDims.at(lite::NHWC_C);\n",
        "  } else if (type == kCHWK2HWCK || type == kCHWK2KHWC) {\n",
        "    *filterC = oriDims.at(lite::CHWK_C);\n",
        "    *filterH = oriDims.at(lite::CHWK_H);\n",
        "    *filterW = oriDims.at(lite::CHWK_W);\n",
        "    *filterK = oriDims.at(lite::CHWK_K);\n",
        "  } else if (type == kKHWC2HWCK || type == kKHWC2CHWK) {\n",
        "    *filterK = oriDims.at(lite::KHWC_K);\n",
        "    *filterH = oriDims.at(lite::KHWC_H);\n",
        "    *filterW = oriDims.at(lite::KHWC_W);\n",
        "    *filterC = oriDims.at(lite::KHWC_C);\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"Unsupported transFilterType: \" << type;\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "STATUS SetFilterDim(const ParamValueLitePtr &tensor, kTransFilterType type, int32_t filterK, int32_t filterC,\n",
        "                    int32_t filterH, int32_t filterW) {\n",
        "  MS_ASSERT(tensor != nullptr);\n",
        "  if (type == kKCHW2HWCK || type == kCKHW2HWCK || type == kNHWC2HWCK || type == kKHWC2HWCK || type == kCHWK2HWCK) {\n",
        "    tensor->set_tensor_shape({filterH, filterW, filterC, filterK});\n",
        "  } else if (type == kKCHW2HWKC || type == kCKHW2HWKC) {\n",
        "    tensor->set_tensor_shape({filterH, filterW, filterK, filterC});\n",
        "  } else if (type == kHWCK2KCHW || type == kHWKC2KCHW || type == kNHWC2KCHW) {\n",
        "    tensor->set_tensor_shape({filterK, filterC, filterH, filterW});\n",
        "  } else if (type == kHWCK2CKHW || type == kHWKC2CKHW || type == kNHWC2CKHW || type == kKCHW2CKHW) {\n",
        "    tensor->set_tensor_shape({filterC, filterK, filterH, filterW});\n",
        "  } else if (type == kKHWC2CHWK) {\n",
        "    tensor->set_tensor_shape({filterC, filterH, filterW, filterK});\n",
        "  } else if (type == kKCHW2KHWC || type == kCKHW2KHWC || type == kCHWK2KHWC) {\n",
        "    tensor->set_tensor_shape({filterK, filterH, filterW, filterC});\n",
        "  } else {\n",
        "    MS_LOG(ERROR) << \"Unsupported transFilterType: \" << type;\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "template<typename T>\n",
        "static STATUS TransFilterData(const ParamValueLitePtr &tensor, kTransFilterType type, int32_t filterK, int32_t filterC,\n",
        "                              int32_t filterH, int32_t filterW) {\n",
        "  MS_ASSERT(tensor != nullptr);\n",
        "  int count = filterH * filterW * filterC * filterK;\n",
        "  if (count <= 0) {\n",
        "    MS_LOG(ERROR) << \"Dim size invalid\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  std::unique_ptr<T[]> buf(new(std::nothrow) T[count]);\n",
        "  if (buf == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new buf failed\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "\n",
        "  void *originWeightData = tensor->tensor_addr();\n",
        "  T *weightData = static_cast<T *>(originWeightData);\n",
        "\n",
        "  if (weightData == nullptr) {\n",
        "    MS_LOG(ERROR) << \"weightData is nullptr\";\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  T *p1Buff = nullptr;\n",
        "  T *p2Buff = nullptr;\n",
        "  switch (type) {\n",
        "    case kCHWK2HWCK:\n",
        "    case kCHWK2KHWC: {\n",
        "      for (int c = 0; c < filterC; ++c) {\n",
        "        for (int h = 0; h < filterH; ++h) {\n",
        "          for (int w = 0; w < filterW; ++w) {\n",
        "            for (int k = 0; k < filterK; ++k) {\n",
        "              p1Buff = weightData + ((c * filterH * filterW * filterK) + (h * filterW * filterK) + (w * filterK) + (k));\n",
        "              if (type == kCHWK2HWCK) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (c * filterK) + (k));\n",
        "              } else if (type == kCHWK2KHWC) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((k * filterH * filterW * filterC) + (h * filterW * filterC) + (w * filterC) + (c));\n",
        "              }\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kKHWC2HWCK: {\n",
        "      for (int k = 0; k < filterK; ++k) {\n",
        "        for (int h = 0; h < filterH; ++h) {\n",
        "          for (int w = 0; w < filterW; ++w) {\n",
        "            for (int c = 0; c < filterC; ++c) {\n",
        "              p1Buff = weightData + ((k * filterH * filterW * filterC) + (h * filterW * filterC) + (w * filterC) + (c));\n",
        "              p2Buff = buf.get() + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (c * filterK) + (k));\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kKCHW2HWCK:\n",
        "    case kKCHW2CKHW:\n",
        "    case kKCHW2KHWC:\n",
        "    case kKCHW2HWKC: {\n",
        "      for (int k = 0; k < filterK; ++k) {\n",
        "        for (int c = 0; c < filterC; ++c) {\n",
        "          for (int h = 0; h < filterH; ++h) {\n",
        "            for (int w = 0; w < filterW; ++w) {\n",
        "              p1Buff = weightData + ((k * filterC * filterH * filterW) + (c * filterH * filterW) + (h * filterW) + (w));\n",
        "              if (type == kKCHW2HWCK) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (c * filterK) + (k));\n",
        "              } else if (type == kKCHW2KHWC) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((k * filterH * filterW * filterC) + (h * filterW * filterC) + (w * filterC) + (c));\n",
        "              } else if (type == kKCHW2CKHW) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((c * filterK * filterH * filterW) + (k * filterH * filterW) + (h * filterW) + (w));\n",
        "              } else {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((h * filterW * filterK * filterC) + (w * filterK * filterC) + (k * filterC) + (c));\n",
        "              }\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kCKHW2HWCK:\n",
        "    case kCKHW2KHWC:\n",
        "    case kCKHW2HWKC: {\n",
        "      for (int c = 0; c < filterC; ++c) {\n",
        "        for (int k = 0; k < filterK; ++k) {\n",
        "          for (int h = 0; h < filterH; ++h) {\n",
        "            for (int w = 0; w < filterW; ++w) {\n",
        "              p1Buff = weightData + ((c * filterK * filterH * filterW) + (k * filterH * filterW) + (h * filterW) + (w));\n",
        "              if (type == kCKHW2HWCK) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (c * filterK) + (k));\n",
        "              } else if (type == kCKHW2KHWC) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((k * filterH * filterW * filterC) + (h * filterW * filterC) + (w * filterC) + (c));\n",
        "              } else {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((h * filterW * filterK * filterC) + (w * filterK * filterC) + (k * filterC) + (c));\n",
        "              }\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kHWCK2KCHW:\n",
        "    case kHWCK2CKHW: {\n",
        "      for (int h = 0; h < filterH; ++h) {\n",
        "        for (int w = 0; w < filterW; ++w) {\n",
        "          for (int c = 0; c < filterC; ++c) {\n",
        "            for (int k = 0; k < filterK; ++k) {\n",
        "              p1Buff = weightData + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (c * filterK) + (k));\n",
        "              if (type == kHWCK2KCHW) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((k * filterC * filterH * filterW) + (c * filterH * filterW) + (h * filterW) + (w));\n",
        "              } else {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((c * filterK * filterH * filterW) + (k * filterH * filterW) + (h * filterW) + (w));\n",
        "              }\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kHWKC2KCHW:\n",
        "    case kHWKC2CKHW: {\n",
        "      for (int h = 0; h < filterH; ++h) {\n",
        "        for (int w = 0; w < filterW; ++w) {\n",
        "          for (int c = 0; c < filterC; ++c) {\n",
        "            for (int k = 0; k < filterK; ++k) {\n",
        "              p1Buff = weightData + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (k * filterC) + (c));\n",
        "              if (type == kHWKC2KCHW) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((k * filterC * filterH * filterW) + (c * filterH * filterW) + (h * filterW) + (w));\n",
        "              } else {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((c * filterK * filterH * filterW) + (k * filterH * filterW) + (h * filterW) + (w));\n",
        "              }\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kNHWC2HWCK:\n",
        "    case kNHWC2KCHW:\n",
        "    case kNHWC2CKHW: {\n",
        "      for (int k = 0; k < filterK; ++k) {\n",
        "        for (int h = 0; h < filterH; ++h) {\n",
        "          for (int w = 0; w < filterW; ++w) {\n",
        "            for (int c = 0; c < filterC; ++c) {\n",
        "              p1Buff = weightData + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (k * filterC) + (c));\n",
        "              if (type == kNHWC2HWCK) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((h * filterW * filterC * filterK) + (w * filterC * filterK) + (c * filterK) + (k));\n",
        "              } else if (type == kNHWC2CKHW) {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((c * filterK * filterH * filterW) + (k * filterH * filterW) + (h * filterW) + (w));\n",
        "              } else {\n",
        "                p2Buff =\n",
        "                    buf.get() + ((k * filterC * filterH * filterW) + (c * filterH * filterW) + (h * filterW) + (w));\n",
        "              }\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case kKHWC2CHWK: {\n",
        "      for (int k = 0; k < filterK; ++k) {\n",
        "        for (int h = 0; h < filterH; ++h) {\n",
        "          for (int w = 0; w < filterW; ++w) {\n",
        "            for (int c = 0; c < filterC; ++c) {\n",
        "              p1Buff = weightData + ((k * filterH * filterW * filterC) + (h * filterW * filterC) + (w * filterC) + (c));\n",
        "              p2Buff = buf.get() + ((c * filterK * filterH * filterW) + (h * filterK * filterW) + (w * filterK) + (k));\n",
        "              *p2Buff = *p1Buff;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    default: {\n",
        "      MS_LOG(ERROR) << \"Unsupported transFilterType: \" << type;\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  auto ret = ::memcpy_s(tensor->tensor_addr(), count * sizeof(T), buf.get(), count * sizeof(T));\n",
        "  if (ret != EOK) {\n",
        "    MS_LOG(ERROR) << \"memcpy_s failed: \" << ret;\n",
        "    return RET_ERROR;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "template<typename T>\n",
        "static STATUS TransFilterFormat(const ParamValueLitePtr &tensor, kTransFilterType type) {\n",
        "  MS_ASSERT(tensor != nullptr);\n",
        "  auto oriDims = tensor->tensor_shape();\n",
        "  if (oriDims.size() != (size_t)lite::DIM_DEFAULT_SIZE) {\n",
        "    MS_LOG(ERROR) << \"Filter dim-num is not supported, dim-num: \" << oriDims.size();\n",
        "    return lite::RET_ERROR;\n",
        "  }\n",
        "\n",
        "  int32_t filterH;\n",
        "  int32_t filterW;\n",
        "  int32_t filterC;\n",
        "  int32_t filterK;\n",
        "  auto status = GetFilterDim(oriDims, type, &filterK, &filterC, &filterH, &filterW);\n",
        "  if (status != lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"GetFilterDim failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  status = SetFilterDim(tensor, type, filterK, filterC, filterH, filterW);\n",
        "  if (status != lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"SetFilterDim failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  status = TransFilterData<T>(tensor, type, filterK, filterC, filterH, filterW);\n",
        "  if (status != lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"TransFilterData failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "\n",
        "  return lite::RET_OK;\n",
        "}\n",
        "\n",
        "STATUS TransFilterFormat(const ParamValueLitePtr &tensor, schema::Format dst_format) {\n",
        "  if (tensor == nullptr) {\n",
        "    return lite::RET_NULL_PTR;\n",
        "  }\n",
        "  auto ori_dims = tensor->tensor_shape();\n",
        "  if (ori_dims.size() != (size_t)lite::DIM_DEFAULT_SIZE) {\n",
        "    MS_LOG(ERROR) << \"Filter dim-num is not supported, dim-num: \" << ori_dims.size();\n",
        "    return lite::RET_ERROR;\n",
        "  }\n",
        "  auto src_format = tensor->format();\n",
        "  auto data_type = tensor->tensor_type();\n",
        "  lite::STATUS status;\n",
        "  switch (dst_format) {\n",
        "    case schema::Format::Format_KHWC: {\n",
        "      switch (src_format) {\n",
        "        case schema::Format::Format_KCHW:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kKCHW2KHWC);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kKCHW2KHWC);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kKCHW2KHWC);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CKHW:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kCKHW2KHWC);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kCKHW2KHWC);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kCKHW2KHWC);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CHWK:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kCHWK2KHWC);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kCHWK2KHWC);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kCHWK2KHWC);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_KHWC:return RET_OK;\n",
        "        default:MS_LOG(ERROR) << \"Unsupported transform from \" << src_format << \" to \"\n",
        "                              << EnumNameFormat(dst_format);\n",
        "          return RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case schema::Format::Format_HWCK: {\n",
        "      switch (src_format) {\n",
        "        case schema::Format::Format_KCHW:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kKCHW2HWCK);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kKCHW2HWCK);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kKCHW2HWCK);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_KHWC:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kKHWC2HWCK);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kKHWC2HWCK);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kKHWC2HWCK);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CKHW:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kCKHW2HWCK);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kCKHW2HWCK);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kCKHW2HWCK);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CHWK:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kCHWK2HWCK);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kCHWK2HWCK);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kCHWK2HWCK);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return lite::RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_HWCK:return RET_OK;\n",
        "        default:MS_LOG(ERROR) << \"Unsupported transform from \" << src_format << \" to \"\n",
        "                              << EnumNameFormat(dst_format);\n",
        "          return RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case schema::Format::Format_KCHW: {\n",
        "      switch (src_format) {\n",
        "        case schema::Format::Format_KCHW:return RET_OK;\n",
        "        case schema::Format::Format_HWCK:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kHWCK2KCHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kHWCK2KCHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kHWCK2KCHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_HWKC:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kHWKC2KCHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kHWKC2KCHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kHWKC2KCHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_KHWC:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kKHWC2KCHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kKHWC2KCHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kKHWC2KCHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CKHW:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kCKHW2KCHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kCKHW2KCHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kCKHW2KCHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CHWK:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kCHWK2KCHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kCHWK2KCHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kCHWK2KCHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        default:MS_LOG(ERROR) << \"Unsupported transform from \" << src_format << \" to \"\n",
        "                              << EnumNameFormat(dst_format);\n",
        "          return RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    case schema::Format::Format_CKHW: {\n",
        "      switch (src_format) {\n",
        "        case schema::Format::Format_HWCK:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kHWCK2CKHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kHWCK2CKHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kHWCK2CKHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_HWKC:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kHWKC2CKHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kHWKC2CKHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kHWKC2CKHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_KCHW:\n",
        "          if (data_type == kNumberTypeFloat32) {\n",
        "            status = TransFilterFormat<float>(tensor, kKCHW2CKHW);\n",
        "          } else if (data_type == kNumberTypeUInt8) {\n",
        "            status = TransFilterFormat<uint8_t>(tensor, kKCHW2CKHW);\n",
        "          } else if (data_type == kNumberTypeInt8) {\n",
        "            status = TransFilterFormat<int8_t>(tensor, kKCHW2CKHW);\n",
        "          } else {\n",
        "            MS_LOG(ERROR) << \"Unsupported data_type: \" << data_type;\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          break;\n",
        "        case schema::Format::Format_CKHW:return RET_OK;\n",
        "        default:MS_LOG(ERROR) << \"Unsupported transform from \" << src_format << \" to \"\n",
        "                              << EnumNameFormat(dst_format);\n",
        "          return RET_ERROR;\n",
        "      }\n",
        "    }\n",
        "      break;\n",
        "    default:MS_LOG(ERROR) << \"Unsupported transform from \" << src_format << \" to \"\n",
        "                          << EnumNameFormat(dst_format);\n",
        "      return RET_ERROR;\n",
        "  }\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"TransFilterData failed: \" << status;\n",
        "    return status;\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "}  // namespace opt\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPaNwA44-rwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MINDSPORE_LITE_SRC_PASS_COMMON_GLLO_UTILS_H_\n",
        "#define MINDSPORE_LITE_SRC_PASS_COMMON_GLLO_UTILS_H_\n",
        "\n",
        "#include <memory>\n",
        "#include \"src/ops//primitive_c.h\"\n",
        "#include \"ir/anf.h\"\n",
        "#include \"ir/func_graph.h\"\n",
        "#include \"src/common/utils.h\"\n",
        "#include \"backend/optimizer/common/pattern_engine.h\"\n",
        "#include \"schema/inner/model_generated.h\"\n",
        "#include \"src/param_value_lite.h\"\n",
        "\n",
        "using PrimitiveCPtr = std::shared_ptr<mindspore::lite::PrimitiveC>;\n",
        "using mindspore::lite::RET_ERROR;\n",
        "using mindspore::lite::RET_OK;\n",
        "using mindspore::lite::STATUS;\n",
        "namespace mindspore {\n",
        "namespace opt {\n",
        "bool IsRealCNodeKernel(const AnfNodePtr &node);\n",
        "\n",
        "bool IsGraphKernel(const AnfNodePtr &node);\n",
        "\n",
        "void CheckIfFuncGraphIsNull(const FuncGraphPtr &graph);\n",
        "\n",
        "void CheckIfAnfNodeIsNull(const AnfNodePtr &node);\n",
        "\n",
        "void CheckIfCNodeIsNull(const CNodePtr &node);\n",
        "\n",
        "void CheckIfVarIsNull(const VarPtr &var);\n",
        "\n",
        "void CheckInputSize(const CNodePtr &node, int size);\n",
        "\n",
        "void CheckIfNodeIsParam(const AnfNodePtr &node);\n",
        "\n",
        "void CheckLeastInputSize(const CNodePtr &node, int size);\n",
        "\n",
        "ParameterPtr AddNewBiasNode(float *bias_data, const FuncGraphPtr &func_graph, int kernel_num,\n",
        "                            const ParamValueLitePtr &weight_tensor);\n",
        "\n",
        "schema::PrimitiveType GetCNodeType(const BaseRef &node);\n",
        "\n",
        "bool IsParamNode(const BaseRef &n);\n",
        "\n",
        "bool IsConvNode(const BaseRef &n);\n",
        "\n",
        "bool IsPoolingNode(const BaseRef &n);\n",
        "\n",
        "bool IsQuantNode(const BaseRef &n);\n",
        "\n",
        "bool CheckIsAllInputsParam(const AnfNodePtr &node);\n",
        "\n",
        "size_t GetOutputTensorNum(const AnfNodePtr &node);\n",
        "\n",
        "bool IsMultiOutputTensors(const FuncGraphPtr &graph, const AnfNodePtr &node);\n",
        "\n",
        "size_t GetTupleGetItemOutIndex(const CNodePtr &tuple_get_item);\n",
        "\n",
        "ParamValueLitePtr  GetLiteParamValue(const AnfNodePtr &node);\n",
        "\n",
        "enum kTransFilterType {\n",
        "  kKCHW2HWCK,  // 0\n",
        "  kKCHW2KHWC,\n",
        "  kCKHW2KHWC,\n",
        "  kCKHW2HWCK,\n",
        "  kKCHW2HWKC,\n",
        "  kCKHW2HWKC,\n",
        "  kHWCK2KCHW,\n",
        "  kHWCK2CKHW,\n",
        "  kHWKC2KCHW,\n",
        "  kHWKC2CKHW,\n",
        "  kNHWC2KCHW,  // 10\n",
        "  kNHWC2CKHW,\n",
        "  kNHWC2HWCK,\n",
        "  kKHWC2HWCK,\n",
        "  kCHWK2HWCK,\n",
        "  kKHWC2CHWK,\n",
        "  kCHWK2KHWC,\n",
        "  kKHWC2KCHW,\n",
        "  kCKHW2KCHW,\n",
        "  kCHWK2KCHW,\n",
        "  kKCHW2CKHW  // 20\n",
        "};\n",
        "\n",
        "STATUS GetFilterDim(const std::vector<int32_t> &oriDims, kTransFilterType type, int32_t *filterK, int32_t *filterC,\n",
        "                    int32_t *filterH, int32_t *filterW);\n",
        "\n",
        "STATUS SetFilterDim(const ParamValueLitePtr &tensor, kTransFilterType type, int32_t filterK, int32_t filterC,\n",
        "                    int32_t filterH, int32_t filterW);\n",
        "\n",
        "template<typename T>\n",
        "static STATUS TransFilterData(const ParamValueLitePtr &tensor, kTransFilterType type, int32_t filterK, int32_t filterC,\n",
        "                              int32_t filterH, int32_t filterW);\n",
        "\n",
        "template<typename T>\n",
        "static lite::STATUS TransFilterFormat(const ParamValueLitePtr &tensor, kTransFilterType type);\n",
        "\n",
        "STATUS TransFilterFormat(const ParamValueLitePtr &tensor, schema::Format dst_format);\n",
        "}  // namespace opt\n",
        "}  // namespace mindspore\n",
        "#endif  // MINDSPORE_LITE_SRC_PASS_COMMON_GLLO_UTILS_H_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk",
        "colab_type": "text"
      },
      "source": [
        "#Data process"
      ]
    }
  ]
}