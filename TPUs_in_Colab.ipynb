{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S93a3Yz_tHf4"
      },
      "source": [
        "From 6c08a8d4eb95796a2c862abeedda2a3bb835c06a Mon Sep 17 00:00:00 2001\r\n",
        "From: guohongzilong <guohongzilong@huawei.com>\r\n",
        "Date: Thu, 4 Mar 2021 19:42:36 +0800\r\n",
        "Subject: [PATCH] fl lite lenet demo\r\n",
        "\r\n",
        "---\r\n",
        " .../java/com/huawei/flclient/FLLiteClient.java     | 572 +++++++++++++++++++++\r\n",
        " .../main/java/com/huawei/flclient/GetModel.java    |  12 +\r\n",
        " .../main/java/com/huawei/flclient/LiteTrain.java   |  83 +++\r\n",
        " .../main/java/com/huawei/flclient/StartFLJob.java  |  43 ++\r\n",
        " .../main/java/com/huawei/flclient/UpdateModel.java |  15 +-\r\n",
        " .../lite/flclient/src/main/native/CMakeLists.txt   |  39 +-\r\n",
        " .../flclient/src/main/native/include/lenet_train.h |  35 ++\r\n",
        " .../flclient/src/main/native/lenet_train_jni.cpp   | 158 ++++++\r\n",
        " .../flclient/src/main/native/src/lenet_train.cpp   | 270 ++++++++++\r\n",
        " .../com/huawei/flclient/test/TestFLClient.java     |   8 +-\r\n",
        " mindspore/lite/include/train_session.h             |  13 +-\r\n",
        " mindspore/lite/src/train/train_session.cc          |  59 ++-\r\n",
        " mindspore/lite/src/train/train_session.h           |   3 +\r\n",
        " 13 files changed, 1296 insertions(+), 14 deletions(-)\r\n",
        " create mode 100644 mindspore/lite/flclient/src/main/java/com/huawei/flclient/FLLiteClient.java\r\n",
        " create mode 100644 mindspore/lite/flclient/src/main/java/com/huawei/flclient/LiteTrain.java\r\n",
        " create mode 100644 mindspore/lite/flclient/src/main/native/include/lenet_train.h\r\n",
        " create mode 100644 mindspore/lite/flclient/src/main/native/lenet_train_jni.cpp\r\n",
        " create mode 100644 mindspore/lite/flclient/src/main/native/src/lenet_train.cpp\r\n",
        "\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/FLLiteClient.java b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/FLLiteClient.java\r\n",
        "new file mode 100644\r\n",
        "index 0000000..f29bc9c\r\n",
        "--- /dev/null\r\n",
        "+++ b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/FLLiteClient.java\r\n",
        "@@ -0,0 +1,572 @@\r\n",
        "+/**\r\n",
        "+ * Copyright 2020 Huawei Technologies Co., Ltd\r\n",
        "+ * <p>\r\n",
        "+ * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "+ * you may not use this file except in compliance with the License.\r\n",
        "+ * You may obtain a copy of the License at\r\n",
        "+ * <p>\r\n",
        "+ * http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "+ * <p>\r\n",
        "+ * Unless required by applicable law or agreed to in writing, software\r\n",
        "+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "+ * See the License for the specific language governing permissions and\r\n",
        "+ * limitations under the License.\r\n",
        "+ */\r\n",
        "+\r\n",
        "+package com.huawei.flclient;\r\n",
        "+\r\n",
        "+import com.huawei.flclient.cipher.BaseUtil;\r\n",
        "+import mindspore.schema.*;\r\n",
        "+\r\n",
        "+import java.io.IOException;\r\n",
        "+import java.nio.ByteBuffer;\r\n",
        "+import java.util.Date;\r\n",
        "+import java.util.concurrent.TimeoutException;\r\n",
        "+\r\n",
        "+public class FLLiteClient {\r\n",
        "+    private FLCommunication flCommunication;\r\n",
        "+\r\n",
        "+    private FLClientStatus status;\r\n",
        "+\r\n",
        "+    private static int iteration = 0;\r\n",
        "+\r\n",
        "+    private int iterations;\r\n",
        "+    private int epochs;\r\n",
        "+    private int batchSize;\r\n",
        "+    private int minSecretNum;\r\n",
        "+    private byte[] prime;\r\n",
        "+    private int featureSize;\r\n",
        "+\r\n",
        "+    private String fl_name;\r\n",
        "+    private String fl_id;\r\n",
        "+    private String ip;\r\n",
        "+    private int port;\r\n",
        "+    private boolean ifAsync;\r\n",
        "+    private EncryptLevel encryptLevel;\r\n",
        "+    private boolean use_elb;\r\n",
        "+    private int serverNum;\r\n",
        "+    private int train_batch_num;\r\n",
        "+    private String test_dataset;\r\n",
        "+    private int test_batch_num;\r\n",
        "+\r\n",
        "+    private SecureProtocol secureProtocol = new SecureProtocol();\r\n",
        "+\r\n",
        "+\r\n",
        "+    public FLLiteClient(String ip, int port, String fl_name, String fl_id, boolean ifAsync, EncryptLevel encryptLevel, boolean use_elb, int serverNum, int train_batch_num, String test_dataset, int test_batch_num) {\r\n",
        "+        flCommunication = FLCommunication.getInstance();\r\n",
        "+        try {\r\n",
        "+            flCommunication.setTimeOut(100);\r\n",
        "+        } catch (TimeoutException e) {\r\n",
        "+            e.printStackTrace();\r\n",
        "+        }\r\n",
        "+        this.ip = ip;\r\n",
        "+        this.port = port;\r\n",
        "+        this.fl_name = fl_name;\r\n",
        "+        this.fl_id = fl_id;\r\n",
        "+        this.ifAsync = ifAsync;\r\n",
        "+        this.encryptLevel = encryptLevel;\r\n",
        "+        this.use_elb = use_elb;\r\n",
        "+        this.serverNum = serverNum;\r\n",
        "+        this.train_batch_num = train_batch_num;\r\n",
        "+        this.test_dataset = test_dataset;\r\n",
        "+        this.test_batch_num = test_batch_num;\r\n",
        "+\r\n",
        "+    }\r\n",
        "+\r\n",
        "+\r\n",
        "+    public void setGlobalParameters(ResponseFLJob flJob) {\r\n",
        "+        FLPlan flPlan = flJob.flPlanConfig();\r\n",
        "+        if (flPlan != null) {\r\n",
        "+            iterations = flPlan.iterations();\r\n",
        "+            epochs = flPlan.epochs();\r\n",
        "+            batchSize = flPlan.miniBatch();\r\n",
        "+//            minSecretNum = 3;\r\n",
        "+//            String prime_s = \"238586b57e1179feb154e90ace3e1886a36714ca789702ba31fda502b2c4eab4c7\";\r\n",
        "+//            byte[] prime_b = BaseUtil.hexString2ByteArray(prime_s);\r\n",
        "+//            prime = prime_b;\r\n",
        "+            CipherPublicParams cipherPublicParams = flPlan.cipher();\r\n",
        "+            minSecretNum = cipherPublicParams.t();\r\n",
        "+            int primeLength = cipherPublicParams.primeLength();\r\n",
        "+            prime = new byte[primeLength];\r\n",
        "+            for (int i = 0; i < primeLength; i++) {\r\n",
        "+                prime[i] = (byte) cipherPublicParams.prime(i);\r\n",
        "+            }\r\n",
        "+            System.out.println(\"[Encrypt] the minSecretNum from server: \" + minSecretNum);\r\n",
        "+            System.out.println(\"[Encrypt] the prime from server: \" + BaseUtil.byte2HexString(prime));\r\n",
        "+        } else{  //todo for test\r\n",
        "+            iterations = 5;\r\n",
        "+            epochs = 20;\r\n",
        "+            batchSize = 32;\r\n",
        "+            minSecretNum = 3;\r\n",
        "+            String prime_s = \"238586b57e1179feb154e90ace3e1886a36714ca789702ba31fda502b2c4eab4c7\";\r\n",
        "+            byte[] prime_b = BaseUtil.hexString2ByteArray(prime_s);\r\n",
        "+            prime = prime_b;\r\n",
        "+            featureSize = 66404;\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public int getIteration() {\r\n",
        "+        return iteration;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public int getIterations() {\r\n",
        "+        return iterations;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public int getEpochs() {\r\n",
        "+        return epochs;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public int getBatchSize() {\r\n",
        "+        return batchSize;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus checkStatus() {\r\n",
        "+        return this.status;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public void syncFLJob(int epoch, int batch_num) {\r\n",
        "+        String url = Common.generateUrl(use_elb, ip, port, serverNum);\r\n",
        "+        // 1. verify server\r\n",
        "+        {\r\n",
        "+            System.out.println(\"[startFLJob] ========Verify server========\");\r\n",
        "+            StartFLJob startFLJob = StartFLJob.getInstance();\r\n",
        "+            byte[] msg = startFLJob.getRequestStartFLJob(fl_name, fl_id, iteration);\r\n",
        "+            byte[] message = new byte[0];\r\n",
        "+            try {\r\n",
        "+                message = flCommunication.syncRequest(url + \"/startFLJob\", msg);\r\n",
        "+            } catch (IOException e) {\r\n",
        "+                e.printStackTrace();\r\n",
        "+            }\r\n",
        "+            ByteBuffer buffer = ByteBuffer.wrap(message);\r\n",
        "+            ResponseFLJob flJob = ResponseFLJob.getRootAsResponseFLJob(buffer);\r\n",
        "+            iteration = flJob.iteration();\r\n",
        "+            FLClientStatus status = startFLJob.doResponse(fl_name,flJob);\r\n",
        "+            if (status != FLClientStatus.SUCCESS) {\r\n",
        "+                throw new RuntimeException();\r\n",
        "+            }\r\n",
        "+        }\r\n",
        "+\r\n",
        "+        // 2. update model, and push features map to server\r\n",
        "+        {\r\n",
        "+            System.out.println(\"[Train] ===========Training===========\");\r\n",
        "+            LiteTrain train = LiteTrain.getInstance();\r\n",
        "+            train.train(fl_name,batch_num,epoch);\r\n",
        "+            UpdateModel updateModel = UpdateModel.getInstance();\r\n",
        "+            byte[] updateModelBuffer = updateModel.getRequestUpdateFLJob(encryptLevel, fl_name, fl_id, iteration, secureProtocol);\r\n",
        "+            byte[] message = new byte[0];\r\n",
        "+            try {\r\n",
        "+                message = flCommunication.syncRequest(url + \"/updateModel\", updateModelBuffer);\r\n",
        "+            } catch (IOException e) {\r\n",
        "+                e.printStackTrace();\r\n",
        "+            }\r\n",
        "+            ByteBuffer debugBuffer = ByteBuffer.wrap(message);\r\n",
        "+            ResponseUpdateModel response = ResponseUpdateModel.getRootAsResponseUpdateModel(debugBuffer);\r\n",
        "+            FLClientStatus status = updateModel.doResponse(response);\r\n",
        "+            if (status != FLClientStatus.SUCCESS) {\r\n",
        "+                throw new RuntimeException();\r\n",
        "+            }\r\n",
        "+        }\r\n",
        "+\r\n",
        "+        // 3. update featuresMap\r\n",
        "+        {\r\n",
        "+            System.out.println(\"[getModel] ===========getModel=============\");\r\n",
        "+            GetModel getModel = GetModel.getInstance();\r\n",
        "+            byte[] buffer = getModel.getRequestGetModel(fl_name, iterations);\r\n",
        "+            byte[] message = new byte[0];\r\n",
        "+            try {\r\n",
        "+                message = flCommunication.syncRequest(url + \"/getModel\", buffer);\r\n",
        "+            } catch (IOException e) {\r\n",
        "+                e.printStackTrace();\r\n",
        "+            }\r\n",
        "+            ByteBuffer debugBuffer = ByteBuffer.wrap(message);\r\n",
        "+            ResponseGetModel rgm = ResponseGetModel.getRootAsResponseGetModel(debugBuffer);\r\n",
        "+            FLClientStatus status = getModel.doResponse(fl_name,rgm);\r\n",
        "+            if (status != FLClientStatus.SUCCESS) {\r\n",
        "+                throw new RuntimeException();\r\n",
        "+            }\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus startFLJob() {\r\n",
        "+        System.out.println(\"[startFLJob] ====================================Verify server====================================\");\r\n",
        "+        String url = Common.generateUrl(use_elb, ip, port, serverNum);\r\n",
        "+        StartFLJob startFLJob = StartFLJob.getInstance();\r\n",
        "+        byte[] msg = startFLJob.getRequestStartFLJob(fl_name, fl_id, iteration);\r\n",
        "+        if (ifAsync) {\r\n",
        "+            try {\r\n",
        "+                flCommunication.asyncRequest(url + \"/startFLJob\", msg, new IAsyncCallBack() {\r\n",
        "+\r\n",
        "+                    @Override\r\n",
        "+                    public FLClientStatus onResponse(byte[] msg) {\r\n",
        "+                        ByteBuffer buffer = ByteBuffer.wrap(msg);\r\n",
        "+                        ResponseFLJob flJob = ResponseFLJob.getRootAsResponseFLJob(buffer);\r\n",
        "+                        FLClientStatus status = startFLJob.doResponse(flJob);\r\n",
        "+                        FLLiteClient.this.status = status;\r\n",
        "+                        status = updateModel();\r\n",
        "+                        if (status != FLClientStatus.SUCCESS) {\r\n",
        "+                            throw new RuntimeException();\r\n",
        "+                        }\r\n",
        "+                        return FLClientStatus.SUCCESS;\r\n",
        "+                    }\r\n",
        "+\r\n",
        "+                    @Override\r\n",
        "+                    public FLClientStatus onFailure(IOException exception) {\r\n",
        "+                        exception.printStackTrace();\r\n",
        "+                        return FLClientStatus.FAILED;\r\n",
        "+                    }\r\n",
        "+                });\r\n",
        "+            } catch (Exception e) {\r\n",
        "+                e.printStackTrace();\r\n",
        "+            }\r\n",
        "+        } else {\r\n",
        "+            try {\r\n",
        "+                byte[] message = flCommunication.syncRequest(url + \"/startFLJob\", msg);\r\n",
        "+                ByteBuffer buffer = ByteBuffer.wrap(message);\r\n",
        "+                ResponseFLJob responseDataBuf = ResponseFLJob.getRootAsResponseFLJob(buffer);\r\n",
        "+                status = judgeStartFLJob(startFLJob, responseDataBuf);\r\n",
        "+                return status;\r\n",
        "+\r\n",
        "+            } catch (IOException e) {\r\n",
        "+                System.out.println(\"[startFLJob] un sloved error code in StartFLJob\");\r\n",
        "+                e.printStackTrace();\r\n",
        "+                status = FLClientStatus.FAILED;\r\n",
        "+                return status;\r\n",
        "+            }\r\n",
        "+        }\r\n",
        "+        return FLClientStatus.SUCCESS;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus judgeStartFLJob(StartFLJob startFLJob, ResponseFLJob responseDataBuf) {\r\n",
        "+        iteration = responseDataBuf.iteration();\r\n",
        "+        FLClientStatus response = startFLJob.doResponse(fl_name,responseDataBuf);\r\n",
        "+        FLClientStatus curStatus;\r\n",
        "+        if (response == FLClientStatus.SUCCESS) {\r\n",
        "+            featureSize = startFLJob.getFeatureSize();    // todo get feature size\r\n",
        "+            System.out.println(\"[startFLJob] ***the feature size get in ResponseFLJob***: \"+featureSize);\r\n",
        "+            setGlobalParameters(responseDataBuf);\r\n",
        "+            return FLClientStatus.SUCCESS;\r\n",
        "+        } else if (response == FLClientStatus.WAIT) {\r\n",
        "+//            long waitTime = getWaitTime(responseDataBuf.nextReqTime());\r\n",
        "+//            sleep(waitTime);\r\n",
        "+//            curStatus = startFLJob();\r\n",
        "+//            return curStatus;\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        } else {\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        }\r\n",
        "+\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus startUpdateModel(FLClientStatus response) {\r\n",
        "+        FLClientStatus curStatus;\r\n",
        "+        if (response == FLClientStatus.SUCCESS) {\r\n",
        "+            System.out.println(\"[startFLJob] startFLJob succeed\");\r\n",
        "+            System.out.println(\"[train] ========================================================global train epoch \"+iteration+\"========================================================\");\r\n",
        "+\r\n",
        "+            // todo zs : need add: Synchronously start the <encryption part> and <train process>.\r\n",
        "+            if (encryptLevel == EncryptLevel.PWEncrypt) {\r\n",
        "+                curStatus = getFeatureMask();\r\n",
        "+                if (curStatus != FLClientStatus.SUCCESS) {\r\n",
        "+                    return FLClientStatus.FAILED;\r\n",
        "+                }\r\n",
        "+                System.out.println(\"[Encrypt] create mask for <\" + encryptLevel.toString() + \">\" + \" ok!\");\r\n",
        "+            } else if (encryptLevel == EncryptLevel.DPEncrypt) {\r\n",
        "+                // TODO jxl set parameters\r\n",
        "+                System.out.println(\"[Encrypt] set parameters for DPEncrypt!\");\r\n",
        "+            } else if (encryptLevel == EncryptLevel.NotEncrypt){\r\n",
        "+                System.out.println(\"[Encrypt] don't mask model\");\r\n",
        "+            } else {\r\n",
        "+                System.out.println(\"[Encrypt] The encrypt level is error!\");\r\n",
        "+            }\r\n",
        "+\r\n",
        "+            curStatus = updateModel();\r\n",
        "+            return curStatus;\r\n",
        "+        } else {\r\n",
        "+            System.out.println(\"[startFLJob] startFLJob failed\");\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+\r\n",
        "+    public FLClientStatus updateModel() {\r\n",
        "+        String url = Common.generateUrl(use_elb, ip, port, serverNum);\r\n",
        "+        System.out.println(\"[updateModel] ==============updateModel url: \"+url+\"==============\");\r\n",
        "+        LiteTrain train = LiteTrain.getInstance();\r\n",
        "+        train.train(fl_name,train_batch_num, epochs * train_batch_num);\r\n",
        "+        UpdateModel updateModelBuf = UpdateModel.getInstance();\r\n",
        "+        byte[] updateModelBuffer = updateModelBuf.getRequestUpdateFLJob(encryptLevel, fl_name, fl_id, iteration, secureProtocol);\r\n",
        "+        if (ifAsync) {\r\n",
        "+            try {\r\n",
        "+                flCommunication.asyncRequest(url + \"/updateModel\", updateModelBuffer, new IAsyncCallBack() {\r\n",
        "+                    @Override\r\n",
        "+                    public FLClientStatus onResponse(byte[] msg) {\r\n",
        "+                        ByteBuffer debugBuffer = ByteBuffer.wrap(msg);\r\n",
        "+                        ResponseUpdateModel updateModel = ResponseUpdateModel.getRootAsResponseUpdateModel(debugBuffer);\r\n",
        "+                        //Debug code\r\n",
        "+                        System.out.println(\"[updateModel] ==========update model content is:================\");\r\n",
        "+                        System.out.println(\"[updateModel] ==========fl name: \" + updateModel.retcode());\r\n",
        "+                        System.out.println(\"[updateModel] ==========reason: \" + updateModel.reason());\r\n",
        "+                        System.out.println(\"[updateModel] ==========time: \" + updateModel.timestemp());\r\n",
        "+                        FLClientStatus status = getModel();\r\n",
        "+                        if (status != FLClientStatus.SUCCESS) {\r\n",
        "+                            throw new RuntimeException();\r\n",
        "+                        }\r\n",
        "+                        return FLClientStatus.SUCCESS;\r\n",
        "+                    }\r\n",
        "+\r\n",
        "+                    @Override\r\n",
        "+                    public FLClientStatus onFailure(IOException exception) {\r\n",
        "+                        exception.printStackTrace();\r\n",
        "+                        return FLClientStatus.FAILED;\r\n",
        "+                    }\r\n",
        "+                });\r\n",
        "+            } catch (Exception e) {\r\n",
        "+                e.printStackTrace();\r\n",
        "+            }\r\n",
        "+        } else {\r\n",
        "+            try {\r\n",
        "+                byte[] message = flCommunication.syncRequest(url + \"/updateModel\", updateModelBuffer);\r\n",
        "+                ByteBuffer debugBuffer = ByteBuffer.wrap(message);\r\n",
        "+                ResponseUpdateModel responseDataBuf = ResponseUpdateModel.getRootAsResponseUpdateModel(debugBuffer);\r\n",
        "+                status = judgeUpdateModel(updateModelBuf, responseDataBuf);\r\n",
        "+                return status;\r\n",
        "+\r\n",
        "+            } catch (IOException e) {\r\n",
        "+                System.out.println(\"[updateModel] un sloved error code in updateModel\");\r\n",
        "+                e.printStackTrace();\r\n",
        "+                status = FLClientStatus.FAILED;\r\n",
        "+                return status;\r\n",
        "+            }\r\n",
        "+        }\r\n",
        "+        return FLClientStatus.SUCCESS;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus judgeUpdateModel(UpdateModel updateModelBuf, ResponseUpdateModel responseDataBuf) {\r\n",
        "+        FLClientStatus response = updateModelBuf.doResponse(responseDataBuf);\r\n",
        "+        FLClientStatus curStatus;\r\n",
        "+        System.out.println(\"[updateModel] response updateModel ok!\");\r\n",
        "+        if (response == FLClientStatus.SUCCESS) {\r\n",
        "+\r\n",
        "+            if (encryptLevel == EncryptLevel.PWEncrypt) {\r\n",
        "+                curStatus = unMasking();\r\n",
        "+                if (curStatus != FLClientStatus.SUCCESS) {\r\n",
        "+                    return FLClientStatus.FAILED;\r\n",
        "+                }\r\n",
        "+                System.out.println(\"[Encrypt] pairwise unmasking ok!\");\r\n",
        "+            } else if (encryptLevel == EncryptLevel.DPEncrypt) {\r\n",
        "+                System.out.println(\"[Encrypt] DPEncrypt don't need unmasking!\");\r\n",
        "+            } else if (encryptLevel == EncryptLevel.NotEncrypt){\r\n",
        "+                System.out.println(\"[Encrypt] don't mask model\");\r\n",
        "+            } else {\r\n",
        "+                System.out.println(\"[Encrypt] The encrypt level is error!\");\r\n",
        "+            }\r\n",
        "+\r\n",
        "+            curStatus = getModel();\r\n",
        "+            return curStatus;\r\n",
        "+        } else if (response == FLClientStatus.WAIT) {\r\n",
        "+//            long waitTime = getWaitTime(responseDataBuf.nextReqTime());\r\n",
        "+//            sleep(waitTime);\r\n",
        "+//            FLClientStatus curResponse = startFLJob();\r\n",
        "+//            curStatus = startUpdateModel(curResponse);\r\n",
        "+//            return curStatus;\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        } else {\r\n",
        "+            System.out.println(\"[updateModel] updateModel failed\");\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        }\r\n",
        "+\r\n",
        "+    }\r\n",
        "+\r\n",
        "+\r\n",
        "+\r\n",
        "+    public FLClientStatus getModel() {\r\n",
        "+        String url = Common.generateUrl(use_elb, ip, port, serverNum);\r\n",
        "+        System.out.println(\"[getModel] ===========getModel url: \"+url+\"==============\");\r\n",
        "+        GetModel getModelBuf = GetModel.getInstance();\r\n",
        "+        byte[] buffer = getModelBuf.getRequestGetModel(fl_name, iteration);\r\n",
        "+        if (ifAsync) {\r\n",
        "+            try {\r\n",
        "+\r\n",
        "+                flCommunication.asyncRequest(url + \"/getModel\", buffer, new IAsyncCallBack() {\r\n",
        "+                    @Override\r\n",
        "+                    public FLClientStatus onResponse(byte[] msg) {\r\n",
        "+                        ByteBuffer debugBuffer = ByteBuffer.wrap(msg);\r\n",
        "+                        ResponseGetModel rgm = ResponseGetModel.getRootAsResponseGetModel(debugBuffer);\r\n",
        "+                        getModelBuf.doResponse(rgm);\r\n",
        "+                        return FLClientStatus.SUCCESS;\r\n",
        "+                    }\r\n",
        "+\r\n",
        "+                    @Override\r\n",
        "+                    public FLClientStatus onFailure(IOException exception) {\r\n",
        "+                        exception.printStackTrace();\r\n",
        "+                        return FLClientStatus.FAILED;\r\n",
        "+                    }\r\n",
        "+                });\r\n",
        "+            } catch (Exception e) {\r\n",
        "+                e.printStackTrace();\r\n",
        "+                return FLClientStatus.FAILED;\r\n",
        "+            }\r\n",
        "+        } else {\r\n",
        "+            try {\r\n",
        "+                byte[] message = flCommunication.syncRequest(url + \"/getModel\", buffer);\r\n",
        "+                System.out.println(\"[getModel] get model request success\");\r\n",
        "+                ByteBuffer debugBuffer = ByteBuffer.wrap(message);\r\n",
        "+                ResponseGetModel responseDataBuf = ResponseGetModel.getRootAsResponseGetModel(debugBuffer);\r\n",
        "+                status = judgeGetModel(getModelBuf, responseDataBuf);\r\n",
        "+                return status;\r\n",
        "+            } catch (IOException e) {\r\n",
        "+                System.out.println(\"[getModel] un sloved error code in getModel\");\r\n",
        "+                e.printStackTrace();\r\n",
        "+                status = FLClientStatus.FAILED;\r\n",
        "+                return FLClientStatus.FAILED;\r\n",
        "+            }\r\n",
        "+\r\n",
        "+        }\r\n",
        "+        return FLClientStatus.SUCCESS;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus judgeGetModel(GetModel getModelBuf, ResponseGetModel responseDataBuf) {\r\n",
        "+        //Debug code\r\n",
        "+        System.out.println(\"[getModel] ==========get model content is:================\");\r\n",
        "+        System.out.println(\"[getModel] ==========retcode: \" + responseDataBuf.retcode());\r\n",
        "+        System.out.println(\"[getModel] ==========reason: \" + responseDataBuf.reason());\r\n",
        "+        System.out.println(\"[getModel] ==========iteration: \" + responseDataBuf.iteration());\r\n",
        "+        System.out.println(\"[getModel] ==========time: \" + responseDataBuf.timestemp());\r\n",
        "+        int retcode = responseDataBuf.retcode();\r\n",
        "+        FLClientStatus curStatus;\r\n",
        "+        switch (retcode) {\r\n",
        "+            case (ResponseCode.SUCCEED):\r\n",
        "+                FLClientStatus status = getModelBuf.doResponse(fl_name,responseDataBuf);\r\n",
        "+                if (status != FLClientStatus.SUCCESS) {\r\n",
        "+                    System.out.println(\"[getModel] catch error in getModel.doResponse\");\r\n",
        "+                    return FLClientStatus.FAILED;\r\n",
        "+                }\r\n",
        "+                System.out.println(\"[test] ==============test combine model==============\");\r\n",
        "+                testCombineModel();\r\n",
        "+                return FLClientStatus.SUCCESS;\r\n",
        "+            case (ResponseCode.SucNotReady):\r\n",
        "+                // todo, server need add next_req_time in ResponseGetModel\r\n",
        "+                sleep(200);\r\n",
        "+                curStatus = getModel();\r\n",
        "+                return curStatus;\r\n",
        "+            case (ResponseCode.RequestError):\r\n",
        "+            case (ResponseCode.SystemError):\r\n",
        "+                System.out.println(\"[getModel] catch RequestError or SystemError\");\r\n",
        "+                return FLClientStatus.FAILED;\r\n",
        "+            default:\r\n",
        "+                return FLClientStatus.FAILED;\r\n",
        "+\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public FLClientStatus getFeatureMask() {\r\n",
        "+        System.out.println(\"[Encrypt] creating feature mask of <\" + encryptLevel.toString() + \">\");\r\n",
        "+        secureProtocol.setPWParameter(fl_id, iteration, ip, port, minSecretNum, prime, featureSize, ifAsync, use_elb, serverNum);\r\n",
        "+        secureProtocol.pwCreateMask();\r\n",
        "+        FLClientStatus response = secureProtocol.getStatus();\r\n",
        "+        FLClientStatus curStatus;\r\n",
        "+        if (response == FLClientStatus.WAIT) {\r\n",
        "+//            System.out.println(\"Create feature mask OutOfTime, need wait and request again\");\r\n",
        "+//            long waitTime = getWaitTime(secureProtocol.getNextRequestTime());\r\n",
        "+//            sleep(waitTime);\r\n",
        "+//            FLClientStatus curResponse = startFLJob();\r\n",
        "+//            curStatus = startUpdateModel(curResponse);\r\n",
        "+//            status = curStatus;\r\n",
        "+//            return curStatus;\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        } else if (response == FLClientStatus.SUCCESS) {\r\n",
        "+            System.out.println(\"[Encrypt] Create feature mask succeed\");\r\n",
        "+            status = FLClientStatus.SUCCESS;\r\n",
        "+            return FLClientStatus.SUCCESS;\r\n",
        "+        } else {\r\n",
        "+            System.out.println(\"[Encrypt] Create feature mask failed\");\r\n",
        "+            status = FLClientStatus.FAILED;\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public  FLClientStatus unMasking() {\r\n",
        "+        FLClientStatus curStatus;\r\n",
        "+        secureProtocol.pwUnmasking();\r\n",
        "+        FLClientStatus response = secureProtocol.getStatus();\r\n",
        "+        if (response == FLClientStatus.WAIT) {\r\n",
        "+//            System.out.println(\"unmasking OutOfTime, need wait and request again\");\r\n",
        "+//            long waitTime = getWaitTime(secureProtocol.getNextRequestTime());\r\n",
        "+//            sleep(waitTime);\r\n",
        "+//            FLClientStatus curResponse = startFLJob();\r\n",
        "+//            curStatus = startUpdateModel(curResponse);\r\n",
        "+//            status = curStatus;\r\n",
        "+//            return curStatus;\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        }else if (response == FLClientStatus.SUCCESS) {\r\n",
        "+            System.out.println(\"[Encrypt] unmasking succeed\");\r\n",
        "+            status = FLClientStatus.SUCCESS;\r\n",
        "+            return FLClientStatus.SUCCESS;\r\n",
        "+        } else {\r\n",
        "+            System.out.println(\"[Encrypt] unmasking failed\");\r\n",
        "+            status = FLClientStatus.FAILED;\r\n",
        "+            return FLClientStatus.FAILED;\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+\r\n",
        "+    public void sleep(long millis) {\r\n",
        "+        try {\r\n",
        "+            Thread.sleep(millis);                 //1000 milliseconds is one second.\r\n",
        "+        } catch(InterruptedException ex) {\r\n",
        "+            Thread.currentThread().interrupt();\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public long getWaitTime(String nextRequestTime) {\r\n",
        "+        Date date = new Date();\r\n",
        "+        long currentTime = date.getTime();\r\n",
        "+        long waitTime = Long.valueOf(nextRequestTime) - currentTime;\r\n",
        "+        return waitTime;\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    public void testCombineModel(){\r\n",
        "+        setInput(test_dataset, test_batch_num);\r\n",
        "+        evaluate();\r\n",
        "+    }\r\n",
        "+\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * init runtime resource only needs to be called once per client\r\n",
        "+     */\r\n",
        "+    public void initRuntimeResource() {\r\n",
        "+//        Train train = Train.getInstance();\r\n",
        "+//        train.prepare();\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * @param dataset,  train or test dataset and label set\r\n",
        "+     * @param batch_num\r\n",
        "+     */\r\n",
        "+    public void setInput(String dataset, int batch_num) {\r\n",
        "+        System.out.println(\"==========set input===========\");\r\n",
        "+        LiteTrain train = LiteTrain.getInstance();\r\n",
        "+        train.setInput(dataset, batch_num);\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * evalute trained model performance\r\n",
        "+     */\r\n",
        "+    public void evaluate() {\r\n",
        "+        System.out.println(\"===========evaluate=============\");\r\n",
        "+        LiteTrain train = LiteTrain.getInstance();\r\n",
        "+        int status = train.inference(fl_name,32,1);\r\n",
        "+        if (status != FLClientStatus.SUCCESS.ordinal()) {\r\n",
        "+            System.out.println(\"inference failed\");\r\n",
        "+            throw new RuntimeException();\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    @Override\r\n",
        "+    protected void finalize() {\r\n",
        "+        LiteTrain train = LiteTrain.getInstance();\r\n",
        "+        train.free();\r\n",
        "+    }\r\n",
        "+}\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/GetModel.java b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/GetModel.java\r\n",
        "index 872d7de..eb19c50 100644\r\n",
        "--- a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/GetModel.java\r\n",
        "+++ b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/GetModel.java\r\n",
        "@@ -78,4 +78,16 @@ public class GetModel {\r\n",
        "         train.updateFeatures(fms);\r\n",
        "         return FLClientStatus.SUCCESS;\r\n",
        "     }\r\n",
        "+    public FLClientStatus doResponse(String modelName,ResponseGetModel getModel) {\r\n",
        "+        int num = getModel.featureMapLength();\r\n",
        "+        ArrayList<FeatureMap> fms = new ArrayList<FeatureMap>();\r\n",
        "+        for (int i = 0; i < num; i++) {\r\n",
        "+            FeatureMap feature = getModel.featureMap(i);\r\n",
        "+            fms.add(feature);\r\n",
        "+            System.out.println(\"get [\" + i + \"] \" + feature.weightFullname() + \", elenums: \" + feature.dataLength());\r\n",
        "+        }\r\n",
        "+        LiteTrain train = LiteTrain.getInstance();\r\n",
        "+        train.updateFeatures(modelName,fms);\r\n",
        "+        return FLClientStatus.SUCCESS;\r\n",
        "+    }\r\n",
        " }\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/LiteTrain.java b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/LiteTrain.java\r\n",
        "new file mode 100644\r\n",
        "index 0000000..877ca40\r\n",
        "--- /dev/null\r\n",
        "+++ b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/LiteTrain.java\r\n",
        "@@ -0,0 +1,83 @@\r\n",
        "+/**\r\n",
        "+ * Copyright 2020 Huawei Technologies Co., Ltd\r\n",
        "+ * <p>\r\n",
        "+ * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "+ * you may not use this file except in compliance with the License.\r\n",
        "+ * You may obtain a copy of the License at\r\n",
        "+ * <p>\r\n",
        "+ * http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "+ * <p>\r\n",
        "+ * Unless required by applicable law or agreed to in writing, software\r\n",
        "+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "+ * See the License for the specific language governing permissions and\r\n",
        "+ * limitations under the License.\r\n",
        "+ */\r\n",
        "+\r\n",
        "+package com.huawei.flclient;\r\n",
        "+\r\n",
        "+import com.google.flatbuffers.FlatBufferBuilder;\r\n",
        "+import mindspore.schema.FeatureMap;\r\n",
        "+import java.util.ArrayList;\r\n",
        "+\r\n",
        "+public  class LiteTrain {\r\n",
        "+    static {\r\n",
        "+        System.loadLibrary(\"fl\");\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    private static LiteTrain train;\r\n",
        "+\r\n",
        "+    private LiteTrain() {\r\n",
        "+    }\r\n",
        "+    public static synchronized LiteTrain getInstance() {\r\n",
        "+        if (train == null) {\r\n",
        "+            train = new LiteTrain();\r\n",
        "+        }\r\n",
        "+        return train;\r\n",
        "+    }\r\n",
        "+    /**\r\n",
        "+     * set the Inference set or Train set\r\n",
        "+     *\r\n",
        "+     * @param fileSet   input binary file path which format is NHWC\r\n",
        "+     * @param batch_num binary file batch num\r\n",
        "+     * @return\r\n",
        "+     */\r\n",
        "+    native int setInput(String fileSet, int num);\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * inference\r\n",
        "+     *\r\n",
        "+     * @return status\r\n",
        "+     */\r\n",
        "+    public native int inference(String modelName,int batch_num,int test_nums);\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * train\r\n",
        "+     *\r\n",
        "+     * @return status\r\n",
        "+     */\r\n",
        "+    public native int train(String modelName, int batch_num,int iterations);\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * get the features map of training model\r\n",
        "+     *\r\n",
        "+     * @param builder FlatBufferBuilder\r\n",
        "+     * @return features offset\r\n",
        "+     */\r\n",
        "+    native int[] getFeaturesMap(String modelName,FlatBufferBuilder builder);\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * update the features map of training model\r\n",
        "+     *\r\n",
        "+     * @param featureMaps\r\n",
        "+     * @return status\r\n",
        "+     */\r\n",
        "+    native int updateFeatures(String modelName,ArrayList<FeatureMap> featureMaps);\r\n",
        "+\r\n",
        "+    /**\r\n",
        "+     * free Inference or Train runtime memory resource\r\n",
        "+     *\r\n",
        "+     * @return status\r\n",
        "+     */\r\n",
        "+    native int free();\r\n",
        "+}\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/StartFLJob.java b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/StartFLJob.java\r\n",
        "index 55641f1..3db5e42 100644\r\n",
        "--- a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/StartFLJob.java\r\n",
        "+++ b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/StartFLJob.java\r\n",
        "@@ -105,6 +105,21 @@ public class StartFLJob {\r\n",
        "         return FLClientStatus.SUCCESS;\r\n",
        "     }\r\n",
        " \r\n",
        "+    private FLClientStatus parseResponse(String modelName,ResponseFLJob flJob) {\r\n",
        "+        int fmCount = flJob.featureMapLength();\r\n",
        "+        ArrayList<FeatureMap> featureMaps = new ArrayList<FeatureMap>();\r\n",
        "+        for (int i = 0; i < fmCount; i++) {\r\n",
        "+            FeatureMap feature = flJob.featureMap(i);\r\n",
        "+            featureMaps.add(feature);\r\n",
        "+            System.out.println(\"get [\" + i + \"] \" + feature.weightFullname() + \", elenums: \" + feature.dataLength());\r\n",
        "+            featureSize += feature.dataLength();\r\n",
        "+        }\r\n",
        "+        if (true) {\r\n",
        "+            LiteTrain train = LiteTrain.getInstance();\r\n",
        "+            train.updateFeatures(modelName,featureMaps);       // load featureMaps to model\r\n",
        "+        }\r\n",
        "+        return FLClientStatus.SUCCESS;\r\n",
        "+    }\r\n",
        "     public FLClientStatus doResponse(ResponseFLJob flJob) {\r\n",
        "         System.out.println(\"[startFLJob] return code: \" + flJob.retcode());\r\n",
        "         System.out.println(\"[startFLJob] reason: \" + flJob.reason());\r\n",
        "@@ -133,6 +148,34 @@ public class StartFLJob {\r\n",
        "         }\r\n",
        "     }\r\n",
        " \r\n",
        "+    public FLClientStatus doResponse(String modelName,ResponseFLJob flJob) {\r\n",
        "+        System.out.println(\"[startFLJob] return code: \" + flJob.retcode());\r\n",
        "+        System.out.println(\"[startFLJob] reason: \" + flJob.reason());\r\n",
        "+        System.out.println(\"[startFLJob] iteration: \" + flJob.iteration());\r\n",
        "+        System.out.println(\"[startFLJob] is selected: \" + flJob.isSelected());\r\n",
        "+        System.out.println(\"[startFLJob] next request time: \" + flJob.nextReqTime());\r\n",
        "+\r\n",
        "+        // skip mind ir\r\n",
        "+        System.out.println(\"[startFLJob] timestamp: \" + flJob.timestemp());\r\n",
        "+        int retcode = flJob.retcode();\r\n",
        "+\r\n",
        "+        switch (retcode) {\r\n",
        "+            case (ResponseCode.SUCCEED):\r\n",
        "+                parseResponse(modelName,flJob);\r\n",
        "+                return FLClientStatus.SUCCESS;\r\n",
        "+            case (ResponseCode.OutOfTime):\r\n",
        "+//            case (ResponseCode.NotSelected):     // todo: need add to fl_job.fbs\r\n",
        "+                return FLClientStatus.WAIT;\r\n",
        "+            case (ResponseCode.SucNotMatch):\r\n",
        "+            case (ResponseCode.SucNotReady):\r\n",
        "+                System.out.println(\"[startFLJob] catch RequestError or SystemError\");\r\n",
        "+                return FLClientStatus.FAILED;\r\n",
        "+            default:\r\n",
        "+                System.out.println(\"[startFLJob] nresolved error code\");\r\n",
        "+                return FLClientStatus.FAILED;\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+\r\n",
        "     public FLClientStatus getStatus() {\r\n",
        "         return this.status;\r\n",
        "     }\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/UpdateModel.java b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/UpdateModel.java\r\n",
        "index 0b886c7..d3e3a36 100644\r\n",
        "--- a/mindspore/lite/flclient/src/main/java/com/huawei/flclient/UpdateModel.java\r\n",
        "+++ b/mindspore/lite/flclient/src/main/java/com/huawei/flclient/UpdateModel.java\r\n",
        "@@ -66,9 +66,16 @@ public class UpdateModel {\r\n",
        "             return this;\r\n",
        "         }\r\n",
        " \r\n",
        "-        public RequestUpdateModelBuilder featuresmap() {\r\n",
        "-            Train train = Train.getInstance();\r\n",
        "-            int[] fmOffsets = train.getFeaturesMap(this.builder);\r\n",
        "+//        public RequestUpdateModelBuilder featuresmap(String modelName) {\r\n",
        "+//            LiteTrain train = LiteTrain.getInstance();\r\n",
        "+//            int[] fmOffsets = train.getFeaturesMap(modelName,this.builder);\r\n",
        "+//            this.fmOffset = RequestUpdateModel.createFeatureMapVector(builder, fmOffsets);\r\n",
        "+//            return this;\r\n",
        "+//        }\r\n",
        "+\r\n",
        "+        public RequestUpdateModelBuilder featuresmap(String modelName) {\r\n",
        "+            LiteTrain train = LiteTrain.getInstance();\r\n",
        "+            int[] fmOffsets = train.getFeaturesMap(modelName,this.builder);\r\n",
        "             this.fmOffset = RequestUpdateModel.createFeatureMapVector(builder, fmOffsets);\r\n",
        "             return this;\r\n",
        "         }\r\n",
        "@@ -118,7 +125,7 @@ public class UpdateModel {\r\n",
        " \r\n",
        "     public byte[] getRequestUpdateFLJob(EncryptLevel encryptLevel, String name, String id, int iteration, SecureProtocol secureProtocol) {\r\n",
        "         RequestUpdateModelBuilder builder = new RequestUpdateModelBuilder(encryptLevel);\r\n",
        "-        return builder.flName(name).time().id(id).featuresmap().iteration(iteration).build(secureProtocol);\r\n",
        "+        return builder.flName(name).time().id(id).featuresmap(name).iteration(iteration).build(secureProtocol);\r\n",
        "     }\r\n",
        " \r\n",
        "     public FLClientStatus doResponse(ResponseUpdateModel response) {\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/native/CMakeLists.txt b/mindspore/lite/flclient/src/main/native/CMakeLists.txt\r\n",
        "index 80f9931..4a3938c 100644\r\n",
        "--- a/mindspore/lite/flclient/src/main/native/CMakeLists.txt\r\n",
        "+++ b/mindspore/lite/flclient/src/main/native/CMakeLists.txt\r\n",
        "@@ -2,14 +2,33 @@ cmake_minimum_required(VERSION 3.14)\r\n",
        " \r\n",
        " project(FederalLearning)\r\n",
        " \r\n",
        "+option(SUPPORT_GPU \"if support gpu\" off)\r\n",
        "+set(BUILD_LITE \"on\")\r\n",
        "+set(SUPPORT_TRAIN \"on\")\r\n",
        "+set(PLATFORM_ARM \"on\")\r\n",
        "+\r\n",
        "+set(TOP_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../../../../../../)\r\n",
        "+set(LITE_DIR ${TOP_DIR}/mindspore/lite)\r\n",
        "+set(MS_VERSION_MAJOR ${MS_VERSION_MAJOR})\r\n",
        "+set(MS_VERSION_MINOR ${MS_VERSION_MINOR})\r\n",
        "+set(MS_VERSION_REVISION ${MS_VERSION_REVISION})\r\n",
        "+set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DMS_VERSION_MAJOR=${MS_VERSION_MAJOR} -DMS_VERSION_MINOR=${MS_VERSION_MINOR} -DMS_VERSION_REVISION=${MS_VERSION_REVISION}\")\r\n",
        "+set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DMS_VERSION_MAJOR=${MS_VERSION_MAJOR} -DMS_VERSION_MINOR=${MS_VERSION_MINOR} -DMS_VERSION_REVISION=${MS_VERSION_REVISION}\")\r\n",
        " set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++17\")\r\n",
        " \r\n",
        " include_directories(${CMAKE_CURRENT_SOURCE_DIR})\r\n",
        " include_directories(${CMAKE_CURRENT_SOURCE_DIR}/linux)\r\n",
        " include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\r\n",
        " include_directories(${CMAKE_CURRENT_SOURCE_DIR}/src)\r\n",
        "+if (ENABLE_MICRO)\r\n",
        " include_directories(${CMAKE_CURRENT_SOURCE_DIR}/src/runtime)\r\n",
        "+endif()\r\n",
        "+include_directories(${LITE_DIR}) ## lite include\r\n",
        "+include_directories(${TOP_DIR}) ## api include\r\n",
        "+include_directories(${TOP_DIR}/mindspore/core/) ## core include\r\n",
        "+include_directories(${LITE_DIR}/build) ## flatbuffers\r\n",
        " \r\n",
        "+if (ENABLE_MICRO)\r\n",
        " set(OP_SRC\r\n",
        "     src/nnacl/arithmetic_common.c\r\n",
        "     src/nnacl/common_func.c\r\n",
        "@@ -43,11 +62,23 @@ set(OP_SRC\r\n",
        "     src/fl_lenet.c\r\n",
        "     src/weight_files/fl_lenet_weight_epoch_0.c\r\n",
        " )\r\n",
        "-\r\n",
        "+else()\r\n",
        "+    set(OP_SRC\r\n",
        "+            src/lenet_train.cpp\r\n",
        "+            )\r\n",
        "+    endif()\r\n",
        "+if (ENABLE_MICRO)\r\n",
        "+    set(SRC_FILES\r\n",
        "+            flearning.cpp)\r\n",
        "+    else()\r\n",
        " set(SRC_FILES\r\n",
        "-  flearning.cpp\r\n",
        "-)\r\n",
        "+        lenet_train_jni.cpp\r\n",
        "+        )\r\n",
        "+endif()\r\n",
        "+find_library(log-lib glog)\r\n",
        " \r\n",
        "-link_directories(${CMAKE_CURRENT_SOURCE_DIR}/lib/)\r\n",
        " add_library(fl SHARED ${SRC_FILES} ${OP_SRC})\r\n",
        "+target_link_libraries(fl mindspore-lite  glog)\r\n",
        "+link_directories(${CMAKE_CURRENT_SOURCE_DIR}/lib/)\r\n",
        "+\r\n",
        " install(TARGETS fl LIBRARY DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/lib)\r\n",
        "\\ No newline at end of file\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/native/include/lenet_train.h b/mindspore/lite/flclient/src/main/native/include/lenet_train.h\r\n",
        "new file mode 100644\r\n",
        "index 0000000..5108e65\r\n",
        "--- /dev/null\r\n",
        "+++ b/mindspore/lite/flclient/src/main/native/include/lenet_train.h\r\n",
        "@@ -0,0 +1,35 @@\r\n",
        "+/**\r\n",
        "+ * Copyright 2020 Huawei Technologies Co., Ltd\r\n",
        "+ *\r\n",
        "+ * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "+ * you may not use this file except in compliance with the License.\r\n",
        "+ * You may obtain a copy of the License at\r\n",
        "+ *\r\n",
        "+ * http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "+ *\r\n",
        "+ * Unless required by applicable law or agreed to in writing, software\r\n",
        "+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "+ * See the License for the specific language governing permissions and\r\n",
        "+ * limitations under the License.\r\n",
        "+ */\r\n",
        "+\r\n",
        "+#ifndef MSLITE_FL_LITE_LENET_H\r\n",
        "+#define MSLITE_FL_LITE_LENET_H\r\n",
        "+\r\n",
        "+#include <string>\r\n",
        "+#include \"include/train_session.h\"\r\n",
        "+\r\n",
        "+using mindspore::session::TrainFeatureParam;\r\n",
        "+\r\n",
        "+int fl_lenet_lite_Train(const std::string &ms_file,const int batch_num, const int iterations);\r\n",
        "+\r\n",
        "+int fl_lenet_lite_Inference(const std::string &ms_file,int batch_num,int test_nums);\r\n",
        "+\r\n",
        "+int fl_lenet_lite_GetFeatures(const std::string &update_ms_file,mindspore::session::TrainFeatureParam *** features,int* size);\r\n",
        "+int fl_lenet_lite_UpdateFeatures(const std::string &update_ms_file,\r\n",
        "+                                 TrainFeatureParam* new_features,int size);\r\n",
        "+mindspore::session::TrainSession * GetSession(const std::string& ms_file,bool train_mode=false);\r\n",
        "+\r\n",
        "+int fl_lenet_lite_SetInputs(const std::string &files, int num);\r\n",
        "+#endif  // MSLITE_FL_LITE_LENET_H\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/native/lenet_train_jni.cpp b/mindspore/lite/flclient/src/main/native/lenet_train_jni.cpp\r\n",
        "new file mode 100644\r\n",
        "index 0000000..617c8cb\r\n",
        "--- /dev/null\r\n",
        "+++ b/mindspore/lite/flclient/src/main/native/lenet_train_jni.cpp\r\n",
        "@@ -0,0 +1,158 @@\r\n",
        "+/**\r\n",
        "+ * Copyright 2020 Huawei Technologies Co., Ltd\r\n",
        "+ *\r\n",
        "+ * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "+ * you may not use this file except in compliance with the License.\r\n",
        "+ * You may obtain a copy of the License at\r\n",
        "+ *\r\n",
        "+ * http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "+ *\r\n",
        "+ * Unless required by applicable law or agreed to in writing, software\r\n",
        "+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "+ * See the License for the specific language governing permissions and\r\n",
        "+ * limitations under the License.\r\n",
        "+ */\r\n",
        "+\r\n",
        "+#include <jni.h>\r\n",
        "+#include \"include/train_session.h\"\r\n",
        "+#include \"include/errorcode.h\"\r\n",
        "+#include \"lenet_train.h\"\r\n",
        "+#include <cstring>\r\n",
        "+#include \"src/common/log_adapter.h\"\r\n",
        "+\r\n",
        "+static jobject fbb;\r\n",
        "+static jmethodID create_string_char;\r\n",
        "+\r\n",
        "+char *JstringToChar(JNIEnv *env, jstring jstr) {\r\n",
        "+  char *rtn = nullptr;\r\n",
        "+  jclass clsstring = env->FindClass(\"java/lang/String\");\r\n",
        "+  jstring strencode = env->NewStringUTF(\"GB2312\");\r\n",
        "+  jmethodID mid = env->GetMethodID(clsstring, \"getBytes\", \"(Ljava/lang/String;)[B\");\r\n",
        "+  jbyteArray barr = (jbyteArray)env->CallObjectMethod(jstr, mid, strencode);\r\n",
        "+  jsize alen = env->GetArrayLength(barr);\r\n",
        "+  jbyte *ba = env->GetByteArrayElements(barr, JNI_FALSE);\r\n",
        "+  if (alen > 0) {\r\n",
        "+    rtn = new char[alen + 1];\r\n",
        "+    memcpy(rtn, ba, alen);\r\n",
        "+    rtn[alen] = 0;\r\n",
        "+  }\r\n",
        "+  env->ReleaseByteArrayElements(barr, ba, 0);\r\n",
        "+  return rtn;\r\n",
        "+}\r\n",
        "+extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_LiteTrain_train(JNIEnv *env, jobject thiz, jstring ms_file,\r\n",
        "+                                                                           jint batch_num, jint iterations) {\r\n",
        "+  return fl_lenet_lite_Train(JstringToChar(env, ms_file), batch_num, iterations);\r\n",
        "+}\r\n",
        "+\r\n",
        "+extern \"C\" jint CreateFeatureMap(JNIEnv *env, const char *name, float *data, size_t size) {\r\n",
        "+  jstring name1 = env->NewStringUTF(name);\r\n",
        "+  jint name_offset = env->CallIntMethod(fbb, create_string_char, name1);\r\n",
        "+  // 1. set data size\r\n",
        "+  jfloatArray ret = env->NewFloatArray(size);\r\n",
        "+  env->SetFloatArrayRegion(ret, 0, size, data);\r\n",
        "+  // 2. get methodid createDataVector\r\n",
        "+  jclass fm_cls = env->FindClass(\"mindspore/schema/FeatureMap\");\r\n",
        "+  jmethodID createDataVector =\r\n",
        "+    env->GetStaticMethodID(fm_cls, \"createDataVector\", \"(Lcom/google/flatbuffers/FlatBufferBuilder;[F)I\");\r\n",
        "+  // 3. calc data offset\r\n",
        "+  jint data_offset = env->CallStaticIntMethod(fm_cls, createDataVector, fbb, ret);\r\n",
        "+  jmethodID createFeatureMap =\r\n",
        "+    env->GetStaticMethodID(fm_cls, \"createFeatureMap\", \"(Lcom/google/flatbuffers/FlatBufferBuilder;II)I\");\r\n",
        "+  jint fm_offset = env->CallStaticIntMethod(fm_cls, createFeatureMap, fbb, name_offset, data_offset);\r\n",
        "+  return fm_offset;\r\n",
        "+}\r\n",
        "+\r\n",
        "+extern \"C\" JNIEXPORT jintArray JNICALL Java_com_huawei_flclient_LiteTrain_getFeaturesMap(JNIEnv *env, jobject thiz,\r\n",
        "+                                                                                         jstring ms_file,\r\n",
        "+                                                                                         jobject builder) {\r\n",
        "+  fbb = builder;\r\n",
        "+  jclass fb_clazz = env->GetObjectClass(builder);\r\n",
        "+  create_string_char = env->GetMethodID(fb_clazz, \"createString\", \"(Ljava/lang/CharSequence;)I\");\r\n",
        "+  TrainFeatureParam **train_features = nullptr;\r\n",
        "+  int feature_size = 0;\r\n",
        "+  auto status = fl_lenet_lite_GetFeatures(JstringToChar(env, ms_file), &train_features, &feature_size);\r\n",
        "+  if(status != mindspore::lite::RET_OK) {\r\n",
        "+    MS_LOG(ERROR) << \"get features failed:\" << ms_file;\r\n",
        "+    return env->NewIntArray(0);\r\n",
        "+  }\r\n",
        "+  jintArray ret = env->NewIntArray(feature_size);\r\n",
        "+  jint *data = env->GetIntArrayElements(ret, NULL);\r\n",
        "+\r\n",
        "+  for (int i = 0; i < feature_size; i++) {\r\n",
        "+        data[i] =\r\n",
        "+          CreateFeatureMap(env, train_features[i]->name, (float *)train_features[i]->data,\r\n",
        "+          train_features[i]->elenums);\r\n",
        "+        MS_LOG(INFO) << \"upload feature:\"<< \", name:\" << train_features[i]->name << \", elenums:\" <<\r\n",
        "+        train_features[i]->elenums;\r\n",
        "+  }\r\n",
        "+    env->ReleaseIntArrayElements(ret, data, 0);\r\n",
        "+    for (int i = 0; i < feature_size; i++) {\r\n",
        "+      delete  train_features[i];\r\n",
        "+    }\r\n",
        "+  return ret;\r\n",
        "+}\r\n",
        "+\r\n",
        "+extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_LiteTrain_updateFeatures(JNIEnv *env, jobject,\r\n",
        "+                                                                                    jstring ms_file, jobject features) {\r\n",
        "+  jclass arr_cls = env->GetObjectClass(features);\r\n",
        "+  jmethodID size_method = env->GetMethodID(arr_cls, \"size\", \"()I\");\r\n",
        "+  jmethodID get_method = env->GetMethodID(arr_cls, \"get\", \"(I)Ljava/lang/Object;\");\r\n",
        "+\r\n",
        "+  jclass fm_cls = env->FindClass(\"mindspore/schema/FeatureMap\");\r\n",
        "+  jmethodID weight_name_method = env->GetMethodID(fm_cls, \"weightFullname\", \"()Ljava/lang/String;\");\r\n",
        "+  jmethodID data_length_method = env->GetMethodID(fm_cls, \"dataLength\", \"()I\");\r\n",
        "+  jmethodID data_method = env->GetMethodID(fm_cls, \"data\", \"(I)F\");\r\n",
        "+  jclass clsstring = env->FindClass(\"java/lang/String\");\r\n",
        "+  jmethodID mid = env->GetMethodID(clsstring, \"getBytes\", \"(Ljava/lang/String;)[B\");\r\n",
        "+  int size = env->CallIntMethod(features, size_method);\r\n",
        "+  // transform FeatureMap to TrainFeatureParm\r\n",
        "+  TrainFeatureParam *features_param = (TrainFeatureParam *)malloc(size * sizeof(TrainFeatureParam));\r\n",
        "+  for (int i = 0; i < size; ++i) {\r\n",
        "+    TrainFeatureParam *param = features_param + i;\r\n",
        "+    jobject feature = env->CallObjectMethod(features, get_method, i);\r\n",
        "+    // set feature_param name\r\n",
        "+    jstring weight_full_name = (jstring)env->CallObjectMethod(feature, weight_name_method);\r\n",
        "+    jstring strencode = env->NewStringUTF(\"GB2312\");\r\n",
        "+    jbyteArray barr = (jbyteArray)env->CallObjectMethod(weight_full_name, mid, strencode);\r\n",
        "+    char *name = nullptr;\r\n",
        "+    jsize alen = env->GetArrayLength(barr);\r\n",
        "+    jbyte *ba = env->GetByteArrayElements(barr, JNI_FALSE);\r\n",
        "+    if (alen > 0) {\r\n",
        "+      name = new char[alen + 1];\r\n",
        "+      if (ba == nullptr) {\r\n",
        "+        MS_LOG(ERROR) << \"name is nullptr\";\r\n",
        "+        return mindspore::lite::RET_ERROR;\r\n",
        "+      }\r\n",
        "+      memcpy(name, ba, alen);\r\n",
        "+      name[alen] = 0;\r\n",
        "+    }\r\n",
        "+    param->name = name;\r\n",
        "+    env->ReleaseByteArrayElements(barr, ba, 0);\r\n",
        "+    int data_length = env->CallIntMethod(feature, data_length_method);\r\n",
        "+    float *data = static_cast<float *>(malloc(data_length * sizeof(float)));\r\n",
        "+    memset(data, 0, data_length * sizeof(float));\r\n",
        "+    for (int j = 0; j < data_length; ++j) {\r\n",
        "+      float *addr = data + j;\r\n",
        "+      *addr = env->CallFloatMethod(feature, data_method, j);\r\n",
        "+    }\r\n",
        "+    param->data = data;\r\n",
        "+    param->elenums = data_length;\r\n",
        "+    param->type = mindspore::kNumberTypeFloat32;\r\n",
        "+    MS_LOG(INFO) << \"get feature:\" << param->name << \",elenums:\" << param->elenums;\r\n",
        "+  }\r\n",
        "+  return fl_lenet_lite_UpdateFeatures(JstringToChar(env, ms_file), features_param, size);\r\n",
        "+}\r\n",
        "+\r\n",
        "+extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_LiteTrain_setInput(JNIEnv *env, jobject, jstring files,\r\n",
        "+                                                                              jint nums) {\r\n",
        "+  return fl_lenet_lite_SetInputs(JstringToChar(env, files),nums);\r\n",
        "+}\r\n",
        "+\r\n",
        "+extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_LiteTrain_inference(JNIEnv *env, jobject, jstring ms_file,\r\n",
        "+                                                                               jint batch_num, jint test_nums) {;\r\n",
        "+  auto accuary = fl_lenet_lite_Inference(JstringToChar(env, ms_file), batch_num, test_nums);\r\n",
        "+  return accuary;\r\n",
        "+}\r\n",
        "+\r\n",
        "+extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_LiteTrain_free(JNIEnv *, jobject) { return 0; }\r\n",
        "\\ No newline at end of file\r\n",
        "diff --git a/mindspore/lite/flclient/src/main/native/src/lenet_train.cpp b/mindspore/lite/flclient/src/main/native/src/lenet_train.cpp\r\n",
        "new file mode 100644\r\n",
        "index 0000000..9a0a6b1\r\n",
        "--- /dev/null\r\n",
        "+++ b/mindspore/lite/flclient/src/main/native/src/lenet_train.cpp\r\n",
        "@@ -0,0 +1,270 @@\r\n",
        "+/**\r\n",
        "+ * Copyright 2020 Huawei Technologies Co., Ltd\r\n",
        "+ *\r\n",
        "+ * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "+ * you may not use this file except in compliance with the License.\r\n",
        "+ * You may obtain a copy of the License at\r\n",
        "+ *\r\n",
        "+ * http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "+ *\r\n",
        "+ * Unless required by applicable law or agreed to in writing, software\r\n",
        "+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "+ * See the License for the specific language governing permissions and\r\n",
        "+ * limitations under the License.\r\n",
        "+ */\r\n",
        "+#include \"lenet_train.h\"\r\n",
        "+#include \"include/errorcode.h\"\r\n",
        "+#include \"include/context.h\"\r\n",
        "+#include <cstring>\r\n",
        "+#include <iostream>\r\n",
        "+#include <fstream>\r\n",
        "+#include \"include/api/lite_context.h\"\r\n",
        "+#include \"src/common/log_adapter.h\"\r\n",
        "+\r\n",
        "+static char *fl_lenet_I0 = 0;\r\n",
        "+static char *fl_lenet_I1 = 0;\r\n",
        "+unsigned int seed_ = time(NULL);\r\n",
        "+\r\n",
        "+std::vector<int> FillInputData(mindspore::session::TrainSession *train_session, int batch_num, bool serially) {\r\n",
        "+  std::vector<int> labels_vec;\r\n",
        "+  auto inputs = train_session->GetInputs();\r\n",
        "+  int batch_size = inputs[0]->shape()[0];\r\n",
        "+  static unsigned int idx = 1;\r\n",
        "+  int data_size = inputs[0]->ElementsNum() / batch_size;\r\n",
        "+  int num_classes = inputs[1]->shape()[1];\r\n",
        "+  char *input_data = reinterpret_cast<char *>(inputs.at(0)->MutableData());\r\n",
        "+  auto labels = reinterpret_cast<float *>(inputs.at(1)->MutableData());\r\n",
        "+  std::fill(labels, labels + inputs.at(1)->ElementsNum(), 0.f);\r\n",
        "+  for (int i = 0; i < batch_size; i++) {\r\n",
        "+    if (serially) {\r\n",
        "+      idx = ++idx % batch_num;\r\n",
        "+    } else {\r\n",
        "+      idx = rand_r(&seed_) % batch_num;\r\n",
        "+    }\r\n",
        "+    std::memcpy(input_data + i * data_size, fl_lenet_I0 + idx * data_size, data_size);\r\n",
        "+    int label_idx = *((int *)(fl_lenet_I1) + idx);\r\n",
        "+    labels[i * num_classes + label_idx] = 1.0;  // Model expects labels in onehot representation\r\n",
        "+    labels_vec.push_back(label_idx);\r\n",
        "+  }\r\n",
        "+  return labels_vec;\r\n",
        "+}\r\n",
        "+\r\n",
        "+mindspore::tensor::MSTensor *SearchOutputsForSize(mindspore::session::TrainSession *train_session, size_t size) {\r\n",
        "+  auto outputs = train_session->GetOutputs();\r\n",
        "+  for (auto it = outputs.begin(); it != outputs.end(); ++it) {\r\n",
        "+    if (it->second->ElementsNum() == size) return it->second;\r\n",
        "+  }\r\n",
        "+  MS_LOG(ERROR) << \"Model does not have an output tensor with size \";\r\n",
        "+  return nullptr;\r\n",
        "+}\r\n",
        "+\r\n",
        "+float GetLoss(mindspore::session::TrainSession *train_session) {\r\n",
        "+  auto outputsv = SearchOutputsForSize(train_session, 1);  // Search for Loss which is a single value tensor\r\n",
        "+  if (outputsv == nullptr) {\r\n",
        "+    return 10000;\r\n",
        "+  }\r\n",
        "+  auto loss = reinterpret_cast<float *>(outputsv->MutableData());\r\n",
        "+  return loss[0];\r\n",
        "+}\r\n",
        "+mindspore::session::TrainSession *GetSession(const std::string &ms_file, bool train_mode) {\r\n",
        "+  // create model file\r\n",
        "+  mindspore::lite::Context context;\r\n",
        "+  context.device_list_[0].device_info_.cpu_device_info_.cpu_bind_mode_ = mindspore::lite::NO_BIND;\r\n",
        "+  context.thread_num_ = 1;\r\n",
        "+  return mindspore::session::TrainSession::CreateSession(ms_file, &context, train_mode);\r\n",
        "+}\r\n",
        "+\r\n",
        "+// net training function\r\n",
        "+int fl_lenet_lite_Inference(const std::string &ms_file, int batch_num, int test_nums) {\r\n",
        "+  auto session = GetSession(ms_file, false);\r\n",
        "+  char *origin_input[] = {fl_lenet_I0, fl_lenet_I1};\r\n",
        "+  float accuracy = 0.0;\r\n",
        "+  session->Eval();\r\n",
        "+  auto inputs = session->GetInputs();\r\n",
        "+  if (inputs[1]->shape().size() != 2) {\r\n",
        "+    return mindspore::lite::RET_ERROR;\r\n",
        "+  }\r\n",
        "+  auto batch_size = inputs[1]->shape()[0];\r\n",
        "+  auto num_of_class = inputs[1]->shape()[1];\r\n",
        "+  for (int j = 0; j < test_nums; ++j) {\r\n",
        "+    auto labels = FillInputData(session, batch_num, true);\r\n",
        "+    session->RunGraph();\r\n",
        "+    auto outputsv = SearchOutputsForSize(session, batch_size * num_of_class);\r\n",
        "+    auto scores = reinterpret_cast<float *>(outputsv->MutableData());\r\n",
        "+    for (int b = 0; b < batch_size; b++) {\r\n",
        "+      int max_idx = 0;\r\n",
        "+      float max_score = scores[num_of_class * b];\r\n",
        "+      for (int c = 0; c < num_of_class; c++) {\r\n",
        "+        if (scores[num_of_class * b + c] > max_score) {\r\n",
        "+          max_score = scores[num_of_class * b + c];\r\n",
        "+          max_idx = c;\r\n",
        "+        }\r\n",
        "+      }\r\n",
        "+      if (labels[b] == max_idx) accuracy += 1.0;\r\n",
        "+    }\r\n",
        "+  }\r\n",
        "+  fl_lenet_I0 = origin_input[0];\r\n",
        "+  fl_lenet_I1 = origin_input[1];\r\n",
        "+  accuracy /= static_cast<float>(batch_size * test_nums);\r\n",
        "+  MS_LOG(INFO) << \"accuracy  is \" << accuracy;\r\n",
        "+  return mindspore::lite::RET_OK;\r\n",
        "+}\r\n",
        "+\r\n",
        "+// net training function\r\n",
        "+int fl_lenet_lite_Train(const std::string &ms_file, const int batch_num, const int iterations) {\r\n",
        "+  auto session = GetSession(ms_file, true);\r\n",
        "+  if (iterations <= 0) {\r\n",
        "+    MS_LOG(ERROR) << \"error iterations or epoch!, epoch:\"\r\n",
        "+                 << \", iterations\" << iterations;\r\n",
        "+    return mindspore::lite::RET_ERROR;\r\n",
        "+  }\r\n",
        "+  MS_LOG(INFO) << \"total iterations :\" << iterations << \"batch_num:\" << batch_num;\r\n",
        "+  char *origin_input[] = {fl_lenet_I0, fl_lenet_I1};\r\n",
        "+  float min_loss = 1000.;\r\n",
        "+  for (int j = 0; j < iterations; ++j) {\r\n",
        "+    FillInputData(session, batch_num, false);\r\n",
        "+    session->RunGraph(nullptr, nullptr);\r\n",
        "+    float loss = GetLoss(session);\r\n",
        "+    if (min_loss > loss) min_loss = loss;\r\n",
        "+    if (j % 50 == 0) {\r\n",
        "+      MS_LOG(INFO) << \"iteration:\" << j << \",Loss is\" << loss << \" [min=\" << min_loss << \"]\";\r\n",
        "+    }\r\n",
        "+  }\r\n",
        "+  session->SaveToFile(ms_file);\r\n",
        "+  fl_lenet_I0 = origin_input[0];\r\n",
        "+  fl_lenet_I1 = origin_input[1];\r\n",
        "+  return mindspore::lite::RET_OK;\r\n",
        "+}\r\n",
        "+\r\n",
        "+int fl_lenet_lite_UpdateFeatures(const std::string &update_ms_file, TrainFeatureParam *new_features, int size) {\r\n",
        "+  auto train_session = GetSession(update_ms_file, false);\r\n",
        "+  auto status = train_session->UpdateFeatureMaps(update_ms_file, new_features, size);\r\n",
        "+  if (status != mindspore::lite::RET_OK) {\r\n",
        "+    MS_LOG(ERROR) << \"update model feature map failed\" << update_ms_file;\r\n",
        "+  }\r\n",
        "+  delete train_session;\r\n",
        "+  return status;\r\n",
        "+}\r\n",
        "+\r\n",
        "+int fl_lenet_lite_GetFeatures(const std::string &update_ms_file, mindspore::session::TrainFeatureParam ***feature,\r\n",
        "+                              int *size) {\r\n",
        "+  auto train_session = GetSession(update_ms_file, false);\r\n",
        "+  std::vector<mindspore::session::TrainFeatureParam *> new_features;\r\n",
        "+  auto status = train_session->GetFeatureMaps(&new_features);\r\n",
        "+  if (status != mindspore::lite::RET_OK) {\r\n",
        "+    MS_LOG(ERROR) << \"get model feature map failed\" << update_ms_file;\r\n",
        "+    delete train_session;\r\n",
        "+    return mindspore::lite::RET_ERROR;\r\n",
        "+  }\r\n",
        "+  *feature = new (std::nothrow) TrainFeatureParam *[new_features.size()];\r\n",
        "+  if (*feature == nullptr) {\r\n",
        "+    MS_LOG(ERROR) << \"create features failed\";\r\n",
        "+    delete train_session;\r\n",
        "+    return mindspore::lite::RET_ERROR;\r\n",
        "+  }\r\n",
        "+  for (int i = 0; i < new_features.size(); i++) {\r\n",
        "+    (*feature)[i] = new_features[i];\r\n",
        "+  }\r\n",
        "+  *size = new_features.size();\r\n",
        "+  delete train_session;\r\n",
        "+  return mindspore::lite::RET_OK;\r\n",
        "+}\r\n",
        "+\r\n",
        "+std::string RealPath(const char *path) {\r\n",
        "+  if (path == nullptr) {\r\n",
        "+    MS_LOG(ERROR) << \"path is nullptr\";\r\n",
        "+    return \"\";\r\n",
        "+  }\r\n",
        "+  if ((strlen(path)) >= PATH_MAX) {\r\n",
        "+    MS_LOG(ERROR) << \"path is too long\";\r\n",
        "+    return \"\";\r\n",
        "+  }\r\n",
        "+  auto resolved_path = std::make_unique<char[]>(PATH_MAX);\r\n",
        "+  if (resolved_path == nullptr) {\r\n",
        "+    MS_LOG(ERROR) << \"new resolved_path failed\";\r\n",
        "+    return \"\";\r\n",
        "+  }\r\n",
        "+#ifdef _WIN32\r\n",
        "+  char *real_path = _fullpath(resolved_path.get(), path, 1024);\r\n",
        "+#else\r\n",
        "+  char *real_path = realpath(path, resolved_path.get());\r\n",
        "+#endif\r\n",
        "+  if (real_path == nullptr || strlen(real_path) == 0) {\r\n",
        "+    MS_LOG(ERROR) << \"file path is not valid : \" << path;\r\n",
        "+    return \"\";\r\n",
        "+  }\r\n",
        "+  std::string res = resolved_path.get();\r\n",
        "+  return res;\r\n",
        "+}\r\n",
        "+\r\n",
        "+char *ReadFile(const char *file, size_t *size) {\r\n",
        "+  if (file == nullptr) {\r\n",
        "+    MS_LOG(ERROR) << \"file is nullptr\";\r\n",
        "+    return nullptr;\r\n",
        "+  }\r\n",
        "+  //  MS_ASSERT(size != nullptr);\r\n",
        "+  std::string real_path = RealPath(file);\r\n",
        "+  std::ifstream ifs(real_path);\r\n",
        "+  if (!ifs.good()) {\r\n",
        "+    MS_LOG(ERROR) << \"file: \" << real_path << \" is not exist\";\r\n",
        "+    return nullptr;\r\n",
        "+  }\r\n",
        "+\r\n",
        "+  if (!ifs.is_open()) {\r\n",
        "+    MS_LOG(ERROR) << \"file: \" << real_path << \" open failed\";\r\n",
        "+    return nullptr;\r\n",
        "+  }\r\n",
        "+\r\n",
        "+  ifs.seekg(0, std::ios::end);\r\n",
        "+  *size = ifs.tellg();\r\n",
        "+  std::unique_ptr<char[]> buf(new (std::nothrow) char[*size]);\r\n",
        "+  if (buf == nullptr) {\r\n",
        "+    MS_LOG(ERROR) << \"malloc buf failed, file: \" << real_path;\r\n",
        "+    ifs.close();\r\n",
        "+    return nullptr;\r\n",
        "+  }\r\n",
        "+  ifs.seekg(0, std::ios::beg);\r\n",
        "+  ifs.read(buf.get(), *size);\r\n",
        "+  ifs.close();\r\n",
        "+\r\n",
        "+  return buf.release();\r\n",
        "+}\r\n",
        "+\r\n",
        "+// Set input tensors.\r\n",
        "+int fl_lenet_lite_SetInputs(const std::string &files, int num) {\r\n",
        "+  std::vector<std::string> res;\r\n",
        "+  if (files.empty()) {\r\n",
        "+    MS_LOG(ERROR) << \"files empty\";\r\n",
        "+    return -1;\r\n",
        "+  }\r\n",
        "+  std::string pattern = \",\";\r\n",
        "+  std::string strs = files + pattern;\r\n",
        "+  size_t pos = strs.find(pattern);\r\n",
        "+  while (pos != strs.npos) {\r\n",
        "+    std::string temp = strs.substr(0, pos);\r\n",
        "+    res.push_back(temp);\r\n",
        "+    strs = strs.substr(pos + 1, strs.size());\r\n",
        "+    pos = strs.find(pattern);\r\n",
        "+  }\r\n",
        "+  if (res.size() != 2) {\r\n",
        "+    MS_LOG(ERROR) << \"res size not equal 2\";\r\n",
        "+    return -1;\r\n",
        "+  }\r\n",
        "+  for (int i = 0; i < 2; i++) {\r\n",
        "+    size_t size;\r\n",
        "+    char *bin_buf = ReadFile(res[i].c_str(), &size);\r\n",
        "+    if (bin_buf == nullptr) {\r\n",
        "+      MS_LOG(ERROR) << \"ReadFile return nullptr\";\r\n",
        "+      return mindspore::lite::RET_ERROR;\r\n",
        "+    }\r\n",
        "+    if (i == 0) {\r\n",
        "+      fl_lenet_I0 = bin_buf;\r\n",
        "+    }\r\n",
        "+    if (i == 1) {\r\n",
        "+      fl_lenet_I1 = bin_buf;\r\n",
        "+    }\r\n",
        "+  }\r\n",
        "+\r\n",
        "+  return 0;\r\n",
        "+}\r\n",
        "\\ No newline at end of file\r\n",
        "diff --git a/mindspore/lite/flclient/src/test/java/com/huawei/flclient/test/TestFLClient.java b/mindspore/lite/flclient/src/test/java/com/huawei/flclient/test/TestFLClient.java\r\n",
        "index 1986e20..fa6ff63 100644\r\n",
        "--- a/mindspore/lite/flclient/src/test/java/com/huawei/flclient/test/TestFLClient.java\r\n",
        "+++ b/mindspore/lite/flclient/src/test/java/com/huawei/flclient/test/TestFLClient.java\r\n",
        "@@ -17,7 +17,7 @@\r\n",
        " package com.huawei.flclient.test;\r\n",
        " \r\n",
        " import com.huawei.flclient.EncryptLevel;\r\n",
        "-import com.huawei.flclient.FLClient;\r\n",
        "+import com.huawei.flclient.FLLiteClient;\r\n",
        " \r\n",
        " public class TestFLClient {\r\n",
        " \r\n",
        "@@ -34,7 +34,7 @@ public class TestFLClient {\r\n",
        "         EncryptLevel encryptLevel = EncryptLevel.valueOf(args[9]);\r\n",
        "         boolean use_elb = Boolean.parseBoolean(args[10]);\r\n",
        "         int serverNum = Integer.parseInt(args[11]);\r\n",
        "-        FLClient client = new FLClient(ip, port, fl_name, fl_id, ifAsync, encryptLevel, use_elb, serverNum, train_batch_num, test_dataset, test_batch_num);\r\n",
        "+        FLLiteClient client = new FLLiteClient(ip, port, fl_name, fl_id, ifAsync, encryptLevel, use_elb, serverNum, train_batch_num, test_dataset, test_batch_num);\r\n",
        "         client.initRuntimeResource();\r\n",
        " \r\n",
        " //        String train_dataset = \"/home/user1/gitdir/master/mindspore/lite/flclient/src/main/resources/data_mindir_case1_2/data_bin_new/f0049_32/f0049_32_train_data.bin,\" +\r\n",
        "@@ -62,7 +62,7 @@ public class TestFLClient {\r\n",
        "      * @param test_dataset,    the binary files for evaluate model preference, eg: \"./test_data.bin,test_label.bin\"\r\n",
        "      * @param test_batch_num,  which is equal to the test_dataset size divided by batch_size\r\n",
        "      */\r\n",
        "-    private static void syncFLJobTest(FLClient client, String train_dateset, int epoch, int train_batch_num,\r\n",
        "+    private static void syncFLJobTest(FLLiteClient client, String train_dateset, int epoch, int train_batch_num,\r\n",
        "                                       String test_dataset, int test_batch_num) {\r\n",
        "         int iterations = 100;\r\n",
        "         client.setInput(train_dateset, train_batch_num);\r\n",
        "@@ -82,7 +82,7 @@ public class TestFLClient {\r\n",
        "      * @param test_dataset,    the binary files for evaluate model preference\r\n",
        "      * @param test_batch_num,  which is equal to the test_dataset size divided by batch_size\r\n",
        "      */\r\n",
        "-    private static void asyncFLJobTest(FLClient client, String train_dateset, int epoch, int train_batch_num,\r\n",
        "+    private static void asyncFLJobTest(FLLiteClient client, String train_dateset, int epoch, int train_batch_num,\r\n",
        "                                        String test_dataset, int test_batch_num) {\r\n",
        "         client.setInput(train_dateset, train_batch_num);\r\n",
        "         client.startFLJob();\r\n",
        "diff --git a/mindspore/lite/include/train_session.h b/mindspore/lite/include/train_session.h\r\n",
        "index 0a7faf4..d3ce3bf 100644\r\n",
        "--- a/mindspore/lite/include/train_session.h\r\n",
        "+++ b/mindspore/lite/include/train_session.h\r\n",
        "@@ -24,7 +24,14 @@\r\n",
        " namespace mindspore {\r\n",
        " namespace session {\r\n",
        " \r\n",
        "-/// \\brief TrainSession Defines a class that allows training a MindSpore model\r\n",
        "+struct TrainFeatureParam{\r\n",
        "+  char* name;\r\n",
        "+  void *data;\r\n",
        "+  size_t elenums;\r\n",
        "+  enum TypeId type;\r\n",
        "+};\r\n",
        "+/// \\brief TrainSession De\r\n",
        "+/// fines a class that allows training a MindSpore model\r\n",
        " class TrainSession : public session::LiteSession {\r\n",
        "  public:\r\n",
        "   /// \\brief Class destructor\r\n",
        "@@ -83,6 +90,10 @@ class TrainSession : public session::LiteSession {\r\n",
        "   /// \\return boolean indication if model is in eval mode\r\n",
        "   bool IsEval() { return train_mode_ == false; }\r\n",
        " \r\n",
        "+  virtual int GetFeatureMaps(std::vector<mindspore::session::TrainFeatureParam *>* feature_maps) =0;\r\n",
        "+\r\n",
        "+  virtual int UpdateFeatureMaps(const std::string &update_ms_file,\r\n",
        "+                                TrainFeatureParam* new_features,int size) =0;\r\n",
        "  protected:\r\n",
        "   bool train_mode_ = false;\r\n",
        " };\r\n",
        "diff --git a/mindspore/lite/src/train/train_session.cc b/mindspore/lite/src/train/train_session.cc\r\n",
        "index 494b417..44e8cb9 100644\r\n",
        "--- a/mindspore/lite/src/train/train_session.cc\r\n",
        "+++ b/mindspore/lite/src/train/train_session.cc\r\n",
        "@@ -23,6 +23,7 @@\r\n",
        " #include <iostream>\r\n",
        " #include <fstream>\r\n",
        " #include <memory>\r\n",
        "+#include <cstring>\r\n",
        " #include \"include/errorcode.h\"\r\n",
        " #include \"src/common/utils.h\"\r\n",
        " #include \"src/tensor.h\"\r\n",
        "@@ -101,7 +102,6 @@ int TrainSession::CompileTrainGraph(mindspore::lite::TrainModel *model) {\r\n",
        "   }\r\n",
        "   orig_output_node_map_ = output_node_map_;\r\n",
        "   orig_output_tensor_map_ = output_tensor_map_;\r\n",
        "-\r\n",
        "   for (auto inTensor : inputs_) inTensor->MutableData();\r\n",
        "   RestoreOps(restore);\r\n",
        "   CompileTrainKernels();      // Prepare a list of train kernels\r\n",
        "@@ -331,6 +331,63 @@ bool TrainSession::IsMaskOutput(kernel::LiteKernel *kernel) const {\r\n",
        "   return (IsOptimizer(kernel) || (kernel->Type() == schema::PrimitiveType_Assign));\r\n",
        " }\r\n",
        " \r\n",
        "+int lite::TrainSession::GetFeatureMaps(std::vector<mindspore::session::TrainFeatureParam *> *feature_maps) {\r\n",
        "+  for (auto tensor : this->tensors_) {\r\n",
        "+    if (tensor->IsConst()) {\r\n",
        "+      auto param = new mindspore::session::TrainFeatureParam();\r\n",
        "+      int len = tensor->tensor_name().length();\r\n",
        "+      char* name = nullptr;\r\n",
        "+      if(len>0) {\r\n",
        "+        name = new char[len+1];\r\n",
        "+        memcpy(name, tensor->tensor_name().c_str(), len);\r\n",
        "+        name[len] = 0;\r\n",
        "+      }\r\n",
        "+      param->name =  name;\r\n",
        "+      param->data = new float[tensor->ElementsNum()];\r\n",
        "+      memcpy(param->data, tensor->data_c(), tensor->ElementsNum()*sizeof(float));\r\n",
        "+      param->data = tensor->data_c();\r\n",
        "+      param->elenums = tensor->ElementsNum();\r\n",
        "+      param->type = tensor->data_type();\r\n",
        "+      feature_maps->push_back(param);\r\n",
        "+    }\r\n",
        "+  }\r\n",
        "+  MS_LOG(INFO) << \"get feature map success\";\r\n",
        "+  return RET_OK;\r\n",
        "+}\r\n",
        "+int lite::TrainSession::UpdateFeatureMaps(const std::string &update_ms_file,\r\n",
        "+                                                                mindspore::session::TrainFeatureParam* new_features,int size) {\r\n",
        "+  std::vector<mindspore::session::TrainFeatureParam *> old_features;\r\n",
        "+  auto status = GetFeatureMaps(&old_features);\r\n",
        "+  if (status != RET_OK) {\r\n",
        "+    MS_LOG(ERROR) << \"get features map failed:\";\r\n",
        "+  }\r\n",
        "+  for (int i=0;i<size;++i) {\r\n",
        "+    mindspore::session::TrainFeatureParam* new_feature = new_features + i;\r\n",
        "+    bool find = false;\r\n",
        "+    for (auto old_feature : old_features) {\r\n",
        "+      if (strcmp(old_feature->name, new_feature->name) == 0) {\r\n",
        "+        if(old_feature->elenums != new_feature->elenums) {\r\n",
        "+          MS_LOG(ERROR) << \"feature name:\"<<old_feature->name<<\",len diff:\"<<\"old is:\"<<old_feature->elenums<<\"new is:\"<<new_feature->elenums;\r\n",
        "+          return RET_ERROR;\r\n",
        "+        }\r\n",
        "+        find = true;\r\n",
        "+        memcpy(old_feature->data, new_feature->data, new_feature->elenums*sizeof(float));\r\n",
        "+        break;\r\n",
        "+      }\r\n",
        "+    }\r\n",
        "+    if (!find) {\r\n",
        "+      MS_LOG(ERROR) << \"cannot find feature:\" << new_feature->name;\r\n",
        "+      return RET_ERROR;\r\n",
        "+    }\r\n",
        "+  }\r\n",
        "+  SaveToFile(update_ms_file);\r\n",
        "+  for (auto feature : old_features) {\r\n",
        "+    delete feature;\r\n",
        "+  }\r\n",
        "+  MS_LOG(INFO) << \"update model:\" << update_ms_file << \",feature map success\";\r\n",
        "+  return RET_OK;\r\n",
        "+}\r\n",
        "+\r\n",
        " }  // namespace lite\r\n",
        " \r\n",
        " session::TrainSession *session::TrainSession::CreateSession(const char *model_buf, size_t size, lite::Context *context,\r\n",
        "diff --git a/mindspore/lite/src/train/train_session.h b/mindspore/lite/src/train/train_session.h\r\n",
        "index fdaabef..1cefe51 100644\r\n",
        "--- a/mindspore/lite/src/train/train_session.h\r\n",
        "+++ b/mindspore/lite/src/train/train_session.h\r\n",
        "@@ -78,6 +78,9 @@ class TrainSession : virtual public session::TrainSession, virtual public lite::\r\n",
        "   int Resize(const std::vector<tensor::MSTensor *> &inputs, const std::vector<std::vector<int>> &dims) override {\r\n",
        "     return lite::RET_ERROR;\r\n",
        "   }\r\n",
        "+  int GetFeatureMaps(std::vector<mindspore::session::TrainFeatureParam *> *feature_maps) override;\r\n",
        "+  int UpdateFeatureMaps(const std::string &update_ms_file,\r\n",
        "+                        mindspore::session::TrainFeatureParam* new_features,int size) override;\r\n",
        " \r\n",
        "  protected:\r\n",
        "   void AllocWorkSpace();\r\n",
        "-- \r\n",
        "2.7.4\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}