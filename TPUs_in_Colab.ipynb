{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-59qPLFPxva"
      },
      "source": [
        "From 132da0b92c04627cc307b0839d462a28dda4015d Mon Sep 17 00:00:00 2001\r\n",
        "From: tx1103mark <zhengjun10@huawei.com>\r\n",
        "Date: Mon, 8 Feb 2021 20:15:25 +0800\r\n",
        "Subject: [PATCH] iod and tod convert union\r\n",
        "\r\n",
        "---\r\n",
        " mindspore/lite/src/ops/addn.cc                | 12 +---\r\n",
        " mindspore/lite/src/ops/conv2d.cc              | 14 ++---\r\n",
        " mindspore/lite/src/ops/pad.cc                 |  9 ---\r\n",
        " mindspore/lite/src/ops/primitive_c.cc         | 24 ++++---\r\n",
        " mindspore/lite/src/ops/primitive_c.h          | 12 +++-\r\n",
        " mindspore/lite/src/ops/tile.cc                | 62 +++++++++----------\r\n",
        " .../lite/tools/anf_exporter/anf_exporter.cc   | 43 ++++---------\r\n",
        " .../lite/tools/anf_exporter/anf_exporter.h    |  5 +-\r\n",
        " .../tools/anf_importer/import_from_mindir.cc  | 14 ++---\r\n",
        " mindspore/lite/tools/common/node_util.cc      | 22 +++----\r\n",
        " mindspore/lite/tools/common/node_util.h       |  2 +\r\n",
        " mindspore/lite/tools/converter/CMakeLists.txt |  6 --\r\n",
        " .../lite/tools/converter/anf_transform.cc     |  1 +\r\n",
        " mindspore/lite/tools/converter/converter.cc   |  2 +-\r\n",
        " .../graph/format_trans_pass.cc                | 50 +++++----------\r\n",
        " .../graph/trans_format_insert_pass.cc         |  7 ---\r\n",
        " .../tools/optimizer/graph/infershape_pass.cc  |  2 -\r\n",
        " .../optimizer/graph/mindir_adjust_pass.cc     |  2 +-\r\n",
        " .../optimizer/graph/mindir_adjust_pass.h      |  2 +\r\n",
        " .../graph/weight_format_hardcode_pass.cc      |  4 --\r\n",
        " .../graph/weight_format_transform_pass.cc     |  2 -\r\n",
        " 21 files changed, 113 insertions(+), 184 deletions(-)\r\n",
        "\r\n",
        "diff --git a/mindspore/lite/src/ops/addn.cc b/mindspore/lite/src/ops/addn.cc\r\n",
        "index 20bbd6eeb..5a0cbb4e9 100644\r\n",
        "--- a/mindspore/lite/src/ops/addn.cc\r\n",
        "+++ b/mindspore/lite/src/ops/addn.cc\r\n",
        "@@ -86,7 +86,7 @@ int AddN::InferShape(std::vector<Tensor *> inputs, std::vector<Tensor *> outputs\r\n",
        "   for (size_t i = 1; i < inputs.size(); ++i) {\r\n",
        "     if (inputs.at(i)->shape().size() > max_dims) {\r\n",
        "       max_dims = inputs.at(i)->shape().size();\r\n",
        "-      max_dims_idx = 0;\r\n",
        "+      max_dims_idx = i;\r\n",
        "     }\r\n",
        "   }\r\n",
        "   output->set_shape(inputs.at(max_dims_idx)->shape());\r\n",
        "@@ -113,16 +113,6 @@ int AddN::InferShape(std::vector<Tensor *> inputs, std::vector<Tensor *> outputs\r\n",
        "         max_dim = dim;\r\n",
        "       }\r\n",
        "     }\r\n",
        "-#ifndef SUPPORT_TRAIN\r\n",
        "-    for (size_t i = 0; i < inputs.size(); ++i) {\r\n",
        "-      size_t shift = max_dims - inputs.at(i)->shape().size();\r\n",
        "-      size_t dim = (i < shift) ? 1 : inputs.at(i)->shape().at(d);\r\n",
        "-      if ((dim != max_dim) && (dim != 1)) {\r\n",
        "-        MS_LOG(ERROR) << \"AddN inputs shape is not equal!\";\r\n",
        "-        return RET_INPUT_TENSOR_ERROR;\r\n",
        "-      }\r\n",
        "-    }\r\n",
        "-#endif\r\n",
        "     output->shape()[d] = max_dim;  // set the biggest dimension in the output tensor\r\n",
        "   }\r\n",
        " \r\n",
        "diff --git a/mindspore/lite/src/ops/conv2d.cc b/mindspore/lite/src/ops/conv2d.cc\r\n",
        "index 5135891ce..c49358650 100644\r\n",
        "--- a/mindspore/lite/src/ops/conv2d.cc\r\n",
        "+++ b/mindspore/lite/src/ops/conv2d.cc\r\n",
        "@@ -149,13 +149,13 @@ void Conv2D::PopulaterConv2DMultiGroup(const Primitive &prim, schema::PrimitiveT\r\n",
        "   attr->padRight = pad_list.at(3);\r\n",
        " \r\n",
        "   auto dilation = CastToInt(prim.GetAttr(\"dilation\"));\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-  attr->dilateH = dilation.at(2);\r\n",
        "-  attr->dilateW = dilation.at(3);\r\n",
        "-#else\r\n",
        "-  attr->dilateH = dilation.at(0);\r\n",
        "-  attr->dilateW = dilation.at(1);\r\n",
        "-#endif\r\n",
        "+  if (train_flag()) {\r\n",
        "+    attr->dilateH = dilation.at(2);\r\n",
        "+    attr->dilateW = dilation.at(3);\r\n",
        "+  } else {\r\n",
        "+    attr->dilateH = dilation.at(0);\r\n",
        "+    attr->dilateW = dilation.at(1);\r\n",
        "+  }\r\n",
        "   auto kernel_size = CastToInt(prim.GetAttr(\"kernel_size\"));\r\n",
        "   attr->kernelH = kernel_size.at(0);\r\n",
        "   attr->kernelW = (kernel_size.size() > 1) ? kernel_size.at(1) : kernel_size.at(0);\r\n",
        "diff --git a/mindspore/lite/src/ops/pad.cc b/mindspore/lite/src/ops/pad.cc\r\n",
        "index 7aca1217b..951b9d4f5 100644\r\n",
        "--- a/mindspore/lite/src/ops/pad.cc\r\n",
        "+++ b/mindspore/lite/src/ops/pad.cc\r\n",
        "@@ -19,9 +19,6 @@\r\n",
        " #ifndef PRIMITIVE_WRITEABLE\r\n",
        " #include \"src/ops/ops_register.h\"\r\n",
        " #endif\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-#include <tuple>\r\n",
        "-#endif\r\n",
        " \r\n",
        " namespace mindspore {\r\n",
        " namespace lite {\r\n",
        "@@ -56,20 +53,16 @@ int Pad::UnPackAttr(const Primitive &prim, const std::vector<AnfNodePtr> &inputs\r\n",
        "     }\r\n",
        "     string paddingmode = \"REFLECT\";\r\n",
        "     if (prim.GetAttr(\"mode\") == nullptr) {\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "       if (prim.name() == \"Pad\") {\r\n",
        "         paddingmode = \"CONSTANT\";\r\n",
        "       } else {\r\n",
        "-#endif\r\n",
        "         MS_LOG(ERROR) << \"get mode failed!\";\r\n",
        "         delete this->primitive_;\r\n",
        "         delete attr;\r\n",
        "         this->primitive_ = nullptr;\r\n",
        "         attr = nullptr;\r\n",
        "         return RET_ERROR;\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "       }\r\n",
        "-#endif\r\n",
        "     } else {\r\n",
        "       paddingmode = GetValue<string>(prim.GetAttr(\"mode\"));\r\n",
        "     }\r\n",
        "@@ -77,7 +70,6 @@ int Pad::UnPackAttr(const Primitive &prim, const std::vector<AnfNodePtr> &inputs\r\n",
        "       attr->paddingMode = schema::PaddingMode_REFLECT;\r\n",
        "     } else if (paddingmode == \"SYMMETRIC\") {\r\n",
        "       attr->paddingMode = schema::PaddingMode_SYMMETRIC;\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "     } else if (paddingmode == \"CONSTANT\") {\r\n",
        "       attr->paddingMode = schema::PaddingMode_CONSTANT;\r\n",
        "       if (prim.GetAttr(\"paddings\") != nullptr) {\r\n",
        "@@ -91,7 +83,6 @@ int Pad::UnPackAttr(const Primitive &prim, const std::vector<AnfNodePtr> &inputs\r\n",
        "           attr->paddings.push_back(i);\r\n",
        "         }\r\n",
        "       }\r\n",
        "-#endif\r\n",
        "     } else {\r\n",
        "       MS_LOG(ERROR) << \"model type not supported!\";\r\n",
        "       delete this->primitive_;\r\n",
        "diff --git a/mindspore/lite/src/ops/primitive_c.cc b/mindspore/lite/src/ops/primitive_c.cc\r\n",
        "index aaa875064..e7ba78b2a 100644\r\n",
        "--- a/mindspore/lite/src/ops/primitive_c.cc\r\n",
        "+++ b/mindspore/lite/src/ops/primitive_c.cc\r\n",
        "@@ -173,7 +173,6 @@\r\n",
        " #include \"src/ops/is_finite.h\"\r\n",
        " #include \"src/ops/batch_matmul.h\"\r\n",
        " \r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        " #include \"src/ops/neg_grad.h\"\r\n",
        " #include \"src/ops/activation_grad.h\"\r\n",
        " #include \"src/ops/apply_momentum.h\"\r\n",
        "@@ -206,7 +205,6 @@\r\n",
        " #include \"src/ops/sigmoid_cross_entropy_with_logits_grad.h\"\r\n",
        " #include \"src/ops/strided_slice_grad.h\"\r\n",
        " #endif\r\n",
        "-#endif\r\n",
        " namespace mindspore {\r\n",
        " namespace lite {\r\n",
        " #ifdef PRIMITIVE_WRITEABLE\r\n",
        "@@ -509,13 +507,14 @@ std::shared_ptr<PrimitiveC> GetTupleGetItemPrim() {\r\n",
        " \r\n",
        " template <typename T, typename = std::enable_if<std::is_base_of<PrimitiveC, T>::value>>\r\n",
        " std::shared_ptr<PrimitiveC> NewPrimitiveC(const mindspore::Primitive &prim, const std::vector<AnfNodePtr> &inputs,\r\n",
        "-                                          const schema::QuantType &quantType) {\r\n",
        "+                                          const schema::QuantType &quantType,bool train_flag=false) {\r\n",
        "   auto primc = std::make_shared<T>();\r\n",
        "   if (primc == nullptr) {\r\n",
        "     MS_LOG(ERROR) << \"make_shared PrimitiveC failed\";\r\n",
        "     return nullptr;\r\n",
        "   }\r\n",
        "   primc->set_quant_type(quantType);\r\n",
        "+  primc->set_train_flag(train_flag);\r\n",
        "   auto ret = primc->UnPackAttr(prim, inputs);\r\n",
        "   if (ret != RET_OK) {\r\n",
        "     MS_LOG(ERROR) << \"UnPackAttr failed\";\r\n",
        "@@ -525,7 +524,7 @@ std::shared_ptr<PrimitiveC> NewPrimitiveC(const mindspore::Primitive &prim, cons\r\n",
        " }\r\n",
        " \r\n",
        " std::shared_ptr<PrimitiveC> PrimitiveC::Create(const Primitive &prim, const std::vector<AnfNodePtr> &inputs,\r\n",
        "-                                               const schema::QuantType &quantType) {\r\n",
        "+                                               const schema::QuantType &quantType,bool train_flag) {\r\n",
        "   const auto &op_type = prim.name();\r\n",
        "   if (op_type == \"ReLU\" || op_type == \"ReLU6\" || op_type == \"Sigmoid\" || op_type == \"HSwish\" || op_type == \"HSigmoid\") {\r\n",
        "     return NewPrimitiveC<Activation>(prim, inputs, quantType);\r\n",
        "@@ -538,7 +537,7 @@ std::shared_ptr<PrimitiveC> PrimitiveC::Create(const Primitive &prim, const std:\r\n",
        "   } else if (op_type == \"Concat\") {\r\n",
        "     return NewPrimitiveC<Concat>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"Conv2D\") {\r\n",
        "-    return NewPrimitiveC<Conv2D>(prim, inputs, quantType);\r\n",
        "+    return NewPrimitiveC<Conv2D>(prim, inputs, quantType,train_flag);\r\n",
        "   } else if (op_type == \"DepthwiseConv2dNative\" || op_type == \"DepthwiseConv2D\") {\r\n",
        "     return NewPrimitiveC<DepthwiseConv2D>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"Dequant\") {\r\n",
        "@@ -646,7 +645,7 @@ std::shared_ptr<PrimitiveC> PrimitiveC::Create(const Primitive &prim, const std:\r\n",
        "   } else if (op_type == \"Range\") {\r\n",
        "     return NewPrimitiveC<Range>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"Tile\") {\r\n",
        "-    return NewPrimitiveC<Tile>(prim, inputs, quantType);\r\n",
        "+    return NewPrimitiveC<Tile>(prim, inputs, quantType,train_flag);\r\n",
        "   } else if (op_type == \"GatherNd\") {\r\n",
        "     return NewPrimitiveC<GatherNd>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"Square\") {\r\n",
        "@@ -667,7 +666,6 @@ std::shared_ptr<PrimitiveC> PrimitiveC::Create(const Primitive &prim, const std:\r\n",
        "     return NewPrimitiveC<ArgMax>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"Gelu\") {\r\n",
        "     return NewPrimitiveC<GeLU>(prim, inputs, quantType);\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "   } else if (op_type == \"SoftmaxCrossEntropyWithLogits\") {\r\n",
        "     return NewPrimitiveC<SoftmaxCrossEntropy>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"SparseSoftmaxCrossEntropyWithLogits\") {\r\n",
        "@@ -688,7 +686,7 @@ std::shared_ptr<PrimitiveC> PrimitiveC::Create(const Primitive &prim, const std:\r\n",
        "     return NewPrimitiveC<PoolingGrad>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"Conv2DBackpropFilter\") {\r\n",
        "     return NewPrimitiveC<Conv2DGradFilter>(prim, inputs, quantType);\r\n",
        "-  } else if (op_type == \"Conv2DBackpropInput\") {\r\n",
        "+  } else if (op_type == \"Conv2DBackpropInput\" && train_flag) {\r\n",
        "     return NewPrimitiveC<Conv2DGradInput>(prim, inputs, quantType);\r\n",
        "   } else if ((op_type == \"BatchNormGrad\") || (op_type == \"FusedBatchNormGradEx\")) {\r\n",
        "     return NewPrimitiveC<BNGrad>(prim, inputs, quantType);\r\n",
        "@@ -728,10 +726,8 @@ std::shared_ptr<PrimitiveC> PrimitiveC::Create(const Primitive &prim, const std:\r\n",
        "     return NewPrimitiveC<Pad>(prim, inputs, quantType);\r\n",
        "   } else if (op_type == \"StridedSliceGrad\") {\r\n",
        "     return NewPrimitiveC<StridedSliceGrad>(prim, inputs, quantType);\r\n",
        "-#else\r\n",
        "-  } else if (op_type == \"Conv2DBackpropInput\") {\r\n",
        "+  } else if (op_type == \"Conv2DBackpropInput\" && !train_flag) {\r\n",
        "     return NewPrimitiveC<DeConv2D>(prim, inputs, quantType);\r\n",
        "-#endif\r\n",
        "   } else {\r\n",
        "     MS_LOG(ERROR) << \"Unsupported primitive type in Create : \" << op_type;\r\n",
        "     return nullptr;\r\n",
        "@@ -1039,7 +1035,6 @@ PrimitiveC *PrimitiveC::Create(mindspore::schema::PrimitiveT *primitive) {\r\n",
        "       return new (std::nothrow) IsFinite(primitive);\r\n",
        "     case schema::PrimitiveType_BatchMatMul:\r\n",
        "       return new (std::nothrow) BatchMatMul(primitive);\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "     case schema::PrimitiveType_ActivationGrad:\r\n",
        "       return new (std::nothrow) ActivationGrad(primitive);\r\n",
        "     case schema::PrimitiveType_PoolingGrad:\r\n",
        "@@ -1112,7 +1107,6 @@ PrimitiveC *PrimitiveC::Create(mindspore::schema::PrimitiveT *primitive) {\r\n",
        "       return new (std::nothrow) SigmoidCrossEntropyWithLogitsGrad(primitive);\r\n",
        "     case schema::PrimitiveType_StridedSliceGrad:\r\n",
        "       return new (std::nothrow) StridedSliceGrad(primitive);\r\n",
        "-#endif\r\n",
        "     default:\r\n",
        "       MS_LOG(ERROR) << \"Unsupported primitive type in Create : \" << schema::EnumNamePrimitiveType(op_type);\r\n",
        "       break;\r\n",
        "@@ -1142,6 +1136,10 @@ bool PrimitiveC::infer_flag() const { return this->infer_flag_; }\r\n",
        " \r\n",
        " void PrimitiveC::set_infer_flag(bool flag) { this->infer_flag_ = flag; }\r\n",
        " \r\n",
        "+bool PrimitiveC::train_flag() const { return this->train_flag_; }\r\n",
        "+\r\n",
        "+void PrimitiveC::set_train_flag(bool flag) { this->train_flag_ = flag; }\r\n",
        "+\r\n",
        " int PrimitiveC::InferShape(std::vector<lite::Tensor *> inputs, std::vector<lite::Tensor *> outputs) {\r\n",
        "   auto input = inputs.front();\r\n",
        "   MS_ASSERT(input != nullptr);\r\n",
        "diff --git a/mindspore/lite/src/ops/primitive_c.h b/mindspore/lite/src/ops/primitive_c.h\r\n",
        "index f972607ab..c311ca79c 100644\r\n",
        "--- a/mindspore/lite/src/ops/primitive_c.h\r\n",
        "+++ b/mindspore/lite/src/ops/primitive_c.h\r\n",
        "@@ -133,6 +133,10 @@ class PrimitiveC : public mindspore::Primitive {\r\n",
        " \r\n",
        "   void set_infer_flag(bool flag);\r\n",
        " \r\n",
        "+  bool train_flag() const;\r\n",
        "+\r\n",
        "+  void set_train_flag(bool flag);\r\n",
        "+\r\n",
        "   static PrimitiveC *Create(mindspore::schema::Primitive *primitive) { return Create(primitive->UnPack()); }\r\n",
        " \r\n",
        "   static PrimitiveC *Create(mindspore::schema::PrimitiveT *primitive);\r\n",
        "@@ -140,7 +144,7 @@ class PrimitiveC : public mindspore::Primitive {\r\n",
        "   static void GetAttrDataFromInput(const AnfNodePtr &inputNode, std::vector<int> *data);\r\n",
        " \r\n",
        "   static std::shared_ptr<PrimitiveC> Create(const Primitive &prim, const std::vector<AnfNodePtr> &inputs,\r\n",
        "-                                            const schema::QuantType &quantType);\r\n",
        "+                                            const schema::QuantType &quantType, bool train_flag=false);\r\n",
        "   void PopulaterQuantParam(const Primitive &prim, const std::vector<AnfNodePtr> &inputs);\r\n",
        "   void FillDefaultInputQuantParamIfNeed(const size_t &inputSize);\r\n",
        "   void PopulaterInputQuantParam(const Primitive &prim, const std::vector<AnfNodePtr> &inputs,\r\n",
        "@@ -159,6 +163,7 @@ class PrimitiveC : public mindspore::Primitive {\r\n",
        "   bool infer_flag_ = true;\r\n",
        "   int op_type_ = OP_TYPE_NOT_SET;\r\n",
        "   bool enable_huffman_code_ = false;\r\n",
        "+  bool train_flag_ = false;\r\n",
        " };\r\n",
        " std::shared_ptr<PrimitiveC> GetReturnPrim();\r\n",
        " \r\n",
        "@@ -179,6 +184,10 @@ class PrimitiveC {\r\n",
        " \r\n",
        "   void set_infer_flag(bool flag);\r\n",
        " \r\n",
        "+  bool train_flag() const;\r\n",
        "+\r\n",
        "+  void set_train_flag(bool flag);\r\n",
        "+\r\n",
        "   virtual int InferShape(std::vector<lite::Tensor *> inputs, std::vector<lite::Tensor *> outputs);\r\n",
        " \r\n",
        "   int Type() const;\r\n",
        "@@ -238,6 +247,7 @@ class PrimitiveC {\r\n",
        "   bool infer_flag_ = true;\r\n",
        "   schema::QuantType quant_type_{schema::QuantType_QUANT_NONE};\r\n",
        "   int op_type_ = OP_TYPE_NOT_SET;\r\n",
        "+   bool train_flag_ = false;\r\n",
        " };\r\n",
        " using PrimitiveCPtr = std::shared_ptr<PrimitiveC>;\r\n",
        " typedef PrimitiveC *(*PrimitiveCCreator)(const schema::Primitive *primitive);\r\n",
        "diff --git a/mindspore/lite/src/ops/tile.cc b/mindspore/lite/src/ops/tile.cc\r\n",
        "index 4c27c6a2e..2b6a784a6 100644\r\n",
        "--- a/mindspore/lite/src/ops/tile.cc\r\n",
        "+++ b/mindspore/lite/src/ops/tile.cc\r\n",
        "@@ -159,41 +159,41 @@ int Tile::InferShape(std::vector<Tensor *> inputs_, std::vector<Tensor *> output\r\n",
        "   } else {\r\n",
        "     multiples = GetMultiples();\r\n",
        "   }\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-  const size_t in_dims = input->shape().size();\r\n",
        "-  const size_t delta_dims = in_dims - multiples.size();\r\n",
        "-\r\n",
        "-  size_t i = 0;\r\n",
        "-  for (; i < delta_dims; ++i) {\r\n",
        "-    int tmp = input->shape().at(i);\r\n",
        "-    out_shape.push_back(tmp);\r\n",
        "-  }\r\n",
        "-  for (; i < in_dims; ++i) {\r\n",
        "-    int tmp = input->shape().at(i) * (multiples[i - delta_dims]);\r\n",
        "-    out_shape.push_back(tmp);\r\n",
        "-  }\r\n",
        "-#else\r\n",
        "-  std::vector<int> dims = GetDims();\r\n",
        "-  if (inputs_.size() == 2 && dims.empty()) {\r\n",
        "-    for (int dim = 0; dim < inputs_[1]->ElementsNum(); ++dim) {\r\n",
        "-      dims.push_back(dim);\r\n",
        "+  if (train_flag()) {\r\n",
        "+    const size_t in_dims = input->shape().size();\r\n",
        "+    const size_t delta_dims = in_dims - multiples.size();\r\n",
        "+\r\n",
        "+    size_t i = 0;\r\n",
        "+    for (; i < delta_dims; ++i) {\r\n",
        "+      int tmp = input->shape().at(i);\r\n",
        "+      out_shape.push_back(tmp);\r\n",
        "     }\r\n",
        "-  }\r\n",
        "-  const size_t in_dims = input->shape().size();\r\n",
        "+    for (; i < in_dims; ++i) {\r\n",
        "+      int tmp = input->shape().at(i) * (multiples[i - delta_dims]);\r\n",
        "+      out_shape.push_back(tmp);\r\n",
        "+    }\r\n",
        "+  } else {\r\n",
        "+    std::vector<int> dims = GetDims();\r\n",
        "+    if (inputs_.size() == 2 && dims.empty()) {\r\n",
        "+      for (int dim = 0; dim < inputs_[1]->ElementsNum(); ++dim) {\r\n",
        "+        dims.push_back(dim);\r\n",
        "+      }\r\n",
        "+    }\r\n",
        "+    const size_t in_dims = input->shape().size();\r\n",
        " \r\n",
        "-  MS_ASSERT(multiples.size() == dims.size());\r\n",
        "-  for (size_t i = 0; i < in_dims; ++i) {\r\n",
        "-    out_shape.push_back(input->shape().at(i));\r\n",
        "-  }\r\n",
        "-  for (size_t i = 0; i < dims.size(); ++i) {\r\n",
        "-    if (input->shape().at(dims.at(i)) != 0 &&\r\n",
        "-        multiples.at(i) > std::numeric_limits<int>::max() / input->shape().at(dims.at(i))) {\r\n",
        "-      MS_LOG(ERROR) << \"The value of multiples[\" << i << \"] is too big\";\r\n",
        "-      return RET_ERROR;\r\n",
        "+    MS_ASSERT(multiples.size() == dims.size());\r\n",
        "+    for (size_t i = 0; i < in_dims; ++i) {\r\n",
        "+      out_shape.push_back(input->shape().at(i));\r\n",
        "+    }\r\n",
        "+    for (size_t i = 0; i < dims.size(); ++i) {\r\n",
        "+      if (input->shape().at(dims.at(i)) != 0 &&\r\n",
        "+          multiples.at(i) > std::numeric_limits<int>::max() / input->shape().at(dims.at(i))) {\r\n",
        "+        MS_LOG(ERROR) << \"The value of multiples[\" << i << \"] is too big\";\r\n",
        "+        return RET_ERROR;\r\n",
        "+      }\r\n",
        "+      out_shape.at(dims.at(i)) = input->shape().at(dims.at(i)) * (multiples.at(i));\r\n",
        "     }\r\n",
        "-    out_shape.at(dims.at(i)) = input->shape().at(dims.at(i)) * (multiples.at(i));\r\n",
        "   }\r\n",
        "-#endif\r\n",
        "   output->set_shape(out_shape);\r\n",
        "   return RET_OK;\r\n",
        " }\r\n",
        "diff --git a/mindspore/lite/tools/anf_exporter/anf_exporter.cc b/mindspore/lite/tools/anf_exporter/anf_exporter.cc\r\n",
        "index 465415887..70234508c 100644\r\n",
        "--- a/mindspore/lite/tools/anf_exporter/anf_exporter.cc\r\n",
        "+++ b/mindspore/lite/tools/anf_exporter/anf_exporter.cc\r\n",
        "@@ -340,8 +340,9 @@ int AnfExporter::ExportSubgraph(const FuncGraphPtr &func_graph, const std::uniqu\r\n",
        "   return RET_OK;\r\n",
        " }\r\n",
        " \r\n",
        "-schema::MetaGraphT *AnfExporter::Export(const FuncGraphPtr &func_graph, bool keep_graph, bool copy_primitive) {\r\n",
        "+schema::MetaGraphT *AnfExporter::Export(const FuncGraphPtr &func_graph, bool keep_graph, bool copy_primitive,bool train_flag) {\r\n",
        "   static int subgraph_index = 0;\r\n",
        "+  this->train_flag = train_flag;\r\n",
        "   auto meta_graphT = std::make_unique<schema::MetaGraphT>();\r\n",
        "   int ret = ExportSubgraph(func_graph, meta_graphT, subgraph_index, keep_graph, copy_primitive);\r\n",
        "   if (ret != RET_OK) {\r\n",
        "@@ -355,24 +356,18 @@ int AnfExporter::ConvertInputCNode(const std::shared_ptr<AnfNode> &input_anode,\r\n",
        "   std::string input_name = input_anode->fullname_with_scope();\r\n",
        "   auto input_cnode = utils::cast<CNodePtr>(input_anode);\r\n",
        "   if (!IsPrimitiveCNode(input_cnode, schema::PrimitiveType_TupleGetItem)) {\r\n",
        "-#ifndef SUPPORT_TRAIN\r\n",
        "-    if (node_id_map_.find(input_name) != node_id_map_.end()) {\r\n",
        "-      output_cnode->inputIndex.emplace_back(node_id_map_[input_name]);\r\n",
        "-    }\r\n",
        "-#else\r\n",
        "     bool found = false;\r\n",
        "     if (node_id_map_.find(input_name) != node_id_map_.end()) {\r\n",
        "       output_cnode->inputIndex.emplace_back(node_id_map_[input_name]);\r\n",
        "       found = true;\r\n",
        "     }\r\n",
        " \r\n",
        "-    if (found == false) {\r\n",
        "+    if (!found) {\r\n",
        "       auto input_index_key = input_name + \"_o:\" + std::to_string(0);\r\n",
        "       if (node_id_map_.find(input_index_key) != node_id_map_.end()) {\r\n",
        "         output_cnode->inputIndex.emplace_back(node_id_map_[input_index_key]);\r\n",
        "       }\r\n",
        "     }\r\n",
        "-#endif\r\n",
        "   } else {\r\n",
        "     auto inputs = input_cnode->inputs();\r\n",
        " \r\n",
        "@@ -397,17 +392,14 @@ int AnfExporter::ConvertInputCNode(const std::shared_ptr<AnfNode> &input_anode,\r\n",
        "                                             : GetValue<int>(value_node->value()));\r\n",
        "     auto iter = node_id_map_.find(input_index_key);\r\n",
        "     if (iter == node_id_map_.end()) {\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "       input_index_key = get_item_input_cnode->fullname_with_scope() + \"_o:\" + std::to_string(0);  // try name with 0\r\n",
        "       iter = node_id_map_.find(input_index_key);\r\n",
        "       if (iter == node_id_map_.end()) {\r\n",
        "         MS_LOG(ERROR) << \"Can not find get_item output tensor \" << input_index_key;\r\n",
        "         return RET_ERROR;\r\n",
        "       }\r\n",
        "-#else\r\n",
        "       MS_LOG(ERROR) << \"Can not find get_item output tensor \" << input_index_key;\r\n",
        "       return RET_ERROR;\r\n",
        "-#endif\r\n",
        "     }\r\n",
        "     output_cnode->inputIndex.emplace_back(iter->second);\r\n",
        "   }\r\n",
        "@@ -487,9 +479,7 @@ int AnfExporter::ProcessTensor(const ValueNodePtr &valueNode, std::unique_ptr<sc\r\n",
        "   (void)std::transform(shape_vector.begin(), shape_vector.end(), std::back_inserter(dims),\r\n",
        "                        [](const int64_t &value) { return static_cast<int32_t>(value); });\r\n",
        "   (*paramTensor)->dims = dims;\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-  if ((*paramTensor)->dims.size() == 0) (*paramTensor)->dims = {1};\r\n",
        "-#endif\r\n",
        "+  if (train_flag && (*paramTensor)->dims.size() == 0) (*paramTensor)->dims = {1};\r\n",
        "   (*paramTensor)->nodeType = schema::NodeType::NodeType_ValueNode;\r\n",
        "   auto data = value->cast<tensor::TensorPtr>();\r\n",
        "   (*paramTensor)->data.resize(data->Size());\r\n",
        "@@ -595,11 +585,11 @@ int AnfExporter::ProcessParamValueLite(const ValueNodePtr &valueNode, std::uniqu\r\n",
        "   (*paramTensor)->format = schema::Format(valueLite->format());\r\n",
        "   (*paramTensor)->dataType = valueLite->tensor_type();\r\n",
        "   (*paramTensor)->dims = valueLite->tensor_shape();\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-  if ((*paramTensor)->dims.size() == 0) {\r\n",
        "+\r\n",
        "+  if (train_flag && (*paramTensor)->dims.size() == 0) {\r\n",
        "     (*paramTensor)->dims = {1};\r\n",
        "   }\r\n",
        "-#endif\r\n",
        "+\r\n",
        "   ret = memcpy_s((*paramTensor)->data.data(), valueLite->tensor_size() * sizeof(uint8_t), valueLite->tensor_addr(),\r\n",
        "                  valueLite->tensor_size());\r\n",
        "   if (ret != EOK) {\r\n",
        "@@ -619,9 +609,7 @@ int AnfExporter::ConvertInputValueNode(const std::shared_ptr<AnfNode> &input_ano\r\n",
        "   auto paramTensor = std::make_unique<schema::TensorT>();\r\n",
        "   auto value = valueNode->value();\r\n",
        "   int ret = RET_OK;\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "   paramTensor->name = valueNode->fullname_with_scope();\r\n",
        "-#endif\r\n",
        "   if (value->isa<tensor::Tensor>()) {\r\n",
        "     ret = ProcessTensor(valueNode, &paramTensor, value, output_cnode, meta_graphT);\r\n",
        "   } else if (value->isa<mindspore::Int32Imm>() || value->isa<mindspore::Int64Imm>()) {\r\n",
        "@@ -713,15 +701,6 @@ void AnfExporter::SetOpOutputNode(const CNodePtr &cnode, const std::unique_ptr<s\r\n",
        "       }\r\n",
        "       msTensor->nodeType = schema::NodeType_CNode;\r\n",
        "       fb_node->outputIndex.emplace_back(meta_graphT->allTensors.size());\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-      std::string name = cnode_name + \"_o:\" + std::to_string(i);\r\n",
        "-      node_id_map_[name] = meta_graphT->allTensors.size();\r\n",
        "-      meta_graphT->allTensors.emplace_back(msTensor);\r\n",
        "-      if (IsPrimitiveCNode(cnode, schema::PrimitiveType_Conv2D) ||\r\n",
        "-          IsPrimitiveCNode(cnode, schema::PrimitiveType_DepthwiseConv2D) ||\r\n",
        "-          IsPrimitiveCNode(cnode, schema::PrimitiveType_Adam))\r\n",
        "-        break;\r\n",
        "-#else\r\n",
        "       if (elements.size() == 1) {\r\n",
        "         node_id_map_[cnode_name] = meta_graphT->allTensors.size();\r\n",
        "         msTensor->name = cnode_name;\r\n",
        "@@ -747,10 +726,10 @@ void AnfExporter::SetOpOutputNode(const CNodePtr &cnode, const std::unique_ptr<s\r\n",
        "       if (IsPrimitiveCNode(cnode, schema::PrimitiveType_Conv2D) ||\r\n",
        "           IsPrimitiveCNode(cnode, schema::PrimitiveType_DepthwiseConv2D) ||\r\n",
        "           IsPrimitiveCNode(cnode, schema::PrimitiveType_FusedBatchNorm) ||\r\n",
        "-          IsPrimitiveCNode(cnode, schema::PrimitiveType_LayerNorm)) {\r\n",
        "+          IsPrimitiveCNode(cnode, schema::PrimitiveType_LayerNorm) ||\r\n",
        "+          IsPrimitiveCNode(cnode, schema::PrimitiveType_Adam)) {\r\n",
        "         break;\r\n",
        "       }\r\n",
        "-#endif\r\n",
        "     }\r\n",
        "   } else {\r\n",
        "     auto ms_tensor = new (std::nothrow) schema::TensorT();\r\n",
        "@@ -843,8 +822,8 @@ CNodePtr AnfExporter::CreatePartialCnode(const FuncGraphPtr &fg, AnfNodePtr node\r\n",
        "   }\r\n",
        " }\r\n",
        " \r\n",
        "-schema::MetaGraphT *Export(const FuncGraphPtr &func_graph, bool keep_graph, bool copy_primitive) {\r\n",
        "+schema::MetaGraphT *Export(const FuncGraphPtr &func_graph, bool keep_graph, bool copy_primitive,bool train_flag) {\r\n",
        "   AnfExporter anf_exporter;\r\n",
        "-  return anf_exporter.Export(func_graph, keep_graph, copy_primitive);\r\n",
        "+  return anf_exporter.Export(func_graph, keep_graph, copy_primitive,train_flag);\r\n",
        " }\r\n",
        " }  // namespace mindspore::lite\r\n",
        "diff --git a/mindspore/lite/tools/anf_exporter/anf_exporter.h b/mindspore/lite/tools/anf_exporter/anf_exporter.h\r\n",
        "index 2a0ea78ce..32c07d11a 100644\r\n",
        "--- a/mindspore/lite/tools/anf_exporter/anf_exporter.h\r\n",
        "+++ b/mindspore/lite/tools/anf_exporter/anf_exporter.h\r\n",
        "@@ -35,7 +35,7 @@ class AnfExporter {\r\n",
        "  public:\r\n",
        "   AnfExporter() = default;\r\n",
        "   virtual ~AnfExporter() = default;\r\n",
        "-  schema::MetaGraphT *Export(const FuncGraphPtr &func_graph, bool keep_graph = false, bool copy_primitive = false);\r\n",
        "+  schema::MetaGraphT *Export(const FuncGraphPtr &func_graph, bool keep_graph = false, bool copy_primitive = false,bool train_flag = false);\r\n",
        "   void SetOpOutputNode(const CNodePtr &cnode, const std::unique_ptr<schema::MetaGraphT> &meta_graphT,\r\n",
        "                        schema::CNodeT *fb_node);\r\n",
        "   int SetOpInputNode(const CNodePtr &cnode, const std::unique_ptr<schema::MetaGraphT> &meta_graphT,\r\n",
        "@@ -90,11 +90,12 @@ class AnfExporter {\r\n",
        "   std::vector<schema::CNodeT *> graph_input_nodes_;\r\n",
        "   std::map<FuncGraphPtr, int> fg_subgraph_map;\r\n",
        "   uint32_t node_idx = 0;\r\n",
        "+  bool train_flag = false;\r\n",
        " };\r\n",
        " // by default, copy_primitive is false, which means that the MetaGraph and func_graph share the same schema::PrimitiveT.\r\n",
        " // but in PostQuantization, the func_graph need to transfer to MetaGraph first and do MetaGraph pass, which may modify\r\n",
        " // the schema::PrimitiveT and cause bug; If all the passes have been done in func_graph, every thing would be simple\r\n",
        " // and clear.\r\n",
        "-schema::MetaGraphT *Export(const FuncGraphPtr &func_graph, bool keep_graph = false, bool copy_primitive = false);\r\n",
        "+schema::MetaGraphT *Export(const FuncGraphPtr &func_graph, bool keep_graph = false, bool copy_primitive = false,bool train_flag = false);\r\n",
        " }  // namespace mindspore::lite\r\n",
        " #endif  // MINDSPORE_LITE_TOOLS_COMMON_ANF_EXPORTER_ANF_EXPORTER_H_\r\n",
        "diff --git a/mindspore/lite/tools/anf_importer/import_from_mindir.cc b/mindspore/lite/tools/anf_importer/import_from_mindir.cc\r\n",
        "index 63a01b9ca..84818bf06 100644\r\n",
        "--- a/mindspore/lite/tools/anf_importer/import_from_mindir.cc\r\n",
        "+++ b/mindspore/lite/tools/anf_importer/import_from_mindir.cc\r\n",
        "@@ -855,14 +855,14 @@ int AnfImporterFromMindir::ParseModelConfigureInfo(const onnx::ModelProto &model\r\n",
        " }\r\n",
        " \r\n",
        " int AnfImporterFromMindir::Import(const converter::Flags *flag) {\r\n",
        "-#if SUPPORT_TRAIN\r\n",
        "-  func_graph_ = LoadMindIR(flag->modelFile, true);\r\n",
        "-  if (func_graph_ != nullptr) {\r\n",
        "-    return RET_OK;\r\n",
        "-  } else {\r\n",
        "-    MS_LOG(ERROR) << \"Parse new mind_ir proto failed, Trying old onnx format\";\r\n",
        "+  if (flag->trainModel) {\r\n",
        "+    func_graph_ = LoadMindIR(flag->modelFile);\r\n",
        "+    if (func_graph_ != nullptr) {\r\n",
        "+      return RET_OK;\r\n",
        "+    } else {\r\n",
        "+      MS_LOG(ERROR) << \"Parse new mind_ir proto failed, Trying old onnx format\";\r\n",
        "+    }\r\n",
        "   }\r\n",
        "-#endif\r\n",
        "   onnx_model_ = ReadOnnxFromBinary(flag->modelFile);\r\n",
        "   if (onnx_model_ == nullptr) {\r\n",
        "     MS_LOG(DEBUG) << \"Parse model failed, which is not an old mindir model\";\r\n",
        "diff --git a/mindspore/lite/tools/common/node_util.cc b/mindspore/lite/tools/common/node_util.cc\r\n",
        "index 4739a8217..69cb0aefe 100644\r\n",
        "--- a/mindspore/lite/tools/common/node_util.cc\r\n",
        "+++ b/mindspore/lite/tools/common/node_util.cc\r\n",
        "@@ -25,7 +25,6 @@\r\n",
        " namespace mindspore {\r\n",
        " namespace lite {\r\n",
        " static const std::vector<schema::PrimitiveType> nhwcOpList = {\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "   schema::PrimitiveType_Conv2DGradFilter,\r\n",
        "   schema::PrimitiveType_Conv2DGradInput,\r\n",
        "   schema::PrimitiveType_GroupConv2DGradInput,\r\n",
        "@@ -35,7 +34,6 @@ static const std::vector<schema::PrimitiveType> nhwcOpList = {\r\n",
        "   schema::PrimitiveType_ApplyMomentum,\r\n",
        "   schema::PrimitiveType_Sgd,\r\n",
        "   schema::PrimitiveType_Adam,\r\n",
        "-#endif\r\n",
        "   schema::PrimitiveType_Conv2D,\r\n",
        "   schema::PrimitiveType_DeConv2D,\r\n",
        "   schema::PrimitiveType_DepthwiseConv2D,\r\n",
        "@@ -52,12 +50,17 @@ static const std::vector<schema::PrimitiveType> nhwcOpList = {\r\n",
        "   schema::PrimitiveType_TopK};\r\n",
        " \r\n",
        " static const std::vector<schema::PrimitiveType> nhwcOpAllInputList = {\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "   schema::PrimitiveType_PoolingGrad, schema::PrimitiveType_ActivationGrad, schema::PrimitiveType_Conv2DGradFilter,\r\n",
        "   schema::PrimitiveType_BNGrad\r\n",
        "-#endif\r\n",
        " };\r\n",
        " \r\n",
        "+// index {} mean all inputs need insert\r\n",
        "+static std::unordered_map<schema::PrimitiveType, std::vector<int>> extNhwcInsertIndex = {\r\n",
        "+  {schema::PrimitiveType_BNGrad, {0, 1}},\r\n",
        "+  {schema::PrimitiveType_ApplyMomentum, {3}},\r\n",
        "+  {schema::PrimitiveType_Sgd, {1}},\r\n",
        "+  {schema::PrimitiveType_Adam, {9}}};\r\n",
        "+\r\n",
        " static const std::vector<schema::PrimitiveType> fp32FullOpList = {\r\n",
        "   schema::PrimitiveType_Concat, schema::PrimitiveType_Add,\r\n",
        "   schema::PrimitiveType_Floor};  // fp32 ops support C4 and nhwc in fp32\r\n",
        "@@ -133,17 +136,10 @@ static const std::vector<schema::PrimitiveType> int8OpList = {schema::PrimitiveT\r\n",
        "                                                               schema::PrimitiveType_L2Norm};\r\n",
        " \r\n",
        " static const std::vector<schema::PrimitiveType> needInsertOpList = {\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-  schema::PrimitiveType_Eltwise,       schema::PrimitiveType_Activation,   schema::PrimitiveType_Concat,\r\n",
        "-  schema::PrimitiveType_Power,         schema::PrimitiveType_StridedSlice, schema::PrimitiveType_Split,\r\n",
        "-  schema::PrimitiveType_Crop,          schema::PrimitiveType_Mul,          schema::PrimitiveType_Add,\r\n",
        "-  schema::PrimitiveType_ActivationGrad\r\n",
        "-#else\r\n",
        "   schema::PrimitiveType_Eltwise, schema::PrimitiveType_Activation,   schema::PrimitiveType_Concat,\r\n",
        "   schema::PrimitiveType_Power,   schema::PrimitiveType_StridedSlice, schema::PrimitiveType_Add,\r\n",
        "   schema::PrimitiveType_Split,   schema::PrimitiveType_Slice,        schema::PrimitiveType_Crop,\r\n",
        "-  schema::PrimitiveType_Mul,     schema::PrimitiveType_Maximum\r\n",
        "-#endif\r\n",
        "+  schema::PrimitiveType_Mul,     schema::PrimitiveType_Maximum,schema::PrimitiveType_ActivationGrad\r\n",
        " };\r\n",
        " \r\n",
        " static const std::unordered_map<int, int> nc2NhAxisMap = {{0, 0}, {1, -1}, {2, 1}, {3, 2}};\r\n",
        "@@ -156,6 +152,8 @@ std::vector<schema::PrimitiveType> Getfp32FullOpList() { return fp32FullOpList;\r\n",
        " \r\n",
        " std::vector<schema::PrimitiveType> GetNhwcOpList() { return nhwcOpList; }\r\n",
        " \r\n",
        "+std::unordered_map<schema::PrimitiveType, std::vector<int>> GetExtNhwcIndexes() { return extNhwcInsertIndex; }\r\n",
        "+\r\n",
        " std::vector<schema::PrimitiveType> GetNhwcAllInputOpList() { return nhwcOpAllInputList; }\r\n",
        " \r\n",
        " std::vector<schema::PrimitiveType> GetUint8NhwcOpList() { return int8NeedNhwcOpList; }\r\n",
        "diff --git a/mindspore/lite/tools/common/node_util.h b/mindspore/lite/tools/common/node_util.h\r\n",
        "index 7f8700197..fb50012e8 100644\r\n",
        "--- a/mindspore/lite/tools/common/node_util.h\r\n",
        "+++ b/mindspore/lite/tools/common/node_util.h\r\n",
        "@@ -62,6 +62,8 @@ std::vector<schema::PrimitiveType> GetNhwcOpList();\r\n",
        " \r\n",
        " std::vector<schema::PrimitiveType> GetNhwcAllInputOpList();\r\n",
        " \r\n",
        "+std::unordered_map<schema::PrimitiveType, std::vector<int>> GetExtNhwcIndexes();\r\n",
        "+\r\n",
        " std::vector<schema::PrimitiveType> Getfp32FullOpList();\r\n",
        " \r\n",
        " std::vector<schema::PrimitiveType> GetUint8NhwcOpList();\r\n",
        "diff --git a/mindspore/lite/tools/converter/CMakeLists.txt b/mindspore/lite/tools/converter/CMakeLists.txt\r\n",
        "index 3420b057c..79f30a493 100644\r\n",
        "--- a/mindspore/lite/tools/converter/CMakeLists.txt\r\n",
        "+++ b/mindspore/lite/tools/converter/CMakeLists.txt\r\n",
        "@@ -103,12 +103,6 @@ set(LITE_SRC\r\n",
        "         ${SRC_DIR}/dequant.cc\r\n",
        "         ${SRC_DIR}/huffman_decode.cc\r\n",
        "         )\r\n",
        "-if(SUPPORT_TRAIN)\r\n",
        "-    set(LITE_SRC\r\n",
        "-            ${LITE_SRC}\r\n",
        "-            )\r\n",
        "-\r\n",
        "-endif()\r\n",
        " set(ARM_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../../src/runtime/kernel/arm)\r\n",
        " file(GLOB KERNEL_SRC\r\n",
        "         ${ARM_DIR}/base/*.cc\r\n",
        "diff --git a/mindspore/lite/tools/converter/anf_transform.cc b/mindspore/lite/tools/converter/anf_transform.cc\r\n",
        "index 49e4156fd..431ff027f 100644\r\n",
        "--- a/mindspore/lite/tools/converter/anf_transform.cc\r\n",
        "+++ b/mindspore/lite/tools/converter/anf_transform.cc\r\n",
        "@@ -177,6 +177,7 @@ int AnfTransform::RunMindirAdjustPass(const FuncGraphPtr &old_graph, const conve\r\n",
        "   auto mindir_adjust_pass = std::make_shared<opt::MindirAdjustPass>();\r\n",
        "   mindir_adjust_pass->SetFmkType(config->fmk);\r\n",
        "   mindir_adjust_pass->SetQuantType(config->quantType);\r\n",
        "+  mindir_adjust_pass->SetTrainFlag(config->trainModel);\r\n",
        "   if (!mindir_adjust_pass->Run(old_graph)) {\r\n",
        "     MS_LOG(ERROR) << \"mindir adjust failed.\";\r\n",
        "     ReturnCode::GetSingleReturnCode()->UpdateReturnCode(RET_ERROR);\r\n",
        "diff --git a/mindspore/lite/tools/converter/converter.cc b/mindspore/lite/tools/converter/converter.cc\r\n",
        "index 09c9049bd..90df173a5 100644\r\n",
        "--- a/mindspore/lite/tools/converter/converter.cc\r\n",
        "+++ b/mindspore/lite/tools/converter/converter.cc\r\n",
        "@@ -88,7 +88,7 @@ MetaGraphT *Converter::Convert(const converter::Flags *flag) {\r\n",
        "   }\r\n",
        " \r\n",
        "   // anf -- fb\r\n",
        "-  auto meta_graph = Export(graph);\r\n",
        "+  auto meta_graph = Export(graph,false,false,flag->trainModel);\r\n",
        "   if (meta_graph == nullptr) {\r\n",
        "     MS_LOG(ERROR) << \"Export to meta graph return nullptr\";\r\n",
        "     return nullptr;\r\n",
        "diff --git a/mindspore/lite/tools/converter/legacy_optimizer/graph/format_trans_pass.cc b/mindspore/lite/tools/converter/legacy_optimizer/graph/format_trans_pass.cc\r\n",
        "index d9cbd4ab6..e491bce5e 100644\r\n",
        "--- a/mindspore/lite/tools/converter/legacy_optimizer/graph/format_trans_pass.cc\r\n",
        "+++ b/mindspore/lite/tools/converter/legacy_optimizer/graph/format_trans_pass.cc\r\n",
        "@@ -48,21 +48,7 @@ STATUS FormatTransPass::GetInsertFormatTrans(const schema::CNodeT &node, FormatT\r\n",
        "                                              FormatTransNodeType *afterNodeType) {\r\n",
        "   if (fmkType == converter::FmkType_TFLITE) {  // inference by nhwc\r\n",
        "     return RET_NO_CHANGE;\r\n",
        "-  } else if (fmkType == converter::FmkType_CAFFE) {  // inference by nchw\r\n",
        "-    if (!IsContain(GetNhwcOpList(), GetCNodeTType(node))) {\r\n",
        "-      return RET_NO_CHANGE;\r\n",
        "-    }\r\n",
        "-    *beforeNodeType = kNCHW2NHWC;\r\n",
        "-    *afterNodeType = kNHWC2NCHW;\r\n",
        "-    return RET_OK;\r\n",
        "-  } else if (fmkType == converter::FmkType_MS) {\r\n",
        "-    if (!IsContain(GetNhwcOpList(), GetCNodeTType(node))) {\r\n",
        "-      return RET_NO_CHANGE;\r\n",
        "-    }\r\n",
        "-    *beforeNodeType = kNCHW2NHWC;\r\n",
        "-    *afterNodeType = kNHWC2NCHW;\r\n",
        "-    return RET_OK;\r\n",
        "-  } else if (fmkType == converter::FmkType_ONNX) {\r\n",
        "+  } else if (fmkType == converter::FmkType_CAFFE || fmkType == converter::FmkType_MS || fmkType == converter::FmkType_ONNX) {\r\n",
        "     if (!IsContain(GetNhwcOpList(), GetCNodeTType(node))) {\r\n",
        "       return RET_NO_CHANGE;\r\n",
        "     }\r\n",
        "@@ -173,11 +159,18 @@ STATUS FormatTransPass::DoNodeInoutFormatTrans(schema::MetaGraphT *graph) {\r\n",
        "     if (node->primitive->value.type == schema::PrimitiveType_DepthToSpace) {\r\n",
        "       reinterpret_cast<schema::DepthToSpaceT *>(attr)->format = schema::Format_NHWC;\r\n",
        "     }\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-    if (IsContain(GetNhwcAllInputOpList(), GetCNodeTType(**iter))) {\r\n",
        "-      int idx_num = node->inputIndex.size();\r\n",
        "-      if (GetCNodeTType(**iter) == schema::PrimitiveType_BNGrad) idx_num = 2;\r\n",
        "-      for (int i = 0; i < idx_num; i++) {\r\n",
        "+    auto specInsertIndexes = GetExtNhwcIndexes();\r\n",
        "+    auto opType = GetCNodeTType(**iter);\r\n",
        "+    if (specInsertIndexes.find(opType) != specInsertIndexes.end()) {\r\n",
        "+      for (auto insert_index : specInsertIndexes[opType]) {\r\n",
        "+        iter = InsertFormatTransNode(graph, iter, kBefore, insert_index, beforeNodeType, &status);\r\n",
        "+        if (status != RET_OK) {\r\n",
        "+          MS_LOG(ERROR) << \"InsertNchw2NhwcNode before \" << nodeName << \"failed\";\r\n",
        "+          return RET_ERROR;\r\n",
        "+        }\r\n",
        "+      }\r\n",
        "+    } else if (IsContain(GetNhwcAllInputOpList(), opType)) {\r\n",
        "+      for (size_t i = 0; i < node->inputIndex.size(); i++) {\r\n",
        "         iter = InsertFormatTransNode(graph, iter, kBefore, i, beforeNodeType, &status);\r\n",
        "         if (status != RET_OK) {\r\n",
        "           MS_LOG(ERROR) << \"InsertNchw2NhwcNode before \" << nodeName << \"failed\";\r\n",
        "@@ -185,23 +178,8 @@ STATUS FormatTransPass::DoNodeInoutFormatTrans(schema::MetaGraphT *graph) {\r\n",
        "         }\r\n",
        "       }\r\n",
        "     } else {\r\n",
        "-      int idx = 0;\r\n",
        "-      if (GetCNodeTType(**iter) == schema::PrimitiveType_ApplyMomentum) idx = 3;\r\n",
        "-      if (GetCNodeTType(**iter) == schema::PrimitiveType_Sgd) idx = 1;\r\n",
        "-      if (GetCNodeTType(**iter) == schema::PrimitiveType_Adam) idx = 9;\r\n",
        "-      iter = InsertFormatTransNode(graph, iter, kBefore, idx, beforeNodeType, &status);\r\n",
        "-      if (status != RET_OK) {\r\n",
        "-        MS_LOG(ERROR) << \"InsertNhwc2NchwNode after \" << nodeName << \"failed\";\r\n",
        "-        return RET_ERROR;\r\n",
        "-      }\r\n",
        "-    }\r\n",
        "-#else\r\n",
        "-    iter = InsertFormatTransNode(graph, iter, kBefore, 0, beforeNodeType, &status);\r\n",
        "-    if (status != RET_OK) {\r\n",
        "-      MS_LOG(ERROR) << \"InsertNhwc2NchwNode after \" << nodeName << \"failed\";\r\n",
        "-      return RET_ERROR;\r\n",
        "+      iter = InsertFormatTransNode(graph, iter, kBefore, 0, beforeNodeType, &status);\r\n",
        "     }\r\n",
        "-#endif\r\n",
        "     iter = InsertFormatTransNode(graph, iter, kAfter, 0, afterNodeType, &status);\r\n",
        "     if (status != RET_OK) {\r\n",
        "       MS_LOG(ERROR) << \"InsertNhwc2NchwNode after \" << nodeName << \"failed\";\r\n",
        "diff --git a/mindspore/lite/tools/converter/legacy_optimizer/graph/trans_format_insert_pass.cc b/mindspore/lite/tools/converter/legacy_optimizer/graph/trans_format_insert_pass.cc\r\n",
        "index 263ee92a7..42e81f669 100644\r\n",
        "--- a/mindspore/lite/tools/converter/legacy_optimizer/graph/trans_format_insert_pass.cc\r\n",
        "+++ b/mindspore/lite/tools/converter/legacy_optimizer/graph/trans_format_insert_pass.cc\r\n",
        "@@ -200,13 +200,6 @@ STATUS TransOpInsertPass::Run(schema::MetaGraphT *graph) {\r\n",
        "       STATUS status = RET_OK;\r\n",
        "       auto input_tensor_size = (*iter)->inputIndex.size();\r\n",
        "       for (size_t i = 0; i < input_tensor_size; i++) {\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "-        auto &tensor = graph->allTensors.at((*iter)->inputIndex[i]);\r\n",
        "-        MS_ASSERT(tensor != nullptr);\r\n",
        "-        if (tensor->nodeType == schema::NodeType_ValueNode) {\r\n",
        "-          continue;\r\n",
        "-        }\r\n",
        "-#endif\r\n",
        "         auto &input_tensor = graph->allTensors.at((*iter)->inputIndex[i]);\r\n",
        "         if (input_tensor->nodeType == NodeType_ValueNode && input_tensor->dims.size() < 4) {\r\n",
        "           continue;\r\n",
        "diff --git a/mindspore/lite/tools/optimizer/graph/infershape_pass.cc b/mindspore/lite/tools/optimizer/graph/infershape_pass.cc\r\n",
        "index 8504626d1..c81bb1563 100644\r\n",
        "--- a/mindspore/lite/tools/optimizer/graph/infershape_pass.cc\r\n",
        "+++ b/mindspore/lite/tools/optimizer/graph/infershape_pass.cc\r\n",
        "@@ -365,9 +365,7 @@ bool InferShapePass::Run(const FuncGraphPtr &func_graph) {\r\n",
        "     auto type = GetCNodeType(cnode);\r\n",
        " \r\n",
        "     if ((type == schema::PrimitiveType_TupleGetItem) ||\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "         (type == schema::PrimitiveType_Depend) || (type == schema::PrimitiveType_ControlDepend) ||\r\n",
        "-#endif\r\n",
        "         (type == schema::PrimitiveType_MakeTuple || type == schema::PrimitiveType_Return)) {\r\n",
        "       continue;\r\n",
        "     }\r\n",
        "diff --git a/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.cc b/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.cc\r\n",
        "index 4db676082..fcc70cc98 100644\r\n",
        "--- a/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.cc\r\n",
        "+++ b/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.cc\r\n",
        "@@ -109,7 +109,7 @@ int MindirAdjustPass::PrimitiveConvert(std::shared_ptr<AnfNode> anf_node) {\r\n",
        "   auto inputs = cnode->inputs();\r\n",
        "   inputs.erase(inputs.begin());\r\n",
        "   if (!CheckPrimitiveType(anf_node, prim::kPrimReturn) && !CheckPrimitiveType(anf_node, prim::kPrimMakeTuple)) {\r\n",
        "-    auto primitive_c = PrimitiveC::Create(*primitive, inputs, quant_type_);\r\n",
        "+    auto primitive_c = PrimitiveC::Create(*primitive, inputs, quant_type_,train_flag_);\r\n",
        "     if (primitive_c == nullptr) {\r\n",
        "       MS_LOG(ERROR) << \"fail to create a primitive_c: \" << cnode->fullname_with_scope();\r\n",
        "       lite::NoSupportOp::GetInstance()->InsertOp(primitive->name());\r\n",
        "diff --git a/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.h b/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.h\r\n",
        "index 77ac864da..b928981a8 100644\r\n",
        "--- a/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.h\r\n",
        "+++ b/mindspore/lite/tools/optimizer/graph/mindir_adjust_pass.h\r\n",
        "@@ -32,6 +32,7 @@ class MindirAdjustPass : public Pass {\r\n",
        "   ~MindirAdjustPass() override = default;\r\n",
        "   void SetQuantType(QuantType quant_type) { quant_type_ = quant_type; }\r\n",
        "   void SetFmkType(FmkType fmk_type) { fmk_type_ = fmk_type; }\r\n",
        "+  void SetTrainFlag(bool train_flag) { train_flag_ = train_flag; }\r\n",
        "   int ParameterNodeConvert(AnfNodePtr anf_node);\r\n",
        "   int PrimitiveConvert(AnfNodePtr anf_node);\r\n",
        "   bool Run(const FuncGraphPtr &graph) override;\r\n",
        "@@ -39,6 +40,7 @@ class MindirAdjustPass : public Pass {\r\n",
        "  protected:\r\n",
        "   QuantType quant_type_ = QuantType::QuantType_QUANT_NONE;\r\n",
        "   FmkType fmk_type_ = FmkType::FmkType_MS;\r\n",
        "+  bool train_flag_ = false;\r\n",
        " };\r\n",
        " }  // namespace mindspore::opt\r\n",
        " #endif  // MINDSPORE_LITE_TOOLS_OPTIMIZER_GRAPH_MINDIR_ADJUST_PASS_H_\r\n",
        "diff --git a/mindspore/lite/tools/optimizer/graph/weight_format_hardcode_pass.cc b/mindspore/lite/tools/optimizer/graph/weight_format_hardcode_pass.cc\r\n",
        "index af8d1ad17..79952ee49 100644\r\n",
        "--- a/mindspore/lite/tools/optimizer/graph/weight_format_hardcode_pass.cc\r\n",
        "+++ b/mindspore/lite/tools/optimizer/graph/weight_format_hardcode_pass.cc\r\n",
        "@@ -131,12 +131,10 @@ lite::STATUS WeightFormatHardCodePass::HardCodeMS(const AnfNodePtr &conv_node,\r\n",
        "         param_value->set_format(schema::Format::Format_CKHW);\r\n",
        "       } else if (op_type == schema::PrimitiveType_DeConv2D) {\r\n",
        "         param_value->set_format(schema::Format::Format_KCHW);\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "       } else if (op_type == schema::PrimitiveType_Conv2DGradInput) {\r\n",
        "         param_value->set_format(schema::Format::Format_KCHW);\r\n",
        "       } else if (op_type == schema::PrimitiveType_GroupConv2DGradInput) {\r\n",
        "         param_value->set_format(schema::Format::Format_CKHW);\r\n",
        "-#endif\r\n",
        "       } else {\r\n",
        "         MS_LOG(ERROR) << \"Unsupported opType: \" << EnumNamePrimitiveType(op_type)\r\n",
        "                       << \", node: \" << conv_node->fullname_with_scope();\r\n",
        "@@ -213,10 +211,8 @@ bool WeightFormatHardCodePass::Run(const FuncGraphPtr &graph) {\r\n",
        "     auto conv_cnode = node->cast<CNodePtr>();\r\n",
        "     auto type = opt::GetCNodeType(node);\r\n",
        "     if (type != schema::PrimitiveType_Conv2D && type != schema::PrimitiveType_DepthwiseConv2D &&\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "         ((type != schema::PrimitiveType_Conv2DGradInput) || (fmk_type != FmkType_MS)) &&\r\n",
        "         ((type != schema::PrimitiveType_GroupConv2DGradInput) || (fmk_type != FmkType_MS)) &&\r\n",
        "-#endif\r\n",
        "         type != schema::PrimitiveType_DeConv2D && type != schema::PrimitiveType_DeDepthwiseConv2D) {\r\n",
        "       continue;\r\n",
        "     }\r\n",
        "diff --git a/mindspore/lite/tools/optimizer/graph/weight_format_transform_pass.cc b/mindspore/lite/tools/optimizer/graph/weight_format_transform_pass.cc\r\n",
        "index f63501901..9c3febe35 100644\r\n",
        "--- a/mindspore/lite/tools/optimizer/graph/weight_format_transform_pass.cc\r\n",
        "+++ b/mindspore/lite/tools/optimizer/graph/weight_format_transform_pass.cc\r\n",
        "@@ -44,9 +44,7 @@ lite::STATUS WeightFormatTransformPass::ConvWeightFormatTrans(const FuncGraphPtr\r\n",
        "     }\r\n",
        "     auto type = opt::GetCNodeType(node);\r\n",
        "     if (type != schema::PrimitiveType_Conv2D && type != schema::PrimitiveType_DepthwiseConv2D\r\n",
        "-#ifdef SUPPORT_TRAIN\r\n",
        "         && type != schema::PrimitiveType_Conv2DGradInput && type != schema::PrimitiveType_GroupConv2DGradInput\r\n",
        "-#endif\r\n",
        "         && type != schema::PrimitiveType_DeConv2D && type != schema::PrimitiveType_DeDepthwiseConv2D) {\r\n",
        "       continue;\r\n",
        "     }\r\n",
        "-- \r\n",
        "2.17.1\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}