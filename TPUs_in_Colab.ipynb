{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkdsmE8yFYWZ"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include <jni.h>\n",
        "#include <cstring>\n",
        "#include \"include/errorcode.h\"\n",
        "#include \"include/train/train_session.h\"\n",
        "#include \"util.h\"\n",
        "#include \"lenet_train.h\"\n",
        "#include \"bert_train.h\"\n",
        "#include \"src/common/log_adapter.h\"\n",
        "\n",
        "static jobject fbb;\n",
        "static jmethodID create_string_char;\n",
        "static jobject jmap;\n",
        "static jstring model_path;\n",
        "\n",
        "char *JstringToChar(JNIEnv *env, jstring jstr) {\n",
        "  char *rtn = nullptr;\n",
        "  jclass clsstring = env->FindClass(\"java/lang/String\");\n",
        "  jstring strencode = env->NewStringUTF(\"GB2312\");\n",
        "  jmethodID mid = env->GetMethodID(clsstring, \"getBytes\", \"(Ljava/lang/String;)[B\");\n",
        "  jbyteArray barr = (jbyteArray)env->CallObjectMethod(jstr, mid, strencode);\n",
        "  jsize alen = env->GetArrayLength(barr);\n",
        "  jbyte *ba = env->GetByteArrayElements(barr, JNI_FALSE);\n",
        "  if (alen > 0) {\n",
        "    rtn = new char[alen + 1];\n",
        "    memcpy(rtn, ba, alen);\n",
        "    rtn[alen] = 0;\n",
        "  }\n",
        "  env->ReleaseByteArrayElements(barr, ba, 0);\n",
        "  return rtn;\n",
        "}\n",
        "\n",
        "extern \"C\" jint CreateFeatureMap(JNIEnv *env, const char *name, float *data, size_t size) {\n",
        "  jstring name1 = env->NewStringUTF(name);\n",
        "  jint name_offset = env->CallIntMethod(fbb, create_string_char, name1);\n",
        "  // 1. set data size\n",
        "  jfloatArray ret = env->NewFloatArray(size);\n",
        "  env->SetFloatArrayRegion(ret, 0, size, data);\n",
        "  // 2. get methodid createDataVector\n",
        "  jclass fm_cls = env->FindClass(\"mindspore/schema/FeatureMap\");\n",
        "  jmethodID createDataVector =\n",
        "    env->GetStaticMethodID(fm_cls, \"createDataVector\", \"(Lcom/google/flatbuffers/FlatBufferBuilder;[F)I\");\n",
        "  // 3. calc data offset\n",
        "  jint data_offset = env->CallStaticIntMethod(fm_cls, createDataVector, fbb, ret);\n",
        "  jmethodID createFeatureMap =\n",
        "    env->GetStaticMethodID(fm_cls, \"createFeatureMap\", \"(Lcom/google/flatbuffers/FlatBufferBuilder;II)I\");\n",
        "  jint fm_offset = env->CallStaticIntMethod(fm_cls, createFeatureMap, fbb, name_offset, data_offset);\n",
        "  return fm_offset;\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_NativeTrain_train(JNIEnv *env, jobject thiz,\n",
        "                                                                             jlong session_ptr,\n",
        "                                                                             jint batch_size, jint epoches,\n",
        "                                                                             jint early_stop_type) {\n",
        "  std::string model_name = JstringToChar(env, model_path);\n",
        "  if(model_name.find(\"lenet\") != std::string::npos){\n",
        "    return TrainLenet(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr), JstringToChar(env, model_path),\n",
        "                      batch_size, epoches);\n",
        "  }\n",
        "  return TrainBert(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr), JstringToChar(env, model_path),\n",
        "                   batch_size, epoches);\n",
        "\n",
        "}\n",
        "\n",
        "extern \"C\" jlong JNICALL Java_com_huawei_flclient_NativeTrain_createSession(JNIEnv *env, jclass, jstring ms_file,\n",
        "                                                                            jlong) {\n",
        "  model_path = ms_file;\n",
        "  return reinterpret_cast<jlong>(CreateSession(JstringToChar(env, ms_file)));\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jobject JNICALL Java_com_huawei_flclient_NativeTrain_getFeaturesMap(JNIEnv *env, jclass,\n",
        "                                                                                         jlong session_ptr) {\n",
        "  jclass strClass = env->FindClass(\"java/lang/String\");\n",
        "  jmethodID ctorID = env->GetMethodID(strClass, \"<init>\", \"([BLjava/lang/String;)V\");\n",
        "  jstring encoding = env->NewStringUTF(\"GB2312\");\n",
        "\n",
        "  TrainFeatureParam **train_features = nullptr;\n",
        "  int feature_size = 0;\n",
        "  auto status =\n",
        "    GetFeatures(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr), &train_features, &feature_size);\n",
        "  if (status != mindspore::lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"get features failed:\";\n",
        "    return NULL;\n",
        "  }\n",
        "  jclass jmapClass = env->FindClass(\"java/util/HashMap\");\n",
        "  if (jmapClass == NULL) {\n",
        "    return NULL;\n",
        "  }\n",
        "  jmethodID mid = env->GetMethodID(jmapClass, \"<init>\", \"()V\");\n",
        "  jmethodID putMethod = env->GetMethodID(jmapClass, \"put\", \"(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;\");\n",
        "  jmethodID getMethod = env->GetMethodID(jmapClass, \"get\", \"(Ljava/lang/Object;)Ljava/lang/Object;\");\n",
        "  bool map_exist = true;\n",
        "  if (jmap == nullptr) {\n",
        "    jmap = env->NewGlobalRef(env->NewObject(jmapClass, mid, feature_size));\n",
        "    map_exist = false;\n",
        "  }\n",
        "  for (int i = 0; i < feature_size; i++) {\n",
        "    jbyteArray bytes = env->NewByteArray(strlen(train_features[i]->name));\n",
        "    env->SetByteArrayRegion(bytes, 0, strlen(train_features[i]->name), (jbyte *)train_features[i]->name);\n",
        "    auto key = (jstring)env->NewObject(strClass, ctorID, bytes, encoding);\n",
        "    jfloatArray feature_data;\n",
        "    if (map_exist) {\n",
        "      feature_data = static_cast<jfloatArray>(env->CallObjectMethod(jmap, getMethod, key));\n",
        "    } else {\n",
        "      feature_data = env->NewFloatArray(train_features[i]->elenums);\n",
        "    }\n",
        "    if (feature_data == nullptr) {\n",
        "      std::cout << \"create null feature data\" << std::endl;\n",
        "    }\n",
        "    jfloat *fd = env->GetFloatArrayElements(feature_data, NULL);\n",
        "    for (int j = 0; j < train_features[i]->elenums; j++) {\n",
        "      fd[j] = reinterpret_cast<float *>(train_features[i]->data)[j];\n",
        "    }\n",
        "    env->ReleaseFloatArrayElements(feature_data, fd, 0);\n",
        "    env->CallObjectMethod(jmap, putMethod, key, feature_data);\n",
        "    env->DeleteLocalRef(bytes);\n",
        "    env->DeleteLocalRef(key);\n",
        "  }\n",
        "  env->DeleteLocalRef(encoding);\n",
        "  for (int i = 0; i < feature_size; i++) {\n",
        "    delete train_features[i]->name;\n",
        "    free(train_features[i]->data);\n",
        "    delete train_features[i];\n",
        "  }\n",
        "  return jmap;\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jintArray JNICALL Java_com_huawei_flclient_NativeTrain_getSeralizeFeaturesMap(JNIEnv *env,\n",
        "                                                                                                   jobject thiz,\n",
        "                                                                                                   jlong session_ptr,\n",
        "                                                                                                   jobject builder) {\n",
        "  fbb = builder;\n",
        "  jclass fb_clazz = env->GetObjectClass(builder);\n",
        "  create_string_char = env->GetMethodID(fb_clazz, \"createString\", \"(Ljava/lang/CharSequence;)I\");\n",
        "  TrainFeatureParam **train_features = nullptr;\n",
        "  int feature_size = 0;\n",
        "  auto status =\n",
        "    GetFeatures(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr), &train_features, &feature_size);\n",
        "  if (status != mindspore::lite::RET_OK) {\n",
        "    MS_LOG(ERROR) << \"get features failed:\";\n",
        "    return env->NewIntArray(0);\n",
        "  }\n",
        "  jintArray ret = env->NewIntArray(feature_size);\n",
        "  jint *data = env->GetIntArrayElements(ret, NULL);\n",
        "\n",
        "  for (int i = 0; i < feature_size; i++) {\n",
        "    data[i] = CreateFeatureMap(env, train_features[i]->name, reinterpret_cast<float *>(train_features[i]->data),\n",
        "                               train_features[i]->elenums);\n",
        "    MS_LOG(INFO) << \"upload feature:\"\n",
        "                 << \", name:\" << train_features[i]->name << \", elenums:\" << train_features[i]->elenums;\n",
        "  }\n",
        "  env->ReleaseIntArrayElements(ret, data, 0);\n",
        "  for (int i = 0; i < feature_size; i++) {\n",
        "    delete train_features[i]->name;\n",
        "    free(train_features[i]->data);\n",
        "    delete train_features[i];\n",
        "  }\n",
        "  return ret;\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_NativeTrain_updateFeatures(JNIEnv *env, jclass,\n",
        "                                                                                      jlong session_ptr,\n",
        "                                                                                      jobject features) {\n",
        "  jclass arr_cls = env->GetObjectClass(features);\n",
        "  jmethodID size_method = env->GetMethodID(arr_cls, \"size\", \"()I\");\n",
        "  jmethodID get_method = env->GetMethodID(arr_cls, \"get\", \"(I)Ljava/lang/Object;\");\n",
        "\n",
        "  jclass fm_cls = env->FindClass(\"mindspore/schema/FeatureMap\");\n",
        "  jmethodID weight_name_method = env->GetMethodID(fm_cls, \"weightFullname\", \"()Ljava/lang/String;\");\n",
        "  jmethodID data_length_method = env->GetMethodID(fm_cls, \"dataLength\", \"()I\");\n",
        "  jmethodID data_method = env->GetMethodID(fm_cls, \"data\", \"(I)F\");\n",
        "  jclass clsstring = env->FindClass(\"java/lang/String\");\n",
        "  jmethodID mid = env->GetMethodID(clsstring, \"getBytes\", \"(Ljava/lang/String;)[B\");\n",
        "  int size = env->CallIntMethod(features, size_method);\n",
        "  // transform FeatureMap to TrainFeatureParm\n",
        "  TrainFeatureParam *features_param = reinterpret_cast<TrainFeatureParam *>(malloc(size * sizeof(TrainFeatureParam)));\n",
        "  for (int i = 0; i < size; ++i) {\n",
        "    TrainFeatureParam *param = features_param + i;\n",
        "    jobject feature = env->CallObjectMethod(features, get_method, i);\n",
        "    // set feature_param name\n",
        "    jstring weight_full_name = (jstring)env->CallObjectMethod(feature, weight_name_method);\n",
        "    jstring strencode = env->NewStringUTF(\"GB2312\");\n",
        "    jbyteArray barr = (jbyteArray)env->CallObjectMethod(weight_full_name, mid, strencode);\n",
        "    char *name = nullptr;\n",
        "    jsize alen = env->GetArrayLength(barr);\n",
        "    jbyte *ba = env->GetByteArrayElements(barr, JNI_FALSE);\n",
        "    if (alen > 0) {\n",
        "      name = new char[alen + 1];\n",
        "      if (ba == nullptr) {\n",
        "        MS_LOG(ERROR) << \"name is nullptr\";\n",
        "        return mindspore::lite::RET_ERROR;\n",
        "      }\n",
        "      memcpy(name, ba, alen);\n",
        "      name[alen] = 0;\n",
        "    }\n",
        "    param->name = name;\n",
        "    env->ReleaseByteArrayElements(barr, ba, 0);\n",
        "    int data_length = env->CallIntMethod(feature, data_length_method);\n",
        "    float *data = static_cast<float *>(malloc(data_length * sizeof(float)));\n",
        "    memset(data, 0, data_length * sizeof(float));\n",
        "    for (int j = 0; j < data_length; ++j) {\n",
        "      float *addr = data + j;\n",
        "      *addr = env->CallFloatMethod(feature, data_method, j);\n",
        "    }\n",
        "    param->data = data;\n",
        "    param->elenums = data_length;\n",
        "    param->type = mindspore::kNumberTypeFloat32;\n",
        "    MS_LOG(INFO) << \"get feature:\" << param->name << \",elenums:\" << param->elenums;\n",
        "  }\n",
        "  return UpdateFeatures(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr), JstringToChar(env, model_path),\n",
        "                        features_param, size);\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_NativeTrain_setInput(JNIEnv *env, jobject, jstring files) {\n",
        "  std::string input_files = JstringToChar(env, files);\n",
        "  std::string pattern = \",\";\n",
        "  std::string strs = input_files + pattern;\n",
        "  size_t pos = strs.find(pattern);\n",
        "  std::vector<std::string> res;\n",
        "  while (pos != strs.npos) {\n",
        "    std::string temp = strs.substr(0, pos);\n",
        "    res.push_back(temp);\n",
        "    strs = strs.substr(pos + 1, strs.size());\n",
        "    pos = strs.find(pattern);\n",
        "  }\n",
        "  if (res.size() == 2) {\n",
        "    return SetLenetInputs(res[0], res[1]);\n",
        "  } else if (res.size() == 3) {\n",
        "    return SetBertInputs(res[0], res[1], res[1]);\n",
        "  }\n",
        "  std::cout << \"input files error\" << std::endl;\n",
        "  return -1;\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jfloat JNICALL Java_com_huawei_flclient_NativeTrain_infer(JNIEnv *env, jclass, jlong session_ptr) {\n",
        "\n",
        "  std::string model_name = JstringToChar(env, model_path);\n",
        "  if(model_name.find(\"lenet\") != std::string::npos){\n",
        "    return InferLenet(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr));\n",
        "  }\n",
        "  return InferBert(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr));\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jintArray JNICALL Java_com_huawei_flclient_NativeTrain_getInferLables(JNIEnv *env, jclass, jlong session_ptr) {\n",
        "\n",
        "  std::string model_name = JstringToChar(env, model_path);\n",
        "  std::vector<int> infer_result;\n",
        "  if(model_name.find(\"lenet\") != std::string::npos){\n",
        "   infer_result = GetLenetInferRes(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr));\n",
        "  } else {\n",
        "    infer_result = GetBertInferRes(reinterpret_cast<mindspore::session::TrainSession *>(session_ptr));\n",
        "  }\n",
        "  jintArray jArray = env->NewIntArray(infer_result.size());\n",
        "  jint *jnum = new jint[infer_result.size()];\n",
        "  for(int i=0;i<infer_result.size();i++) {\n",
        "    *(jnum+i) = infer_result[i];\n",
        "  }\n",
        "  env->SetIntArrayRegion(jArray, 0, infer_result.size(), jnum);\n",
        "  return jArray;\n",
        "}\n",
        "\n",
        "extern \"C\" JNIEXPORT jint JNICALL Java_com_huawei_flclient_NativeTrain_free(JNIEnv *env, jclass, jlong session_ptr) {\n",
        "  env->DeleteGlobalRef(jmap);\n",
        "  jmap = NULL;\n",
        "  if (0 != session_ptr) {\n",
        "    delete (reinterpret_cast<mindspore::session::TrainSession *>(session_ptr));\n",
        "  }\n",
        "  std::string model_name = JstringToChar(env, model_path);\n",
        "  if(model_name.find(\"lenet\") != std::string::npos){\n",
        "    FreeLenetInput();\n",
        "  } else {\n",
        "    FreeBertInput();\n",
        "  }\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9oyK8xdFbi8"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#ifndef MSLITE_FL_LENET_TRAIN_H\n",
        "#define MSLITE_FL_LENET_TRAIN_H\n",
        "\n",
        "#include \"include/train/train_session.h\"\n",
        "#include <string>\n",
        "using mindspore::session::TrainSession;\n",
        "int SetLenetInputs(const std::string &input_data, const std::string &label_data);\n",
        "float InferLenet(TrainSession *session);\n",
        "void FreeLenetInput();\n",
        "std::vector<int> GetLenetInferRes(TrainSession *session);\n",
        "int TrainLenet(TrainSession *session,const std::string &save_path,int batch_size,int epoches);\n",
        "#endif  // MSLITE_FL_LENET_TRAIN_H\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIDCla3fF0vy"
      },
      "source": [
        "/**\n",
        " * Copyright 2021 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "#include \"lenet_train.h\"\n",
        "#include \"util.h\"\n",
        "#include <cstring>\n",
        "#include <fstream>\n",
        "#include <iostream>\n",
        "#include \"include/errorcode.h\"\n",
        "#include \"src/common/log_adapter.h\"\n",
        "#include <climits>\n",
        "\n",
        "static char *fl_lenet_I0 = 0;\n",
        "static char *fl_lenet_I1 = 0;\n",
        "static int input_size = 0;\n",
        "static int batch_num = 0;\n",
        "#define LENET_LABEL_CLASS 62\n",
        "std::vector<int> FillLenetInput(mindspore::session::TrainSession *train_session, int batch_idx,int train_mode=true) {\n",
        "  std::vector<int> labels_vec;\n",
        "  auto inputs = train_session->GetInputs();\n",
        "  int batch_size = inputs[0]->shape()[0];\n",
        "  int data_size = inputs[0]->ElementsNum() / batch_size;\n",
        "  int num_classes = inputs[1]->shape()[1];\n",
        "  float* input_data = reinterpret_cast<float *>(inputs.at(0)->MutableData());\n",
        "  auto labels = reinterpret_cast<float *>(inputs.at(1)->MutableData());\n",
        "  std::fill(labels, labels + inputs.at(1)->ElementsNum(), 0.f);\n",
        "  int label_idx = 0;\n",
        "  for (int i = 0; i < batch_size; i++) {\n",
        "    std::memcpy(input_data + i * data_size,\n",
        "                (float *)fl_lenet_I0 + +batch_idx * inputs[0]->ElementsNum() + i * data_size,\n",
        "                data_size * sizeof(float));\n",
        "    if(train_mode) {\n",
        "      label_idx = *(reinterpret_cast<int *>(fl_lenet_I1) + batch_idx * batch_size + i);\n",
        "      labels[i * num_classes + label_idx] = 1.0;  // Model expects labels in onehot representation\n",
        "      labels_vec.push_back(label_idx);\n",
        "    }\n",
        "  }\n",
        "  return labels_vec;\n",
        "}\n",
        "\n",
        "\n",
        "// net inference function\n",
        "float InferLenet(TrainSession *session) {\n",
        "  auto labels = FillLenetInput(session,0);\n",
        "  auto infer_acc = CalculateAccuracy(session,labels,LENET_LABEL_CLASS);\n",
        "  std::cout << \"inference acc is:\" << infer_acc << std::endl;\n",
        "  return infer_acc;\n",
        "}\n",
        "\n",
        "// net inference function\n",
        "std::vector<int> GetLenetInferRes(TrainSession *session) {\n",
        "  (void)FillLenetInput(session,0,false);\n",
        "  return GetInferResult(session,LENET_LABEL_CLASS);\n",
        "}\n",
        "\n",
        "// net training function\n",
        "int TrainLenet(TrainSession *session,const std::string &save_path,int batch_size,int epoches) {\n",
        "  if (epoches <= 0) {\n",
        "    MS_LOG(ERROR) << \"error iterations or epoch!, epoch:\"\n",
        "                  << \", iterations\" << epoches;\n",
        "    return mindspore::lite::RET_ERROR;\n",
        "  }\n",
        "  batch_num = input_size/(session->GetInputs()[0]->ElementsNum() * sizeof(float));\n",
        "  std::cout << \"total train epoches :\" << epoches << \",batch_num:\" << batch_num<<std::endl;\n",
        "  for (int j = 0; j < epoches; ++j) {\n",
        "    float sum_loss_per_epoch = 0.0f;\n",
        "    float sum_acc_per_epoch = 0.0f;\n",
        "    for(int k=0;k<batch_num;++k) {\n",
        "      auto lables = FillLenetInput(session,k);\n",
        "      session->Train();\n",
        "      session->RunGraph(nullptr, nullptr);\n",
        "      sum_loss_per_epoch+=GetLoss(session);\n",
        "      sum_acc_per_epoch += CalculateAccuracy(session,lables,LENET_LABEL_CLASS);\n",
        "    }\n",
        "    std::cout << \"epoch \" << \"[\" <<j<<\"]\" << \",mean Loss \" << sum_loss_per_epoch/batch_num <<\",train acc \"<<  sum_acc_per_epoch/batch_num<<std::endl;\n",
        "  }\n",
        "  session->SaveToFile(save_path);\n",
        "  return mindspore::lite::RET_OK;\n",
        "}\n",
        "\n",
        "std::string RealPath(const char *path) {\n",
        "  if (path == nullptr) {\n",
        "    MS_LOG(ERROR) << \"path is nullptr\";\n",
        "    return \"\";\n",
        "  }\n",
        "  if ((strlen(path)) >= PATH_MAX) {\n",
        "    MS_LOG(ERROR) << \"path is too long\";\n",
        "    return \"\";\n",
        "  }\n",
        "  auto resolved_path = std::make_unique<char[]>(PATH_MAX);\n",
        "  if (resolved_path == nullptr) {\n",
        "    MS_LOG(ERROR) << \"new resolved_path failed\";\n",
        "    return \"\";\n",
        "  }\n",
        "#ifdef _WIN32\n",
        "  char *real_path = _fullpath(resolved_path.get(), path, 1024);\n",
        "#else\n",
        "  char *real_path = realpath(path, resolved_path.get());\n",
        "#endif\n",
        "  if (real_path == nullptr || strlen(real_path) == 0) {\n",
        "    MS_LOG(ERROR) << \"file path is not valid : \" << path;\n",
        "    return \"\";\n",
        "  }\n",
        "  std::string res = resolved_path.get();\n",
        "  return res;\n",
        "}\n",
        "\n",
        "char *ReadFile(const char *file, size_t *size) {\n",
        "  if (file == nullptr) {\n",
        "    MS_LOG(ERROR) << \"file is nullptr\";\n",
        "    return nullptr;\n",
        "  }\n",
        "  //  MS_ASSERT(size != nullptr);\n",
        "  std::string real_path = RealPath(file);\n",
        "  std::ifstream ifs(real_path);\n",
        "  if (!ifs.good()) {\n",
        "    MS_LOG(ERROR) << \"file: \" << real_path << \" is not exist\";\n",
        "    return nullptr;\n",
        "  }\n",
        "\n",
        "  if (!ifs.is_open()) {\n",
        "    MS_LOG(ERROR) << \"file: \" << real_path << \" open failed\";\n",
        "    return nullptr;\n",
        "  }\n",
        "\n",
        "  ifs.seekg(0, std::ios::end);\n",
        "  *size = ifs.tellg();\n",
        "  std::unique_ptr<char[]> buf(new (std::nothrow) char[*size]);\n",
        "  if (buf == nullptr) {\n",
        "    MS_LOG(ERROR) << \"malloc buf failed, file: \" << real_path;\n",
        "    ifs.close();\n",
        "    return nullptr;\n",
        "  }\n",
        "  ifs.seekg(0, std::ios::beg);\n",
        "  ifs.read(buf.get(), *size);\n",
        "  ifs.close();\n",
        "\n",
        "  return buf.release();\n",
        "}\n",
        "\n",
        "// Set input tensors.\n",
        "int SetLenetInputs(const std::string &input_data, const std::string &label_data) {\n",
        "  size_t input0_size = 0;\n",
        "  char *bin_buf = ReadFile(input_data.c_str(), &input0_size);\n",
        "  if (bin_buf == nullptr) {\n",
        "    MS_LOG(ERROR) << \"ReadFile return nullptr\";\n",
        "    return -1;\n",
        "  }\n",
        "  fl_lenet_I0 = bin_buf;\n",
        "  size_t input1_size = 0;\n",
        "  bin_buf = ReadFile(label_data.c_str(), &input1_size);\n",
        "  if (bin_buf == nullptr) {\n",
        "    MS_LOG(ERROR) << \"ReadFile return nullptr\";\n",
        "    return -1;\n",
        "  }\n",
        "  fl_lenet_I1 = bin_buf;\n",
        "  input_size = input0_size;\n",
        "  return input0_size;\n",
        "}\n",
        "void FreeLenetInput() {\n",
        "  delete fl_lenet_I0;\n",
        "  delete fl_lenet_I1;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkBWEK8FF2ZB"
      },
      "source": [
        "cmake_minimum_required(VERSION 3.14)\n",
        "\n",
        "project(FederalLearning)\n",
        "\n",
        "option(SUPPORT_GPU \"if support gpu\" off)\n",
        "set(BUILD_LITE \"on\")\n",
        "set(SUPPORT_TRAIN \"on\")\n",
        "set(PLATFORM_ARM \"on\")\n",
        "\n",
        "set(TOP_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../../../../../../)\n",
        "set(LITE_DIR ${TOP_DIR}/mindspore/lite)\n",
        "set(MS_VERSION_MAJOR ${MS_VERSION_MAJOR})\n",
        "set(MS_VERSION_MINOR ${MS_VERSION_MINOR})\n",
        "set(MS_VERSION_REVISION ${MS_VERSION_REVISION})\n",
        "set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DMS_VERSION_MAJOR=${MS_VERSION_MAJOR} -DMS_VERSION_MINOR=${MS_VERSION_MINOR} \\\n",
        "  -DMS_VERSION_REVISION=${MS_VERSION_REVISION}\")\n",
        "set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DMS_VERSION_MAJOR=${MS_VERSION_MAJOR} -DMS_VERSION_MINOR=${MS_VERSION_MINOR} \\\n",
        "  -DMS_VERSION_REVISION=${MS_VERSION_REVISION}\")\n",
        "set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++17\")\n",
        "\n",
        "include_directories(${CMAKE_CURRENT_SOURCE_DIR})\n",
        "include_directories(${CMAKE_CURRENT_SOURCE_DIR}/linux)\n",
        "include_directories(${CMAKE_CURRENT_SOURCE_DIR}/dataset)\n",
        "\n",
        "\n",
        "include_directories(${LITE_DIR}) ## lite include\n",
        "include_directories(${TOP_DIR}) ## api include\n",
        "include_directories(${TOP_DIR}/mindspore/core/) ## core include\n",
        "include_directories(${LITE_DIR}/build) ## flatbuffers\n",
        "\n",
        "set(OP_SRC\n",
        "        lite_train_jni.cpp\n",
        "        util.cpp\n",
        "        bert_train.cpp\n",
        "        lenet_train.cpp\n",
        "        dataset/CustomizedTokenizer.cc\n",
        "            )\n",
        "find_library(log-lib glog)\n",
        "\n",
        "add_library(fl SHARED ${OP_SRC})\n",
        "\n",
        "link_directories(${CMAKE_CURRENT_SOURCE_DIR}/lib/)\n",
        "\n",
        "install(TARGETS fl LIBRARY DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/lib)\n",
        "add_executable(test test_train.cc ${OP_SRC} )\n",
        "target_link_libraries(test mindspore-lite  glog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO327t7WIdp5"
      },
      "source": [
        "//\n",
        "// Created by meng on 3/30/21.\n",
        "//\n",
        "#include \"CustomizedTokenizer.h\"\n",
        "\n",
        "CustomizedTokenizer::CustomizedTokenizer() = default;\n",
        "\n",
        "CustomizedTokenizer::~CustomizedTokenizer() {\n",
        "  _vocab.clear();\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::init(const string &vocab_file, bool do_lower_case) {\n",
        "  _do_lower_case = do_lower_case;\n",
        "  _load_vocab(vocab_file);\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::tokenize(const string &text, string output_tokens[MAX_SEQ_LENGTH], int &seq_length) {\n",
        "//  clock_t startTime;\n",
        "//  double time_cost = 0.0;\n",
        "  _text = text;\n",
        "  _split_text();\n",
        "\n",
        "  int output_tokens_pos = 0;\n",
        "\n",
        "//  startTime = clock();\n",
        "  for (int i = 0; i < _tokens_length; ++i) {\n",
        "    int length = _tokens[i].length();\n",
        "    if (length > _max_input_chars_per_word) {\n",
        "      output_tokens[output_tokens_pos] = \"[UNK]\";\n",
        "      output_tokens_pos++;\n",
        "      if (MAX_SEQ_LENGTH <= output_tokens_pos) {\n",
        "        _tokens_length = 0;\n",
        "        output_tokens[MAX_SEQ_LENGTH - 1] = \"[SEP]\";\n",
        "        return;\n",
        "      }\n",
        "      continue;\n",
        "    }\n",
        "\n",
        "    bool is_bad = false;\n",
        "    int start = 0;\n",
        "    vector<string> sub_tokens;\n",
        "    while (start < length) {\n",
        "      int end = length;\n",
        "      string cur_substr;\n",
        "      while (start < end) {\n",
        "        string substr = _tokens[i].substr(start, end - start);\n",
        "        if (start > 0) {\n",
        "          substr.insert(0,\"##\");\n",
        "        }\n",
        "        if (_vocab.find(substr) != _vocab.end()) {\n",
        "          cur_substr = substr;\n",
        "          break;\n",
        "        }\n",
        "        end--;\n",
        "      }\n",
        "      if (cur_substr.empty()) {\n",
        "        is_bad = true;\n",
        "        break;\n",
        "      }\n",
        "      sub_tokens.emplace_back(cur_substr);\n",
        "      start = end;\n",
        "    }\n",
        "    if (is_bad) {\n",
        "      output_tokens[output_tokens_pos] = \"[UNK]\";\n",
        "      output_tokens_pos++;\n",
        "      if (MAX_SEQ_LENGTH <= output_tokens_pos) {\n",
        "        _tokens_length = 0;\n",
        "        output_tokens[MAX_SEQ_LENGTH - 1] = \"[SEP]\";\n",
        "        return;\n",
        "      }\n",
        "    } else {\n",
        "      for (const string& sub_token: sub_tokens) {\n",
        "        output_tokens[output_tokens_pos] = sub_token;\n",
        "        output_tokens_pos++;\n",
        "        if (MAX_SEQ_LENGTH <= output_tokens_pos) {\n",
        "          _tokens_length = 0;\n",
        "          output_tokens[MAX_SEQ_LENGTH - 1] = \"[SEP]\";\n",
        "          return;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  output_tokens[output_tokens_pos] = \"[SEP]\";\n",
        "  for (int i = output_tokens_pos + 1; i < MAX_SEQ_LENGTH; ++i) {\n",
        "    output_tokens[i] = \"[PAD]\";\n",
        "  }\n",
        "//  time_cost += (double)(clock() - startTime) / CLOCKS_PER_SEC;\n",
        "  _tokens_length = 0;\n",
        "\n",
        "//  cout << \"Local run time is: \" << time_cost << endl;\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::tokenize(const string &text, int input_ids[MAX_SEQ_LENGTH],\n",
        "                                   int attention_mask[MAX_SEQ_LENGTH], int token_type_ids[MAX_SEQ_LENGTH],\n",
        "                                   int &seq_length) {\n",
        "//  clock_t startTime;\n",
        "//  double time_cost = 0.0;\n",
        "  _text = text;\n",
        "  _split_text();\n",
        "\n",
        "  int output_tokens_pos = 0;\n",
        "\n",
        "//  startTime = clock();\n",
        "  for (int i = 0; i < _tokens_length; ++i) {\n",
        "    int length = _tokens[i].length();\n",
        "    if (length > _max_input_chars_per_word) {\n",
        "      input_ids[output_tokens_pos] = _vocab[\"[UNK]\"];\n",
        "      attention_mask[output_tokens_pos] = 1;\n",
        "      token_type_ids[output_tokens_pos] = 0;\n",
        "      output_tokens_pos++;\n",
        "      if (MAX_SEQ_LENGTH <= output_tokens_pos) {\n",
        "        _tokens_length = 0;\n",
        "        input_ids[MAX_SEQ_LENGTH - 1] = _vocab[\"[SEP]\"];\n",
        "        attention_mask[MAX_SEQ_LENGTH - 1] = 1;\n",
        "        token_type_ids[MAX_SEQ_LENGTH - 1] = 0;\n",
        "        return;\n",
        "      }\n",
        "      continue;\n",
        "    }\n",
        "\n",
        "    bool is_bad = false;\n",
        "    int start = 0;\n",
        "    vector<string> sub_tokens;\n",
        "    while (start < length) {\n",
        "      int end = length;\n",
        "      string cur_substr;\n",
        "      while (start < end) {\n",
        "        string substr = _tokens[i].substr(start, end - start);\n",
        "        if (start > 0) {\n",
        "          substr.insert(0,\"##\");\n",
        "        }\n",
        "        if (_vocab.find(substr) != _vocab.end()) {\n",
        "          cur_substr = substr;\n",
        "          break;\n",
        "        }\n",
        "        end--;\n",
        "      }\n",
        "      if (cur_substr.empty()) {\n",
        "        is_bad = true;\n",
        "        break;\n",
        "      }\n",
        "      sub_tokens.emplace_back(cur_substr);\n",
        "      start = end;\n",
        "    }\n",
        "    if (is_bad) {\n",
        "      input_ids[output_tokens_pos] = _vocab[\"[UNK]\"];\n",
        "      attention_mask[output_tokens_pos] = 1;\n",
        "      token_type_ids[output_tokens_pos] = 0;\n",
        "      output_tokens_pos++;\n",
        "      if (MAX_SEQ_LENGTH <= output_tokens_pos) {\n",
        "        _tokens_length = 0;\n",
        "        input_ids[MAX_SEQ_LENGTH - 1] = _vocab[\"[SEP]\"];\n",
        "        attention_mask[MAX_SEQ_LENGTH - 1] = 1;\n",
        "        token_type_ids[MAX_SEQ_LENGTH - 1] = 0;\n",
        "        return;\n",
        "      }\n",
        "    } else {\n",
        "      for (const string& sub_token: sub_tokens) {\n",
        "        input_ids[output_tokens_pos] = _vocab[sub_token];\n",
        "        attention_mask[output_tokens_pos] = 1;\n",
        "        token_type_ids[output_tokens_pos] = 0;\n",
        "        output_tokens_pos++;\n",
        "        if (MAX_SEQ_LENGTH <= output_tokens_pos) {\n",
        "          _tokens_length = 0;\n",
        "          input_ids[MAX_SEQ_LENGTH - 1] = _vocab[\"[SEP]\"];\n",
        "          attention_mask[MAX_SEQ_LENGTH - 1] = 1;\n",
        "          token_type_ids[MAX_SEQ_LENGTH - 1] = 0;\n",
        "          return;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "//  time_cost += (double)(clock() - startTime) / CLOCKS_PER_SEC;\n",
        "  input_ids[output_tokens_pos] = _vocab[\"[SEP]\"];\n",
        "  attention_mask[output_tokens_pos] = 1;\n",
        "  token_type_ids[output_tokens_pos] = 0;\n",
        "  for (int i = output_tokens_pos + 1; i < MAX_SEQ_LENGTH; ++i) {\n",
        "    input_ids[i] = 0;\n",
        "    attention_mask[i] = 0;\n",
        "    token_type_ids[i] = 0;\n",
        "  }\n",
        "  _tokens_length = 0;\n",
        "\n",
        "//  cout << \"Local run time is: \" << time_cost << endl;\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::_lower_token(const string &token, string &new_token) {\n",
        "  int length = token.length();\n",
        "  if (token[0] == '[') {\n",
        "    if (length == 5) {\n",
        "      if (token[4] == ']') {\n",
        "        if (token[1] == 'U' && token[2] == 'N' && token[3] == 'K') {\n",
        "          new_token = token;\n",
        "          return;\n",
        "        }\n",
        "        if (token[1] == 'S' && token[2] == 'E' && token[3] == 'P') {\n",
        "          new_token = token;\n",
        "          return;\n",
        "        }\n",
        "        if (token[1] == 'P' && token[2] == 'A' && token[3] == 'D') {\n",
        "          new_token = token;\n",
        "          return;\n",
        "        }\n",
        "        if (token[1] == 'C' && token[2] == 'L' && token[3] == 'S') {\n",
        "          new_token = token;\n",
        "          return;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    if (length == 6) {\n",
        "      if (token[1] == 'M' && token[2] == 'A' && token[3] == 'S' && token[4] == 'K' && token[5] == ']') {\n",
        "        new_token = token;\n",
        "        return;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  for (char ch: token) {\n",
        "    if (ch <= 90 && ch >= 65) {\n",
        "      new_token += char(ch + 32);\n",
        "    } else {\n",
        "      new_token += ch;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::_clean_text() {\n",
        "  int pos;\n",
        "  pos = _text.find(\"\\u00A0\");\n",
        "  while (pos > 0) {\n",
        "    _text = _text.replace(pos, 2, \" \");\n",
        "    pos = _text.find(\"\\u00A0\");\n",
        "  }\n",
        "  pos = _text.find(\"\\u2800\");\n",
        "  while (pos > 0) {\n",
        "    _text = _text.replace(pos, 3, \" \");\n",
        "    pos = _text.find(\"\\u2800\");\n",
        "  }\n",
        "  pos = _text.find(\"\\u3000\");\n",
        "  while (pos > 0) {\n",
        "    _text = _text.replace(pos, 3, \" \");\n",
        "    pos = _text.find(\"\\u3000\");\n",
        "  }\n",
        "  pos = _text.find(\"\\ufeff\");\n",
        "  while (pos > 0) {\n",
        "    _text = _text.replace(pos, 3, \"\");\n",
        "    pos = _text.find(\"\\ufeff\");\n",
        "  }\n",
        "  pos = _text.find(\"\\ue312\");\n",
        "  while (pos > 0) {\n",
        "    _text = _text.replace(pos, 3, \"\");\n",
        "    pos = _text.find(\"\\ue312\");\n",
        "  }\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::_load_vocab(const string &vocab_file) {\n",
        "  int index = 0;\n",
        "  fstream fin;\n",
        "  fin.open(vocab_file, ios::in);\n",
        "  if (fin.is_open()) {\n",
        "    string basicString;\n",
        "    while (!fin.eof()) {\n",
        "      getline(fin, basicString, '\\n');\n",
        "      if (!basicString.empty()) {\n",
        "        _vocab[basicString] = index;\n",
        "      }\n",
        "      index++;\n",
        "    }\n",
        "    fin.close();\n",
        "  }\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::_fixed_matching(int &pos, string &token) {\n",
        "  token += _text[pos];\n",
        "  int rest_length = _text_length - pos;\n",
        "  if (rest_length < 2) {\n",
        "    pos++;\n",
        "    return;\n",
        "  }\n",
        "  if (_text[pos] == char(-16)) {\n",
        "    if (rest_length < 4) {\n",
        "      pos++;\n",
        "      return;\n",
        "    }\n",
        "    if (_text[pos+1] < char(0) && _text[pos+2] < char(0) && _text[pos+3] < char(0)) {\n",
        "      token += _text[pos+1];\n",
        "      token += _text[pos+2];\n",
        "      token += _text[pos+3];\n",
        "      pos += 4;\n",
        "      return;\n",
        "    }\n",
        "    pos++;\n",
        "    return;\n",
        "  }\n",
        "  if (_text[pos] >= char(-62) && _text[pos] <= char(-37)) {\n",
        "    if (_text[pos+1] < char(0)) {\n",
        "      token += _text[pos+1];\n",
        "      pos += 2;\n",
        "      return;\n",
        "    }\n",
        "    pos++;\n",
        "    return;\n",
        "  }\n",
        "  if (rest_length < 3) {\n",
        "    pos++;\n",
        "    return;\n",
        "  }\n",
        "  if (_text[pos+1] < char(0) && _text[pos+2] < char(0)) {\n",
        "    token += _text[pos+1];\n",
        "    token += _text[pos+2];\n",
        "    pos += 3;\n",
        "    return;\n",
        "  }\n",
        "  pos++;\n",
        "}\n",
        "\n",
        "void CustomizedTokenizer::_split_text() {\n",
        "  // Performs invalid character removal and whitespace cleanup on text.\n",
        "  _clean_text();\n",
        "  _text_length = _text.length();\n",
        "  int pos = 0;\n",
        "  string token;\n",
        "  _tokens[_tokens_length++] = \"[CLS]\";\n",
        "  while (pos < _text_length) {\n",
        "    if (_tokens_length == MAX_SEQ_LENGTH) {\n",
        "      break;\n",
        "    }\n",
        "    if (_text[pos] == ' ') {\n",
        "      if (!token.empty()) {\n",
        "        if (token[0] >= char(0) && _do_lower_case) {\n",
        "          string new_token;\n",
        "          _lower_token(token, new_token);\n",
        "          _tokens[_tokens_length++] = new_token;\n",
        "        } else {\n",
        "          _tokens[_tokens_length++] = token;\n",
        "        }\n",
        "        token.clear();\n",
        "      }\n",
        "      pos++;\n",
        "      continue;\n",
        "    }\n",
        "    // We treat all non-letter/number ASCII as punctuation.\n",
        "    // Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    // Punctuation class but we treat them as punctuation anyways, for\n",
        "    // consistency.\n",
        "    if ((_text[pos] >= char(33) && _text[pos] <= char(47)) ||\n",
        "      (_text[pos] >= char(58) && _text[pos] <= char(64)) ||\n",
        "      (_text[pos] >= char(91) && _text[pos] <= char(96)) ||\n",
        "      (_text[pos] >= char(123) && _text[pos] <= char(126))\n",
        "      ) {\n",
        "      if (!token.empty()) {\n",
        "        if (token[0] >= char(0) && _do_lower_case) {\n",
        "          string new_token;\n",
        "          _lower_token(token, new_token);\n",
        "          _tokens[_tokens_length++] = new_token;\n",
        "        } else {\n",
        "          _tokens[_tokens_length++] = token;\n",
        "        }\n",
        "        token.clear();\n",
        "      }\n",
        "      if (_text[pos] == char(91)) {\n",
        "        if (pos < _text_length - 4) {\n",
        "          if (_text[pos+1] == 'S' && _text[pos+2] == 'E' && _text[pos+3] == 'P' && _text[pos+4] == ']') {\n",
        "            _tokens[_tokens_length++] = \"[SEP]\";\n",
        "            pos += 5;\n",
        "            continue;\n",
        "          }\n",
        "          if (_text[pos+1] == 'U' && _text[pos+2] == 'N' && _text[pos+3] == 'K' && _text[pos+4] == ']') {\n",
        "            _tokens[_tokens_length++] = \"[UNK]\";\n",
        "            pos += 5;\n",
        "            continue;\n",
        "          }\n",
        "          if (_text[pos+1] == 'P' && _text[pos+2] == 'A' && _text[pos+3] == 'D' && _text[pos+4] == ']') {\n",
        "            _tokens[_tokens_length++] = \"[PAD]\";\n",
        "            pos += 5;\n",
        "            continue;\n",
        "          }\n",
        "          if (_text[pos+1] == 'C' && _text[pos+2] == 'L' && _text[pos+3] == 'S' && _text[pos+4] == ']') {\n",
        "            _tokens[_tokens_length++] = \"[CLS]\";\n",
        "            pos += 5;\n",
        "            continue;\n",
        "          }\n",
        "        }\n",
        "        if (pos < _text_length - 5) {\n",
        "          if (_text[pos+1] == 'M' && _text[pos+2] == 'A' && _text[pos+3] == 'S' && _text[pos+4] == 'K' &&\n",
        "            _text[pos+5] == ']') {\n",
        "            _tokens[_tokens_length++] = \"[MASK]\";\n",
        "            pos += 6;\n",
        "            continue;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      string temp = {_text[pos]};\n",
        "      _tokens[_tokens_length++] = temp;\n",
        "      pos++;\n",
        "      continue;\n",
        "    }\n",
        "    if (_text[pos] < char(0)) {\n",
        "      if (!token.empty()) {\n",
        "        if (token[0] >= char(0) && _do_lower_case) {\n",
        "          string new_token;\n",
        "          _lower_token(token, new_token);\n",
        "          _tokens[_tokens_length++] = new_token;\n",
        "        } else {\n",
        "          _tokens[_tokens_length++] = token;\n",
        "        }\n",
        "        token.clear();\n",
        "      }\n",
        "      _fixed_matching(pos, token);\n",
        "      _tokens[_tokens_length++] = token;\n",
        "      token.clear();\n",
        "    } else {\n",
        "      token += _text[pos];\n",
        "      pos++;\n",
        "    }\n",
        "  }\n",
        "  if (!token.empty()) {\n",
        "    if (token[0] >= char(0) && _do_lower_case) {\n",
        "      string new_token;\n",
        "      _lower_token(token, new_token);\n",
        "      _tokens[_tokens_length++] = new_token;\n",
        "    } else {\n",
        "      _tokens[_tokens_length++] = token;\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zViN4vOoIeX0"
      },
      "source": [
        "//\n",
        "// Created by meng on 3/30/21.\n",
        "//\n",
        "\n",
        "#ifndef FEDERALLEARNING_MINDSPORE_LITE_FLCLIENT_SRC_MAIN_NATIVE_DATASET_CUSTOMIZEDTOKENIZER_H_\n",
        "#define FEDERALLEARNING_MINDSPORE_LITE_FLCLIENT_SRC_MAIN_NATIVE_DATASET_CUSTOMIZEDTOKENIZER_H_\n",
        "#include <fstream>\n",
        "#include <iostream>\n",
        "#include <cstdio>\n",
        "#include <vector>\n",
        "#include <map>\n",
        "#include <ctime>\n",
        "//#include <cstring>\n",
        "//#include <algorithm>\n",
        "//#include <cstdlib>\n",
        "using namespace std;\n",
        "#define MAX_SEQ_LENGTH 32\n",
        "#define BATCH_SIZE 16\n",
        "class CustomizedTokenizer\n",
        "{\n",
        " public:\n",
        "  CustomizedTokenizer();\n",
        "  ~CustomizedTokenizer();\n",
        "\n",
        "  void init(const std::string &vocab_file, bool do_lower_case);\n",
        "  void tokenize(const string &text, string output_tokens[MAX_SEQ_LENGTH], int &seq_length);\n",
        "  void tokenize(const string &text, int input_ids[MAX_SEQ_LENGTH],\n",
        "                int attention_mask[MAX_SEQ_LENGTH], int token_type_ids[MAX_SEQ_LENGTH], int &seq_length);\n",
        "\n",
        "//private:\n",
        "  string _text;\n",
        "  string _tokens[MAX_SEQ_LENGTH];\n",
        "  int _tokens_length = 0;\n",
        "  int _text_length = 0;\n",
        "\n",
        "  map<string, int> _vocab;\n",
        "  bool _do_lower_case = true;\n",
        "  int _max_input_chars_per_word = 100;\n",
        "\n",
        "  static void _lower_token(const string &token, string &new_token);\n",
        "  void _split_text();\n",
        "  void _clean_text();\n",
        "  void _fixed_matching(int &pos, string &token);\n",
        "  void _load_vocab(const string &vocab_file);\n",
        "};\n",
        "\n",
        "\n",
        "\n",
        "#endif  // FEDERALLEARNING_MINDSPORE_LITE_FLCLIENT_SRC_MAIN_NATIVE_DATASET_CUSTOMIZEDTOKENIZER_H_\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}