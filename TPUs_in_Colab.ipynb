{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT3qRPx9LSPm"
      },
      "source": [
        "diff --git a/build.sh b/build.sh\n",
        "index 6addf18..cafc92a 100755\n",
        "--- a/build.sh\n",
        "+++ b/build.sh\n",
        "@@ -492,11 +492,8 @@ write_commit_file() {\n",
        " build_lite_x86_64_jni_and_jar()\n",
        " {\n",
        "     # copy x86 so\n",
        "-    local is_train=off\n",
        "+    local is_train=on\n",
        "     cd ${BASEPATH}/output/tmp\n",
        "-    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "-      is_train=on\n",
        "-    fi\n",
        "     local pkg_name=mindspore-lite-${VERSION_STR}-linux-x64\n",
        " \n",
        "     cd ${BASEPATH}/output/tmp/\n",
        "@@ -506,7 +503,12 @@ build_lite_x86_64_jni_and_jar()\n",
        "     rm -rf ${LITE_JAVA_PATH}/native/libs/linux_x86/ && mkdir -pv ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "     cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "     cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "-    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+    local train_so=$pkg_name/runtime/lib/libmindspore-lite-train.so\n",
        "+    if [ ! -f \"$train_so\" ]; then\n",
        "+      echo \"not exist\"\n",
        "+      is_train=off\n",
        "+    fi\n",
        "+    if [ \"$is_train\" == on ]; then\n",
        "         cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/linux_x86/libs/\n",
        "         cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/linux_x86/\n",
        "     fi\n",
        "@@ -641,10 +643,7 @@ build_lite_arm64_and_jni() {\n",
        "     # build arm64\n",
        "     build_lite \"arm64\"\n",
        "     # copy arm64 so\n",
        "-    local is_train=off\n",
        "-    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "-      is_train=on\n",
        "-    fi\n",
        "+    local is_train=on\n",
        "     local pkg_name=mindspore-lite-${VERSION_STR}-android-aarch64\n",
        "     cd \"${BASEPATH}/mindspore/lite/build\"\n",
        " \n",
        "@@ -654,7 +653,12 @@ build_lite_arm64_and_jni() {\n",
        "     rm -rf ${LITE_JAVA_PATH}/native/libs/arm64-v8a/   && mkdir -p ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "     cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "     cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "-    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+    local train_so=$pkg_name/runtime/lib/libmindspore-lite-train.so\n",
        "+    if [ ! -f \"$train_so\" ]; then\n",
        "+      echo \"not exist\"\n",
        "+      is_train=off\n",
        "+    fi\n",
        "+    if [ \"$is_train\" == on ]; then\n",
        "       cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/arm64-v8a/\n",
        "       cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/arm64-v8a/\n",
        "     fi\n",
        "@@ -679,10 +683,7 @@ build_lite_arm32_and_jni() {\n",
        "     # build arm32\n",
        "     build_lite \"arm32\"\n",
        "     # copy arm32 so\n",
        "-    local is_train=off\n",
        "-    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "-      is_train=on\n",
        "-    fi\n",
        "+    local is_train=on\n",
        "     local pkg_name=mindspore-lite-${VERSION_STR}-android-aarch32\n",
        "     cd \"${BASEPATH}/mindspore/lite/build\"\n",
        " \n",
        "@@ -692,7 +693,12 @@ build_lite_arm32_and_jni() {\n",
        "     rm -rf ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/   && mkdir -pv ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "     cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "     cp ./${pkg_name}/runtime/lib/*.so* ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "-    if [ \"${MSLITE_ENABLE_TRAIN}\" ]; then\n",
        "+    local train_so=$pkg_name/runtime/lib/libmindspore-lite-train.so\n",
        "+    if [ ! -f \"$train_so\" ]; then\n",
        "+      echo \"not exist\"\n",
        "+      is_train=off\n",
        "+    fi\n",
        "+    if [ \"$is_train\" == on ]; then\n",
        "       cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/java/app/libs/armeabi-v7a/\n",
        "       cp ./${pkg_name}/runtime/third_party/libjpeg-turbo/lib/*.so* ${LITE_JAVA_PATH}/native/libs/armeabi-v7a/\n",
        "     fi\n",
        "diff --git a/mindspore/lite/test/st/scripts/run_benchmark_arm.sh b/mindspore/lite/test/st/scripts/run_benchmark_arm.sh\n",
        "index c1cc3eb..cc2e5f3 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_benchmark_arm.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_benchmark_arm.sh\n",
        "@@ -102,14 +102,14 @@ function Run_Converter() {\n",
        "           continue\n",
        "         fi\n",
        "         echo ${model_name}'_train' >> \"${run_converter_log_file}\"\n",
        "-        echo './converter_lite  --fmk=MINDIR --modelFile='${models_path}'/'${model_name}' --outputFile='${ms_models_path}'/'${model_name}'_train  --trainModel=true' >> \"${run_converter_log_file}\"\n",
        "-        ./converter_lite  --fmk=MINDIR --modelFile=${models_path}/${model_name} --outputFile=${ms_models_path}/${model_name}'_train' --trainModel=true\n",
        "+        echo './converter_lite  --fmk=MINDIR --modelFile='${train_models_path}'/'${model_name}' --outputFile='${ms_models_path}'/'${model_name}'_train  --trainModel=true' >> \"${run_converter_log_file}\"\n",
        "+        ./converter_lite  --fmk=MINDIR --modelFile=${train_models_path}/${model_name} --outputFile=${train_modes_path}/${model_name}'_train' --trainModel=true\n",
        "         if [ $? = 0 ]; then\n",
        "             converter_result='converter mindspore '${model_name}'_train pass';echo ${converter_result} >> ${run_converter_result_file}\n",
        "         else\n",
        "             converter_result='converter mindspore '${model_name}'_train failed';echo ${converter_result} >> ${run_converter_result_file};return 1\n",
        "         fi\n",
        "-    done < ${models_mindspore_train_config}\n",
        "+    done < ${models_ms_train_config}\n",
        " \n",
        "     # Convert TFLite PostTraining models:\n",
        "     while read line; do\n",
        "@@ -606,13 +606,17 @@ function Run_arm64() {\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "     if [ -f ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ]; then\n",
        "         cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+        cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/libjpeg-turbo/lib/libjpeg.so* ${benchmark_test_path}/ || exit 1\n",
        "+        cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/libjpeg-turbo/lib/libturbojpeg.so* ${benchmark_test_path}/ || exit 1\n",
        "     fi\n",
        "     cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "     cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "     cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        " \n",
        "     cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version_arm}-android-aarch64/runtime/lib/libmindspore-lite-train.so ${benchmark_train_test_path}/libmindspore-lite-train.so || exit 1\n",
        "     cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm64_path}/mindspore-lite-${version}-android-aarch64/tools/benchmark_train/benchmark_train ${benchmark_test_path}/benchmark_train || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -621,6 +625,7 @@ function Run_arm64() {\n",
        "     echo 'cd  /data/local/tmp/benchmark_test' > adb_cmd.txt\n",
        "     echo 'cp  /data/local/tmp/libc++_shared.so ./' >> adb_cmd.txt\n",
        "     echo 'chmod 777 benchmark' >> adb_cmd.txt\n",
        "+    echo 'chmod 777 benchmark_train' >> adb_cmd.txt\n",
        " \n",
        "     adb -s ${device_id} shell < adb_cmd.txt\n",
        " \n",
        "@@ -1016,6 +1021,63 @@ function Run_arm64() {\n",
        "             run_result='arm64: '${model_name}' failed'; echo ${run_result} >> ${run_benchmark_result_file}; return 1\n",
        "         fi\n",
        "     done < ${models_for_process_only_config}\n",
        "+    fail=0\n",
        "+    # Run mindir converted train models:\n",
        "+    tmp_dir=/data/local/tmp/benchmark_train_test\n",
        "+    adb -s ${device_id} push ${benchmark_train_test_path} $tmp_dir > adb_push_log.txt\n",
        "+    while read line; do\n",
        "+        LFS=\" \" read -r -a line_array <<< ${line}\n",
        "+        model_prefix=${line_array[0]}\n",
        "+        model_name=${line_array[0]}'_train'\n",
        "+        accuracy_limit=0.5\n",
        "+        if [[ $model_name == \\#* ]]; then\n",
        "+            continue\n",
        "+        fi\n",
        "+        if [[ \"${line_array[1]}\" == \"weight_quant\" ]]; then\n",
        "+            model_name=${line_array[0]}'_train_quant'\n",
        "+            accuracy_limit=${line_array[2]}\n",
        "+        fi\n",
        "+        export_file=\"${tmp_dir}/${model_name}_tod\"\n",
        "+        inference_file=\"${tmp_dir}/${model_name}_infer\"\n",
        "+\n",
        "+        if [[ \"${line_array[1]}\" == \"noarm32\" ]] && [[ \"$1\" == arm32 ]]; then\n",
        "+            run_result=$1': '${model_name}' irrelevant'; echo ${run_result} >> ${run_benchmark_train_result_file}\n",
        "+            continue\n",
        "+        fi\n",
        "+        # run benchmark_train test with clib data\n",
        "+        echo ${model_name} >> \"${run_arm64_fp32_log_file}\"\n",
        "+        adb -s ${device_id} push ${train_io_path}/${model_prefix}_input*.bin ${train_io_path}/${model_prefix}_output*.bin /data/local/tmp/benchmark_train_test >> adb_push_log.txt\n",
        "+        echo 'cd /data/local/tmp/benchmark_train_test' > ${adb_cmd_arm64_run_file}\n",
        "+        echo 'chmod 777 benchmark_train' >> ${adb_cmd_arm64_run_file}\n",
        "+        echo 'cp  /data/local/tmp/libc++_shared.so ./' >> ${adb_cmd_arm64_run_file}\n",
        "+        adb -s ${device_id} shell < ${adb_cmd_arm64_run_file} >> ${run_arm64_fp32_log_file}\n",
        "+        echo \"rm -f ${export_file}* ${inference_file}*\" >> ${run_arm64_fp32_log_file}\n",
        "+        echo \"rm -f ${export_file}* ${inference_file}*\" >> ${adb_cmd_arm64_run_file}\n",
        "+        adb -s ${device_id} shell < ${adb_cmd_arm64_run_file} >> ${run_arm64_fp32_log_file}\n",
        "+        adb_cmd=$(cat <<-ENDM\n",
        "+        export LD_LIBRARY_PATH=./:/data/local/tmp/:/data/local/tmp/benchmark_train_test;./benchmark_train \\\n",
        "+        --epochs=${epoch_num} \\\n",
        "+        --modelFile=${model_name}.ms \\\n",
        "+        --inDataFile=${tmp_dir}/${model_prefix}_input \\\n",
        "+        --expectedDataFile=${tmp_dir}/${model_prefix}_output \\\n",
        "+        --numThreads=${threads} \\\n",
        "+        --accuracyThreshold=${accuracy_limit} \\\n",
        "+        --inferenceFile=${inference_file} \\\n",
        "+        --exportFile=${export_file}\n",
        "+ENDM\n",
        "+        )\n",
        "+        echo \"${adb_cmd}\" >> ${run_arm64_fp32_log_file}\n",
        "+        echo \"${adb_cmd}\" >> ${adb_cmd_arm64_run_file}\n",
        "+        adb -s ${device_id} shell < ${adb_cmd_arm64_run_file} >> ${run_arm64_fp32_log_file}\n",
        "+        # TODO: change to arm_type\n",
        "+        if [ $? = 0 ]; then\n",
        "+            run_result=$1': '${model_name}' pass'; echo ${run_result} >> ${run_benchmark_train_result_file}\n",
        "+        else\n",
        "+            run_result=$1': '${model_name}' failed'; echo ${run_result} >> ${run_benchmark_train_result_file};\n",
        "+            fail=1\n",
        "+        fi\n",
        "+    done < ${models_ms_train_config}\n",
        "+    return ${fail}\n",
        " }\n",
        " \n",
        " # Run on arm32 platform:\n",
        "@@ -1027,12 +1089,16 @@ function Run_arm32() {\n",
        "     cd ${benchmark_test_path} || exit 1\n",
        "     if [ -f ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libminddata-lite.so ]; then\n",
        "         cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libminddata-lite.so ${benchmark_test_path}/libminddata-lite.so || exit 1\n",
        "+        cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/libjpeg-turbo/lib/libturbojpeg.so* ${benchmark_test_path}/ || exit 1\n",
        "+        cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/libjpeg-turbo/lib/libjpeg.so* ${benchmark_test_path}/ || exit 1\n",
        "     fi\n",
        "     cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai.so ${benchmark_test_path}/libhiai.so || exit 1\n",
        "     cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai_ir.so ${benchmark_test_path}/libhiai_ir.so || exit 1\n",
        "     cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/third_party/hiai_ddk/lib/libhiai_ir_build.so ${benchmark_test_path}/libhiai_ir_build.so || exit 1\n",
        "-    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libmindspore-lite.so ${benchmark_test_path}/libmindspore-lite-train.so || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/runtime/lib/libmindspore-lite-train.so ${benchmark_test_path}/libmindspore-lite.so || exit 1\n",
        "     cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/tools/benchmark/benchmark ${benchmark_test_path}/benchmark || exit 1\n",
        "+    cp -a ${arm32_path}/mindspore-lite-${version}-android-aarch32/tools/benchmark_train/benchmark_train ${benchmark_test_path}/benchmark || exit 1\n",
        " \n",
        "     # adb push all needed files to the phone\n",
        "     adb -s ${device_id} push ${benchmark_test_path} /data/local/tmp/ > adb_push_log.txt\n",
        "@@ -1072,6 +1138,65 @@ function Run_arm32() {\n",
        "             run_result='arm32: '${model_name}' failed'; echo ${run_result} >> ${run_benchmark_result_file}; return 1\n",
        "         fi\n",
        "     done < ${models_arm32_config}\n",
        "+\n",
        "+    fail=0\n",
        "+    # Run mindir converted train models:\n",
        "+    tmp_dir=/data/local/tmp/benchmark_train_test\n",
        "+    # cp train ms to adb\n",
        "+    adb -s ${device_id} push ${benchmark_train_test_path} $tmp_dir > adb_push_log.txt\n",
        "+    while read line; do\n",
        "+        LFS=\" \" read -r -a line_array <<< ${line}\n",
        "+        model_prefix=${line_array[0]}\n",
        "+        model_name=${line_array[0]}'_train'\n",
        "+        accuracy_limit=0.5\n",
        "+        if [[ $model_name == \\#* ]]; then\n",
        "+            continue\n",
        "+        fi\n",
        "+        if [[ \"${line_array[1]}\" == \"weight_quant\" ]]; then\n",
        "+            model_name=${line_array[0]}'_train_quant'\n",
        "+            accuracy_limit=${line_array[2]}\n",
        "+        fi\n",
        "+        export_file=\"${tmp_dir}/${model_name}_tod\"\n",
        "+        inference_file=\"${tmp_dir}/${model_name}_infer\"\n",
        "+\n",
        "+        if [[ \"${line_array[1]}\" == \"noarm32\" ]] && [[ \"$1\" == arm32 ]]; then\n",
        "+            run_result=$1': '${model_name}' irrelevant'; echo ${run_result} >> ${run_benchmark_train_result_file}\n",
        "+            continue\n",
        "+        fi\n",
        "+        # run benchmark_train test without clib data\n",
        "+        echo ${model_name} >> \"${run_arm32_log_file}\"\n",
        "+        adb -s ${device_id} push ${train_io_path}/${model_prefix}_input*.bin ${train_io_path}/${model_prefix}_output*.bin  /data/local/tmp/benchmark_train_test >> adb_push_log.txt\n",
        "+        echo 'cd /data/local/tmp/benchmark_train_test' > ${adb_cmd_arm32_run_file}\n",
        "+        echo 'chmod 777 benchmark_train' >> ${adb_cmd_arm32_run_file}\n",
        "+        echo 'cp  /data/local/tmp/libc++_shared.so ./' >> ${adb_cmd_arm32_run_file}\n",
        "+        adb -s ${device_id} shell < adb_run_cmd.txt >> ${run_arm32_log_file}\n",
        "+        echo \"rm -f ${export_file}* ${inference_file}*\" >> ${run_arm32_log_file}\n",
        "+        echo \"rm -f ${export_file}* ${inference_file}*\" >> ${adb_cmd_arm32_run_file}\n",
        "+        adb -s ${device_id} shell < ${adb_cmd_arm32_run_file} >> ${run_arm32_log_file}\n",
        "+        adb_cmd=$(cat <<-ENDM\n",
        "+        export LD_LIBRARY_PATH=./:/data/local/tmp/:/data/local/tmp/benchmark_train_test;./benchmark_train \\\n",
        "+        --epochs=${epoch_num} \\\n",
        "+        --modelFile=${model_name}.ms \\\n",
        "+        --inDataFile=${tmp_dir}/${model_prefix}_input \\\n",
        "+        --expectedDataFile=${tmp_dir}/${model_prefix}_output \\\n",
        "+        --numThreads=${threads} \\\n",
        "+        --accuracyThreshold=${accuracy_limit} \\\n",
        "+        --inferenceFile=${inference_file} \\\n",
        "+        --exportFile=${export_file}\n",
        "+ENDM\n",
        "+        )\n",
        "+        echo \"${adb_cmd}\" >> ${run_arm32_log_file}\n",
        "+        echo \"${adb_cmd}\" >> ${adb_cmd_arm32_run_file}\n",
        "+        adb -s ${device_id} shell < ${adb_cmd_arm32_run_file} >> ${run_arm32_log_file}\n",
        "+        # TODO: change to arm_type\n",
        "+        if [ $? = 0 ]; then\n",
        "+            run_result=$1': '${model_name}' pass'; echo ${run_result} >> ${run_benchmark_train_result_file}\n",
        "+        else\n",
        "+            run_result=$1': '${model_name}' failed'; echo ${run_result} >> ${run_benchmark_train_result_file};\n",
        "+            fail=1\n",
        "+        fi\n",
        "+    done < ${models_ms_train_config}\n",
        "+    return ${fail}\n",
        " }\n",
        " \n",
        " # Run on arm64-fp16 platform:\n",
        "@@ -1456,7 +1581,8 @@ function Print_Benchmark_Result() {\n",
        " basepath=$(pwd)\n",
        " echo ${basepath}\n",
        " #set -e\n",
        "-\n",
        "+train_models_path=\"\"\n",
        "+train_io_path=\"\"\n",
        " # Example:sh run_benchmark_arm.sh -r /home/temp_test -m /home/temp_test/models -d \"8KE5T19620002408\" -e arm_cpu\n",
        " while getopts \"r:m:d:e:\" opt; do\n",
        "     case ${opt} in\n",
        "@@ -1468,6 +1594,14 @@ while getopts \"r:m:d:e:\" opt; do\n",
        "             models_path=${OPTARG}\n",
        "             echo \"models_path is ${OPTARG}\"\n",
        "             ;;\n",
        "+        i)\n",
        "+            train_io_path=${OPTARG}\n",
        "+            echo \"train_io_path is ${OPTARG}\"\n",
        "+            ;;\n",
        "+        M)\n",
        "+            train_models_path=${OPTARG}\n",
        "+            echo \"train_models_path is ${models_path}\"\n",
        "+            ;;\n",
        "         d)\n",
        "             device_id=${OPTARG}\n",
        "             echo \"device_id is ${OPTARG}\"\n",
        "@@ -1476,6 +1610,10 @@ while getopts \"r:m:d:e:\" opt; do\n",
        "             backend=${OPTARG}\n",
        "             echo \"backend is ${OPTARG}\"\n",
        "             ;;\n",
        "+        q)\n",
        "+           threads=${OPTARG}\n",
        "+           echo \"threads=${threads}\"\n",
        "+           ;;\n",
        "         ?)\n",
        "         echo \"unknown para\"\n",
        "         exit 1;;\n",
        "@@ -1514,9 +1652,32 @@ models_with_multiple_inputs_config=${basepath}/../config/models_with_multiple_in\n",
        " models_for_process_only_config=${basepath}/../config/models_for_process_only.cfg\n",
        " models_tf_weightquant_config=${basepath}/../config/models_tf_weightquant.cfg\n",
        " models_codegen_config=${basepath}/../codegen/models_codegen.cfg\n",
        "+models_ms_train_config=${basepath}/../config/models_ms_train.cfg\n",
        " \n",
        " ms_models_path=${basepath}/ms_models\n",
        "+train_modes_path = ${basepath}/ms_train_models\n",
        "+rm -rf ${train_modes_path}\n",
        "+mkdir -p ${train_modes_path}\n",
        " build_path=${basepath}/codegen_build\n",
        "+logs_path=${basepath}/logs_train\n",
        "+rm -rf ${logs_path}\n",
        "+mkdir -p ${logs_path}\n",
        "+adb_cmd_arm64_run_file = ${logs_path}/adb_arm64_cmd.txt\n",
        "+adb_cmd_arm32_run_file = ${logs_path}/adb_arm32_cmd.txt\n",
        "+\n",
        "+train_models_path\n",
        "+if [[ train_models_path == \"\" ]]\n",
        "+then\n",
        "+  echo \"train_io path is empty\"\n",
        "+  train_models_path=\"../${models_path}/models_train/\"\n",
        "+fi\n",
        "+echo train_models_path\n",
        "+if [[ $train_io_path == \"\" ]]\n",
        "+then\n",
        "+  echo \"train_io path is empty\"\n",
        "+  train_io_path=${train_models_path}/input_output\n",
        "+fi\n",
        "+echo $train_io_path\n",
        " \n",
        " # Write converter result to temp file\n",
        " run_converter_log_file=${basepath}/run_converter_log.txt\n",
        "@@ -1555,6 +1716,9 @@ echo 'run arm64_fp32 logs: ' > ${run_arm64_fp32_log_file}\n",
        " run_arm64_fp32_codegen_log_file=${basepath}/run_arm64_fp32_codegen_log.txt\n",
        " echo 'run arm64_codegen logs: ' > ${run_arm64_fp32_codegen_log_file}\n",
        " \n",
        "+run_arm64_train_fp32_log_file=${basepath}/run_arm64_train_fp32_log.txt\n",
        "+echo 'run arm64_train_fp32 logs: ' > ${run_arm64_train_fp32_log_file}\n",
        "+\n",
        " run_arm32_fp32_codegen_log_file=${basepath}/run_arm32_fp32_codegen_log.txt\n",
        " echo 'run arm32_codegen logs: ' > ${run_arm32_fp32_codegen_log_file}\n",
        " \n",
        "@@ -1572,9 +1736,13 @@ echo \"Push files to the arm and run benchmark\"\n",
        " benchmark_test_path=${basepath}/benchmark_test\n",
        " rm -rf ${benchmark_test_path}\n",
        " mkdir -p ${benchmark_test_path}\n",
        "+benchmark_train_test_path = =${basepath}/benchmark_train_test\n",
        "+rm -rf ${benchmark_train_test_path}\n",
        "+mkdir -p ${benchmark_train_test_path}\n",
        " cp -a ${ms_models_path}/*.ms ${benchmark_test_path} || exit 1\n",
        " # Copy models converted using old release of mslite converter for compatibility test\n",
        " cp -a ${models_path}/compatibility_test/*.ms ${benchmark_test_path} || exit 1\n",
        "+cp -a {train_modes_path}/*.ms ${benchmark_train_test_path} || exit 1\n",
        " \n",
        " backend=${backend:-\"all\"}\n",
        " isFailed=0\n",
        "@@ -1646,6 +1814,20 @@ if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_fp32\" ]\n",
        "     sleep 1\n",
        " fi\n",
        " \n",
        "+if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_fp32\" || $backend == \"train\"]]; then\n",
        "+    # Run on arm64\n",
        "+    arm64_path=${release_path}/android_aarch64\n",
        "+    # mv ${arm64_path}/*train-android-aarch64* ./train\n",
        "+    file_name=$(ls ${arm64_path}/*android-aarch64.tar.gz)\n",
        "+    IFS=\"-\" read -r -a file_name_array <<< \"$file_name\"\n",
        "+    version=${file_name_array[2]}\n",
        "+\n",
        "+    echo \"start Run arm64 train ...\"\n",
        "+    Run_arm64\n",
        "+    Run_arm64_fp32_status=$?\n",
        "+    sleep 1\n",
        "+fi\n",
        "+\n",
        " if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_fp16\" ]]; then\n",
        "     # Run on arm64-fp16\n",
        "     arm64_path=${release_path}/android_aarch64\n",
        "@@ -1675,6 +1857,13 @@ if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_codegen\n",
        "         isFailed=1\n",
        "     fi\n",
        " fi\n",
        "+if [[ $backend == \"all\" || $backend == \"arm64_cpu\" || $backend == \"arm64_train\" ]]; then\n",
        "+    if [[ ${Run_arm64_train} != 0 ]];then\n",
        "+        echo \"Run_arm64_train failed\"\n",
        "+        cat ${run_arm64_fp32_train_log_file}\n",
        "+        isFailed=1\n",
        "+    fi\n",
        "+fi\n",
        " if [[ $backend == \"all\" || $backend == \"arm32_cpu\" || $backend == \"arm32_codegen\" ]]; then\n",
        "     if [[ ${Run_arm32_codegen_status} != 0 ]];then\n",
        "         echo \"Run_arm32 codegen failed\"\n",
        "diff --git a/mindspore/lite/test/st/scripts/run_benchmark_x86.sh b/mindspore/lite/test/st/scripts/run_benchmark_x86.sh\n",
        "index ebeed83..5f8aab1 100644\n",
        "--- a/mindspore/lite/test/st/scripts/run_benchmark_x86.sh\n",
        "+++ b/mindspore/lite/test/st/scripts/run_benchmark_x86.sh\n",
        "@@ -1,5 +1,36 @@\n",
        " #!/bin/bash\n",
        " \n",
        "+# Run Export on x86 platform and create output test files:\n",
        "+docker_image=mindspore_build:210301\n",
        "+function Run_Export(){\n",
        "+    cd $models_path || exit 1\n",
        "+    if [[ -z \"${CLOUD_MODEL_ZOO}\" ]]; then\n",
        "+        echo \"CLOUD_MODEL_ZOO is not defined - exiting export models\"\n",
        "+        exit 1\n",
        "+    fi\n",
        "+    # Export mindspore train models:\n",
        "+    while read line; do\n",
        "+        LFS=\" \" read -r -a line_array <<< ${line}\n",
        "+        model_name=${line_array[0]}\n",
        "+        if [[ $model_name == \\#* ]]; then\n",
        "+          continue\n",
        "+        fi\n",
        "+        echo ${model_name}'_train_export.py' >> \"${export_log_file}\"\n",
        "+        echo 'exporting' ${model_name}\n",
        "+        if [ -n \"$docker_image\" ]; then\n",
        "+          echo 'docker run --user '\"$(id -u):$(id -g)\"' --env CLOUD_MODEL_ZOO=${CLOUD_MODEL_ZOO} -w $PWD --runtime=nvidia -v /home/$USER:/home/$USER -v /opt/share:/opt/share --privileged=true '${docker_image}' python '${models_path}'/'${model_name}'_train_export.py' >>  \"${export_log_file}\"\n",
        "+          docker run --user \"$(id -u):$(id -g)\" --env CLOUD_MODEL_ZOO=${CLOUD_MODEL_ZOO} -w $PWD --runtime=nvidia -v /home/$USER:/home/$USER -v /opt/share:/opt/share --privileged=true \"${docker_image}\" python ${models_path}'/'${model_name}_train_export.py \"${epoch_num}\"\n",
        "+        else\n",
        "+          echo 'CLOUD_MODEL_ZOO=${CLOUD_MODEL_ZOO} python '${models_path}'/'${model_name}'_train_export.py' >>  \"${export_log_file}\"\n",
        "+          CLOUD_MODEL_ZOO=${CLOUD_MODEL_ZOO} python ${models_path}'/'${model_name}_train_export.py \"${epoch_num}\"\n",
        "+        fi\n",
        "+        if [ $? = 0 ]; then\n",
        "+            export_result='export mindspore '${model_name}'_train_export pass';echo ${export_result} >> ${export_result_file}\n",
        "+        else\n",
        "+            export_result='export mindspore '${model_name}'_train_export failed';echo ${export_result} >> ${export_result_file}\n",
        "+        fi\n",
        "+    done < ${models_ms_train_config}\n",
        "+}\n",
        " # Run converter on x86 platform:\n",
        " function Run_Converter() {\n",
        "     # Unzip x86 runtime and converter\n",
        "@@ -97,17 +128,25 @@ function Run_Converter() {\n",
        " \n",
        "     # Convert mindspore train models:\n",
        "     while read line; do\n",
        "-        model_name=${line}\n",
        "+        LFS=\" \" read -r -a line_array <<< ${line}\n",
        "+        WEIGHT_QUANT=\"\"\n",
        "+        model_prefix=${line_array[0]}'_train'\n",
        "+        model_name=${line_array[0]}'_train'\n",
        "         if [[ $model_name == \\#* ]]; then\n",
        "           continue\n",
        "         fi\n",
        "-        echo ${model_name}'_train' >> \"${run_converter_log_file}\"\n",
        "-        echo './converter_lite  --fmk=MINDIR --modelFile='${models_path}'/'${model_name}' --outputFile='${ms_models_path}'/'${model_name}'_train  --trainModel=true' >> \"${run_converter_log_file}\"\n",
        "-        ./converter_lite  --fmk=MINDIR --modelFile=${models_path}/${model_name} --outputFile=${ms_models_path}/${model_name}'_train' --trainModel=true\n",
        "+        if [[ \"${line_array[1]}\" == \"weight_quant\" ]]; then\n",
        "+            WEIGHT_QUANT=\"--quantType=WeightQuant --bitNum=8 --quantWeightSize=0 --quantWeightChannel=0\"\n",
        "+            model_name=${line_array[0]}'_train_quant'\n",
        "+        fi\n",
        "+\n",
        "+        echo ${model_name} >> \"${run_converter_log_file}\"\n",
        "+        echo './converter_lite  --fmk=MINDIR --modelFile='${train_models_path}'/'${model_prefix}'.mindir --outputFile='${ms_models_path}'/'${model_name}' --trainModel=true' ${WEIGHT_QUANT} >> \"${run_converter_log_file}\"\n",
        "+        ./converter_lite --fmk=MINDIR --modelFile=${train_models_path}/${model_prefix}.mindir --outputFile=${ms_models_path}/${model_name} --trainModel=true ${WEIGHT_QUANT}\n",
        "         if [ $? = 0 ]; then\n",
        "-            converter_result='converter mindspore '${model_name}'_train pass';echo ${converter_result} >> ${run_converter_result_file}\n",
        "+            converter_result='converter mindspore '${model_name}' pass';echo ${converter_result} >> ${run_converter_result_file}\n",
        "         else\n",
        "-            converter_result='converter mindspore '${model_name}'_train failed';echo ${converter_result} >> ${run_converter_result_file};return 1\n",
        "+            converter_result='converter mindspore '${model_name}' failed';echo ${converter_result} >> ${run_converter_result_file};return 1\n",
        "         fi\n",
        "     done < ${models_mindspore_train_config}\n",
        " \n",
        "@@ -346,7 +385,7 @@ function Run_Converter() {\n",
        " function Run_x86() {\n",
        "     echo 'cd  '${x86_path}'/mindspore-lite-'${version}'-linux-x64' >> \"${run_x86_log_file}\"\n",
        "     cd ${x86_path}/mindspore-lite-${version}-linux-x64 || return 1\n",
        "-    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./runtime/lib\n",
        "+    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./runtime/lib:./runtime/third_party/libjpeg-turbo/lib\n",
        "     cp tools/benchmark/benchmark ./ || exit 1\n",
        " \n",
        "     # Run tf converted models:\n",
        "@@ -666,6 +705,40 @@ function Run_x86() {\n",
        "             run_result='x86: '${model_name}' failed'; echo ${run_result} >> ${run_benchmark_result_file}; return 1\n",
        "         fi\n",
        "     done < ${models_for_process_only_config}\n",
        "+\n",
        "+    # Run mindspore converted train models:\n",
        "+    fail=0\n",
        "+    while read line; do\n",
        "+        LFS=\" \" read -r -a line_array <<< ${line}\n",
        "+        model_prefix=${line_array[0]}\n",
        "+        model_name=${line_array[0]}'_train'\n",
        "+        accuracy_limit=0.5\n",
        "+        if [[ $model_name == \\#* ]]; then\n",
        "+          continue\n",
        "+        fi\n",
        "+        if [[ \"${line_array[1]}\" == \"weight_quant\" ]]; then\n",
        "+            model_name=${line_array[0]}'_train_quant'\n",
        "+            accuracy_limit=${line_array[2]}\n",
        "+        fi\n",
        "+        export_file=\"${ms_models_path}/${model_name}_tod\"\n",
        "+        inference_file=\"${ms_models_path}/${model_name}_infer\"\n",
        "+        rm -f ${inference_file}\"*\"\n",
        "+        rm -f ${export_file}\"*\"\n",
        "+        echo ${model_name} >> \"${run_x86_log_file}\"\n",
        "+        ${run_valgrind}./tools/benchmark_train/benchmark_train \\\n",
        "+        --modelFile=${ms_models_path}/${model_name}.ms \\\n",
        "+        --inDataFile=${train_io_path}/${model_prefix}_input \\\n",
        "+        --expectedDataFile=${train_io_path}/${model_prefix}_output --epochs=${epoch_num} --numThreads=${threads} \\\n",
        "+        --accuracyThreshold=${accuracy_limit} --inferenceFile=${inference_file} \\\n",
        "+        --exportFile=${export_file} >> \"${run_x86_log_file}\"\n",
        "+        if [ $? = 0 ]; then\n",
        "+            run_result='x86: '${model_name}' pass'; echo ${run_result} >> ${run_benchmark_train_result_file}\n",
        "+        else\n",
        "+            run_result='x86: '${model_name}' failed'; echo ${run_result} >> ${run_benchmark_train_result_file}\n",
        "+            fail=1\n",
        "+        fi\n",
        "+    done < ${models_ms_train_config}\n",
        "+    return ${fail}\n",
        " }\n",
        " \n",
        " # Run on x86 sse platform:\n",
        "@@ -1339,10 +1412,42 @@ function Print_Benchmark_Result() {\n",
        "     MS_PRINT_TESTCASE_END_MSG\n",
        " }\n",
        " \n",
        "+function Print_Benchmark_Train_Result() {\n",
        "+    MS_PRINT_TESTCASE_START_MSG\n",
        "+    while read line; do\n",
        "+        arr=(\"${line}\")\n",
        "+        printf \"%-20s %-100s %-7s\\n\" ${arr[0]} ${arr[1]} ${arr[2]}\n",
        "+    done < ${run_benchmark_train_result_file}\n",
        "+    MS_PRINT_TESTCASE_END_MSG\n",
        "+}\n",
        "+\n",
        " basepath=$(pwd)\n",
        " echo ${basepath}\n",
        " #set -e\n",
        "+logs_path=${basepath}/logs_train\n",
        "+rm -rf ${logs_path}\n",
        "+mkdir -p ${logs_path}\n",
        "+# Export model if enabled\n",
        "+if [[ $enable_export == 1 ]]; then\n",
        "+    echo \"Start Exporting models ...\"\n",
        "+    # Write export result to temp file\n",
        "+    export_log_file=${logs_path}/export_log.txt\n",
        "+    echo ' ' > ${export_log_file}\n",
        "+\n",
        "+    export_result_file=${logs_path}/export_result.txt\n",
        "+    echo ' ' > ${export_result_file}\n",
        "+    # Run export\n",
        "+    Run_Export\n",
        "+    Print_Result ${export_result_file}\n",
        " \n",
        "+fi\n",
        "+\n",
        "+# Write benchmark_train result to temp file\n",
        "+run_benchmark_train_result_file=${logs_path}/run_benchmark_train_result.txt\n",
        "+echo ' ' > ${run_benchmark_train_result_file}\n",
        "+\n",
        "+train_models_path=\"\"\n",
        "+train_io_path=\"\"\n",
        " # Example:sh run_benchmark_x86.sh -r /home/temp_test -m /home/temp_test/models -e arm_cpu\n",
        " while getopts \"r:m:e:\" opt; do\n",
        "     case ${opt} in\n",
        "@@ -1354,10 +1459,27 @@ while getopts \"r:m:e:\" opt; do\n",
        "             models_path=${OPTARG}\n",
        "             echo \"models_path is ${OPTARG}\"\n",
        "             ;;\n",
        "+        M)\n",
        "+            train_models_path=${OPTARG}\n",
        "+            echo \"train_models_path is ${models_path}\"\n",
        "+            ;;\n",
        "         e)\n",
        "             backend=${OPTARG}\n",
        "             echo \"backend is ${OPTARG}\"\n",
        "             ;;\n",
        "+        i)\n",
        "+            train_io_path=${OPTARG}\n",
        "+            echo \"train_io_path is ${OPTARG}\"\n",
        "+            ;;\n",
        "+        v)\n",
        "+            run_valgrind=\"valgrind --log-file=valgrind.log \"\n",
        "+            echo \"Run x86 with valgrind\"\n",
        "+            ;;\n",
        "+        p)\n",
        "+            enable_export=1\n",
        "+            docker_image=${OPTARG}\n",
        "+            echo \"enable_export = 1, docker_image = ${OPTARG}\"\n",
        "+            ;;\n",
        "         ?)\n",
        "         echo \"unknown para\"\n",
        "         exit 1;;\n",
        "@@ -1390,10 +1512,24 @@ models_for_process_only_config=${basepath}/../config/models_for_process_only.cfg\n",
        " models_tf_weightquant_config=${basepath}/../config/models_tf_weightquant.cfg\n",
        " models_codegen_config=${basepath}/../codegen/models_codegen.cfg\n",
        " models_codegen_parallel_config=${basepath}/../codegen/models_codegen_parallel.cfg\n",
        "+models_ms_train_config=${basepath}/../config/models_ms_train.cfg\n",
        " \n",
        " ms_models_path=${basepath}/ms_models\n",
        " build_path=${basepath}/codegen_build\n",
        " build_parallal_path=${basepath}/codegen_parallel_build\n",
        "+train_models_path\n",
        "+if [[ train_models_path == \"\" ]]\n",
        "+then\n",
        "+  echo \"train_io path is empty\"\n",
        "+  train_models_path=\"../${models_path}/models_train/\"\n",
        "+fi\n",
        "+echo train_models_path\n",
        "+if [[ $train_io_path == \"\" ]]\n",
        "+then\n",
        "+  echo \"train_io path is empty\"\n",
        "+  train_io_path=${train_models_path}/input_output\n",
        "+fi\n",
        "+echo $train_io_path\n",
        " \n",
        " # Write converter result to temp file\n",
        " run_converter_log_file=${basepath}/run_converter_log.txt\n",
        "@@ -1564,4 +1700,9 @@ Print_Benchmark_Result\n",
        " if [[ $isFailed == 1 ]]; then\n",
        "     exit 1\n",
        " fi\n",
        "+echo \"Run x86 train end\"\n",
        "+Print_Benchmark_Train_Result\n",
        "+if [[ $isFailed == 1 ]]; then\n",
        "+    exit 1\n",
        "+fi\n",
        " exit 0\n",
        "diff --git a/scripts/lite_release_package.sh b/scripts/lite_release_package.sh\n",
        "index 7e4e77f..442f59b 100644\n",
        "--- a/scripts/lite_release_package.sh\n",
        "+++ b/scripts/lite_release_package.sh\n",
        "@@ -50,78 +50,32 @@ function android_release_package()\n",
        " {\n",
        "     arch=$1\n",
        "     device=$2\n",
        "-    src_inference_pkg_name=\"mindspore-lite-${version}-inference-android-${arch}\"\n",
        "-    src_train_pkg_name=\"mindspore-lite-${version}-train-android-${arch}\"\n",
        "     dst_pkg_name=\"mindspore-lite-${version}-android-${arch}\"\n",
        "-\n",
        "-    rm -rf ${src_inference_pkg_name}\n",
        "-    rm -rf ${src_train_pkg_name}\n",
        "     rm -rf ${dst_pkg_name}\n",
        "-    tar -xzf ${input_path}/android_${arch}/${device}/${src_inference_pkg_name}.tar.gz\n",
        "-    tar -xzf ${input_path}/android_${arch}/${device}/${src_train_pkg_name}.tar.gz\n",
        "-\n",
        "-    cp -r ${src_train_pkg_name}/tools/benchmark_train/ ${src_inference_pkg_name}/tools/\n",
        "-    cp -r ${src_train_pkg_name}/train/ ${src_inference_pkg_name}/\n",
        "-    mkdir -p ${output_path}/release/android/${device}/\n",
        "-    mv ${src_inference_pkg_name} ${dst_pkg_name}\n",
        "-    # Copy java runtime to Android package\n",
        "-    cp ${input_path}/aar/avx/mindspore-lite-*maven*.zip ${dst_pkg_name}\n",
        "-    tar -czf ${output_path}/release/android/${device}/${dst_pkg_name}.tar.gz ${dst_pkg_name}\n",
        "+    mv ${input_path}/android_${arch}/${device}/${dst_pkg_name}.tar.gz ${output_path}/release/android/${device}/${dst_pkg_name}.tar.gz\n",
        "     cd ${output_path}/release/android/${device}/\n",
        "     sha256sum ${dst_pkg_name}.tar.gz > ${dst_pkg_name}.tar.gz.sha256\n",
        "-    cd -\n",
        "-\n",
        "-    verify_every_file ${src_train_pkg_name}/tools/benchmark_train/ ${dst_pkg_name}\n",
        "-    verify_every_file ${src_train_pkg_name}/train/ ${dst_pkg_name}\n",
        "-\n",
        "-    rm -rf ${src_train_pkg_name}\n",
        "-    rm -rf ${dst_pkg_name}\n",
        " }\n",
        " \n",
        " function linux_release_package()\n",
        " {\n",
        "-    src_inference_pkg_name=\"mindspore-lite-${version}-inference-linux-x64\"\n",
        "-    src_train_pkg_name=\"mindspore-lite-${version}-train-linux-x64\"\n",
        "-    dst_pkg_name=\"mindspore-lite-${version}-linux-x64\"\n",
        " \n",
        "-    rm -rf ${src_inference_pkg_name}\n",
        "-    rm -rf ${src_train_pkg_name}\n",
        "+    dst_pkg_name=\"mindspore-lite-${version}-linux-x64\"\n",
        "     rm -rf ${dst_pkg_name}\n",
        "-    tar -xzf ${input_path}/ubuntu_x86/avx/${src_inference_pkg_name}.tar.gz\n",
        "-    tar -xzf ${input_path}/ubuntu_x86/${src_train_pkg_name}.tar.gz\n",
        "-\n",
        "-    cp -r ${src_train_pkg_name}/tools/benchmark_train/ ${src_inference_pkg_name}/tools/\n",
        "-    cp -r ${src_train_pkg_name}/train/ ${src_inference_pkg_name}/\n",
        "-\n",
        "     mkdir -p ${output_path}/release/linux/\n",
        "-    mv ${src_inference_pkg_name} ${dst_pkg_name}\n",
        "-    tar -czf ${output_path}/release/linux/${dst_pkg_name}.tar.gz ${dst_pkg_name}\n",
        "+    mv ${input_path}/ubuntu_x86/${dst_pkg_name}.tar.gz ${output_path}/release/linux/\n",
        "     cd ${output_path}/release/linux/\n",
        "     sha256sum ${dst_pkg_name}.tar.gz > ${dst_pkg_name}.tar.gz.sha256\n",
        "-    cd -\n",
        "-\n",
        "-    verify_every_file ${src_train_pkg_name}/tools/benchmark_train/ ${dst_pkg_name}\n",
        "-    verify_every_file ${src_train_pkg_name}/train/ ${dst_pkg_name}\n",
        "-    rm -rf ${src_train_pkg_name}\n",
        "-    rm -rf ${dst_pkg_name}\n",
        " }\n",
        " \n",
        " function windows_release_package()\n",
        " {\n",
        "-    src_inference_pkg_name=\"mindspore-lite-${version}-inference-win-x64\"\n",
        "-    dst_pkg_name=\"mindspore-lite-${version}-win-x64\"\n",
        "+    pkg_name=\"mindspore-lite-${version}-win-x64\"\n",
        " \n",
        "-    rm -rf ${src_inference_pkg_name}\n",
        "-    rm -rf ${dst_pkg_name}\n",
        "-    unzip ${input_path}/windows_x64/avx/${src_inference_pkg_name}.zip\n",
        "-\n",
        "-    mv ${src_inference_pkg_name} ${dst_pkg_name}\n",
        "-    mkdir -p ${output_path}/release/windows/\n",
        "-    zip -r ${output_path}/release/windows/${dst_pkg_name}.zip ${dst_pkg_name}\n",
        "+    rm -rf ${pkg_name}\n",
        "+    mv  ${input_path}/windows_x64/avx/${pkg_name}.zip ${output_path}/release/windows/${dst_pkg_name}.zip\n",
        "     cd ${output_path}/release/windows/\n",
        "     sha256sum ${dst_pkg_name}.zip > ${dst_pkg_name}.zip.sha256\n",
        "-    cd -\n",
        "-    rm -rf ${dst_pkg_name}\n",
        " }\n",
        " \n",
        " echo \"============================== begin ==============================\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}