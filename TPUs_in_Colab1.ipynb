{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgnKioCim7Ht"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * distributed under the License is distributed on an AS\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/converter/parser/tf/tf_model_parser.h\"\n",
        "#include <map>\n",
        "#include \"src/common/log_adapter.h\"\n",
        "#include \"tf_util.h\"\n",
        "#include \"tools/common/graph_util.h\"\n",
        "#include \"tools/converter/parser/tf/tf_node_parser_registry.h\"\n",
        "#include \"src/param_value_lite.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "FuncGraphPtr TFModelParser::Parse(const std::string &modelFile, const std::string &weightFile,\n",
        "                                  const QuantType &quantType) {\n",
        "  auto status = ValidateFileStr(modelFile, \".prototxt\");\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"INPUT ILLEGAL: modelFile must be *.prototxt\";\n",
        "    ReturnCode::GetSingleReturnCode()->UpdateReturnCode(status);\n",
        "    return nullptr;\n",
        "  }\n",
        "  if (!TensorFlowUtils::TfReadProtoFromBinary(modelFile.c_str(), tf_graph_def.get())) {\n",
        "    MS_LOG(ERROR) << \"Open modelFile for TF converter failed!\";\n",
        "    ReturnCode::GetSingleReturnCode()->UpdateReturnCode(RET_ERROR);\n",
        "    return nullptr;\n",
        "  }\n",
        "  funcGraphPtr = std::make_shared<FuncGraph>();\n",
        "  status = ConvertGraphInputs();\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"Convert graph inputs failed.\";\n",
        "    ReturnCode::GetSingleReturnCode()->UpdateReturnCode(status);\n",
        "    return nullptr;\n",
        "  }\n",
        "  status = ConvertOps();\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"Convert ops failed.\";\n",
        "    ReturnCode::GetSingleReturnCode()->UpdateReturnCode(status);\n",
        "    return nullptr;\n",
        "  }\n",
        "\n",
        "  status = ConvertGraphOutputs();\n",
        "  if (status != RET_OK) {\n",
        "    MS_LOG(ERROR) << \"Convert graph outputs failed.\";\n",
        "    ReturnCode::GetSingleReturnCode()->UpdateReturnCode(status);\n",
        "    return nullptr;\n",
        "  }\n",
        "  return funcGraphPtr;\n",
        "}\n",
        "STATUS TFModelParser::ConvertConstTensor(const tensorflow::NodeDef *node, ParameterPtr parameter) {\n",
        "  tensorflow::AttrValue attr_value;\n",
        "  if (TensorFlowUtils::FindAttrValue(node, \"value\", attr_value)) {\n",
        "    tensorflow::AttrValue data_type;\n",
        "    tensorflow::DataType type;\n",
        "    // datatype\n",
        "    if (TensorFlowUtils::FindAttrValue(node, \"dtype\", data_type)) {\n",
        "      type = data_type.type();\n",
        "    }\n",
        "    const tensorflow::TensorProto &tensorProto = attr_value.tensor();\n",
        "    const tensorflow::TensorShapeProto &tensorShape = tensorProto.tensor_shape();\n",
        "    parameter = funcGraphPtr->add_parameter();\n",
        "    std::vector<int64_t> shape_vector;\n",
        "    int shape_size = 1;\n",
        "    shape_vector.resize(tensorShape.dim_size());\n",
        "    for (int i = 0; i < tensorShape.dim_size(); i++) {\n",
        "      shape_vector[i] = tensorShape.dim(i).size();\n",
        "      shape_size *= shape_vector[i];\n",
        "    }\n",
        "    // convert const to paramter\n",
        "    TypePtr ms_data_ype;\n",
        "    auto paramValue = std::make_shared<ParamValueLite>();\n",
        "    if (type == tensorflow::DT_FLOAT) {\n",
        "      ms_data_ype = kFloat32;\n",
        "      auto tensor_data = new(std::nothrow) float[shape_size];\n",
        "      if (tensorProto.float_val_size() == 1) {\n",
        "        float value = tensorProto.float_val(0);\n",
        "        for (size_t i = 0; i < shape_size; i++) {\n",
        "          tensor_data[i] = value;\n",
        "        }\n",
        "      }\n",
        "      if (tensorProto.tensor_content().size() == shape_size * sizeof(float)) {\n",
        "        const auto addr = reinterpret_cast<const float *>(tensorProto.tensor_content().data());\n",
        "        auto ret = ::memcpy_s(tensor_data, shape_size * sizeof(float), addr, shape_size * sizeof(float));\n",
        "        if (ret != EOK) {\n",
        "          MS_LOG(ERROR) << \"memcpy_s failed\";\n",
        "          return RET_ERROR;\n",
        "        }\n",
        "      }\n",
        "      paramValue->set_tensor_addr(tensor_data);\n",
        "      paramValue->set_tensor_size(shape_size * sizeof(float));\n",
        "    } else if (type == tensorflow::DT_INT32) {\n",
        "      ms_data_ype = kInt32;\n",
        "      auto tensor_data = new(std::nothrow) int[shape_size];\n",
        "      if (tensorProto.int_val_size() == 1) {\n",
        "        int value = tensorProto.int_val(0);\n",
        "        for (size_t i = 0; i < shape_size; i++) {\n",
        "          tensor_data[i] = value;\n",
        "        }\n",
        "      }\n",
        "      if (tensorProto.tensor_content().size() == shape_size * sizeof(int32_t)) {\n",
        "        const auto addr = reinterpret_cast<const int32_t *>(tensorProto.tensor_content().data());\n",
        "        auto ret = ::memcpy_s(tensor_data, shape_size * sizeof(int32_t), addr, shape_size * sizeof(int32_t));\n",
        "        if (ret != EOK) {\n",
        "          MS_LOG(ERROR) << \"memcpy_s failed\";\n",
        "          return RET_ERROR;\n",
        "        }\n",
        "      }\n",
        "      paramValue->set_tensor_addr(tensor_data);\n",
        "      paramValue->set_tensor_size(shape_size * sizeof(int));\n",
        "    } else if (type == tensorflow::DT_BOOL) {\n",
        "      ms_data_ype = kFloat32;\n",
        "      auto tensor_data = new(std::nothrow) int[shape_size];\n",
        "      if (tensorProto.bool_val_size() == 1) {\n",
        "        int value = tensorProto.bool_val(0);\n",
        "        for (size_t i = 0; i < shape_size; i++) {\n",
        "          tensor_data[i] = value;\n",
        "        }\n",
        "      }\n",
        "      paramValue->set_tensor_addr(tensor_data);\n",
        "      paramValue->set_tensor_size(shape_size * sizeof(int));\n",
        "    } else {\n",
        "      MS_LOG(ERROR) << \"Unsupport dataType,\" << node->name();\n",
        "      return RET_ERROR;\n",
        "    }\n",
        "    auto abstract_tensor = std::make_shared<abstract::AbstractTensor>(ms_data_ype, shape_vector);\n",
        "    parameter->set_abstract(abstract_tensor);\n",
        "    parameter->set_name(\"const_\" + std::to_string(anf_node_map.size()) + \"_parameter\");\n",
        "\n",
        "    std::vector<int> param_shape;\n",
        "    (void) std::transform(shape_vector.begin(), shape_vector.end(), std::back_inserter(param_shape),\n",
        "                          [](const int64_t &value) { return static_cast<int>(value); });\n",
        "\n",
        "    MS_ASSERT(paramValue != nullptr);\n",
        "    paramValue->set_tensor_shape(param_shape);\n",
        "    paramValue->set_tensor_type(ms_data_ype->type_id());\n",
        "    paramValue->set_format(schema::Format::Format_NHWC);\n",
        "    paramValue->set_tensor_size(shape_size * sizeof(int));\n",
        "    parameter->set_default_param(paramValue);\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS TFModelParser::ConvertOutputTensor(const tensorflow::NodeDef *op, const CNodePtr &anf_node, int &output_size) {\n",
        "  if (output_size == 1) {\n",
        "    std::vector<int64_t> shape_vector;\n",
        "    anf_node->set_abstract(std::make_shared<abstract::AbstractTensor>(kFloat32, shape_vector));\n",
        "    anf_node_map.insert(std::pair(op->name(), anf_node));\n",
        "  } else {\n",
        "    AbstractBasePtrList abstractList;\n",
        "    for (int output_idx = 0; output_idx < output_size; output_idx++) {\n",
        "      std::vector<int64_t> shape_vector;\n",
        "      abstractList.emplace_back(std::make_shared<abstract::AbstractTensor>(kFloat32, shape_vector));\n",
        "      auto tupleGetItemPrimPtr = GetTupleGetItemPrim();\n",
        "      if (tupleGetItemPrimPtr == nullptr) {\n",
        "        MS_LOG(ERROR) << \"GetTupleGetItemPrim return nullptr\";\n",
        "        return RET_NULL_PTR;\n",
        "      }\n",
        "      auto tupleGetItemPrim = NewValueNode(tupleGetItemPrimPtr);\n",
        "      auto getItemValue = NewValueNode(MakeValue<int>(output_idx));\n",
        "      std::vector<AnfNodePtr> inputs{tupleGetItemPrim, anf_node, getItemValue};\n",
        "      CNodePtr getItemCNode = funcGraphPtr->NewCNode(inputs);\n",
        "      std::string output_item_name = anf_node->fullname_with_scope() + \"_getitem_\" + std::to_string(output_idx);\n",
        "      getItemCNode->set_fullname_with_scope(output_item_name);\n",
        "      anf_node_map.insert(std::pair(output_item_name, getItemCNode));\n",
        "    }\n",
        "    anf_node->set_abstract(std::make_shared<abstract::AbstractTuple>(abstractList));\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS TFModelParser::ConvertOps() {\n",
        "  NoSupportOp::GetInstance()->SetFmkType(\"TENSORFLOW\");\n",
        "  STATUS status = RET_OK;\n",
        "\n",
        "  // redirect identity to it's input0\n",
        "  ClipIdentityAndStopGradient();\n",
        "  int op_idx = 0;\n",
        "  for (int i = 0; i < tf_graph_def->node_size(); i++) {\n",
        "    auto node_def = tf_graph_def->mutable_node(i);\n",
        "    tf_node_map[node_def->name()] = node_def;\n",
        "    auto tf_op_type = node_def->op();\n",
        "    if (tf_op_type == \"Placeholder\" || tf_op_type == \"Const\") {\n",
        "      continue;\n",
        "    }\n",
        "    auto node_parser = TFNodeParserRegistry::GetInstance()->GetNodeParser(tf_op_type);\n",
        "    if (node_parser == nullptr) {\n",
        "      NoSupportOp::GetInstance()->InsertOp(tf_op_type);\n",
        "      status = (status == RET_OK ? RET_NOT_FIND_OP : status);\n",
        "      MS_LOG(ERROR) << \"cannot find node parser:\" << tf_op_type;\n",
        "      continue;\n",
        "    }\n",
        "    PrimitiveC *primitiveC = nullptr;\n",
        "    int output_size = 1;\n",
        "    if (status == RET_OK) {\n",
        "      status = node_parser->Parse(node_def, tf_graph_def, primitiveC, output_size);\n",
        "      if (status != RET_OK) {\n",
        "        MS_LOG(ERROR) << \"node \" << tf_op_type.c_str() << \" parser failed\";\n",
        "        continue;\n",
        "      }\n",
        "      std::vector<AnfNodePtr> opInputs = {NewValueNode(std::shared_ptr<PrimitiveC>(primitiveC))};\n",
        "      // parse inputs\n",
        "      for (int j = 0; j < node_def->input_size(); j++) {\n",
        "        auto input_node = tf_node_map[node_def->input(i)];\n",
        "        // last node output\n",
        "        if (anf_node_map.find(input_node->name()) != anf_node_map.end()) {\n",
        "          opInputs.emplace_back(anf_node_map[input_node->name()]);\n",
        "          continue;\n",
        "        }\n",
        "        // const tensor\n",
        "        if (input_node->op() == \"Const\") {\n",
        "          ParameterPtr parameter;\n",
        "          if (ConvertConstTensor(input_node, parameter) != RET_OK) {\n",
        "            MS_LOG(ERROR) << \"convert const tensor failed,\" << input_node->name();\n",
        "            return RET_ERROR;\n",
        "          }\n",
        "          opInputs.emplace_back(parameter);\n",
        "          anf_node_map[parameter->fullname_with_scope()] = parameter;\n",
        "          continue;\n",
        "        }\n",
        "        MS_LOG(ERROR) << \"node\" << node_def->name() << \"has inputs neither a node output nor a weight tensor.\";\n",
        "        return RET_ERROR;\n",
        "      }\n",
        "      auto anf_node = funcGraphPtr->NewCNode(opInputs);\n",
        "      anf_node->set_fullname_with_scope(tf_op_type + \"-\" + std::to_string(op_idx++));\n",
        "\n",
        "      // parse outputs\n",
        "      status = ConvertOutputTensor(node_def, anf_node, output_size);\n",
        "      if (status != RET_OK) {\n",
        "        MS_LOG(ERROR) << \"Convert output tensors for \" << anf_node->fullname_with_scope() << \" failed.\";\n",
        "        ReturnCode::GetSingleReturnCode()->UpdateReturnCode(status);\n",
        "        return status;\n",
        "      }\n",
        "    }\n",
        "    // redirect identity to it's input0\n",
        "    ClipIdentityAndStopGradient();\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS TFModelParser::ConvertGraphInputs() {\n",
        "  for (int i = 0; i < tf_graph_def->node_size(); i++) {\n",
        "    auto node_def = tf_graph_def->mutable_node(i);\n",
        "    tf_node_map[node_def->name()] = node_def;\n",
        "    if (node_def->op() == \"Placeholder\") {\n",
        "      auto parameter = funcGraphPtr->add_parameter();\n",
        "      if (ConvertConstTensor(node_def, parameter) != RET_OK) {\n",
        "        MS_LOG(ERROR) << \"convert const tensor failed\";\n",
        "        return RET_ERROR;\n",
        "      }\n",
        "      anf_node_map[node_def->name()] = parameter;\n",
        "      graph_input_names.emplace_back(node_def->name());\n",
        "    }\n",
        "  }\n",
        "  return RET_OK;\n",
        "}\n",
        "STATUS TFModelParser::ConvertGraphOutputs() {\n",
        "  return RET_OK;\n",
        "}\n",
        "\n",
        "std::string TFModelParser::GetOriginInputName(const tensorflow::NodeDef &node) {\n",
        "  if (node.op() != \"Identity\" && node.op() != \"StopGradient\") {\n",
        "    return node.name();\n",
        "  }\n",
        "  auto tmpNode = node;\n",
        "  while (tmpNode.op() == \"Identity\" || tmpNode.op() == \"StopGradient\") {\n",
        "    tmpNode = *tf_node_map[tmpNode.input(0)];\n",
        "  }\n",
        "  return tmpNode.name();\n",
        "}\n",
        "\n",
        "void TFModelParser::ClipIdentityAndStopGradient() {\n",
        "  for (auto &pair : tf_node_map) {\n",
        "    pair.second = tf_node_map[GetOriginInputName(*pair.second)];\n",
        "  }\n",
        "}\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6OR7UU9k8Ml"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/converter/parser/tf/tf_add_parser.h\"\n",
        "#include <string>\n",
        "#include \"tools/converter/parser/tf/tf_node_parser_registry.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "STATUS TFAddParser::Parse(const tensorflow::NodeDef *tf_op, const std::unique_ptr<tensorflow::GraphDef> &tf_model,\n",
        "                          PrimitiveC *primitiveC, int &output_size) {\n",
        "  auto attr = std::make_unique<schema::PrimitiveT>();\n",
        "  attr->value.type = schema::PrimitiveType_Add;\n",
        "  primitiveC = PrimitiveC::Create(attr.release());\n",
        "  MS_LOG(INFO) << \"primitive name\" << primitiveC->type_name();\n",
        "  return RET_OK;\n",
        "}\n",
        "TFNodeRegistrar g_tfAddParser(\"Add\", new TFAddParser());\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYCXgdctk_l8"
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * distributed under the License is distributed on an AS\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"tools/converter/parser/tf/tf_node_parser_registry.h\"\n",
        "#include <map>\n",
        "#include \"src/common/log_adapter.h\"\n",
        "\n",
        "namespace mindspore {\n",
        "namespace lite {\n",
        "TFNodeParserRegistry::~TFNodeParserRegistry() {\n",
        "  for (const auto &iter : parsers) {\n",
        "    delete iter.second;\n",
        "  }\n",
        "  this->parsers.clear();\n",
        "}\n",
        "\n",
        "TFNodeParserRegistry *TFNodeParserRegistry::GetInstance() {\n",
        "  static TFNodeParserRegistry instance;\n",
        "  return &instance;\n",
        "}\n",
        "\n",
        "TFNodeParser *TFNodeParserRegistry::GetNodeParser(const std::string &name) {\n",
        "  auto it = parsers.find(name);\n",
        "  if (it != parsers.end()) {\n",
        "    return it->second;\n",
        "  }\n",
        "  return nullptr;\n",
        "}\n",
        "}  // namespace lite\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e_AU97aLkOd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}