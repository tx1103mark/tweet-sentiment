{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs in Colab",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TPUs_in_Colab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIR6iAnttgJk"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWkkNU4fGmLt"
      },
      "source": [
        "From 1ce16998e092ecffcc5fa2cfffee6544c35fb3f2 Mon Sep 17 00:00:00 2001\n",
        "From: zhengjun10 <zhengjun10@huawei.com>\n",
        "Date: Sat, 5 Jun 2021 14:48:11 +0800\n",
        "Subject: [PATCH] master code standard rectification\n",
        "\n",
        "---\n",
        " .../export_models/models/densenet_train_export.py  | 39 +++++++++++++++++++---\n",
        " .../lite/examples/export_models/models/effnet.py   | 29 ++++++++++------\n",
        " .../export_models/models/effnet_train_export.py    |  6 ++--\n",
        " .../models/effnet_tune_train_export.py             | 12 ++++---\n",
        " .../export_models/models/googlenet_train_export.py |  6 ++--\n",
        " .../export_models/models/lenet_train_export.py     |  6 ++--\n",
        " .../examples/export_models/models/mini_alexnet.py  |  3 ++\n",
        " .../models/mini_alexnet_train_export.py            |  6 ++--\n",
        " .../models/mobilenetv1_train_export.py             |  6 ++--\n",
        " .../models/mobilenetv2_train_export.py             |  6 ++--\n",
        " .../models/mobilenetv3_train_export.py             |  6 ++--\n",
        " .../export_models/models/nin_train_export.py       |  6 ++--\n",
        " .../export_models/models/resnet_train_export.py    |  6 ++--\n",
        " .../models/shufflenetv2_train_export.py            |  6 ++--\n",
        " .../examples/export_models/models/train_utils.py   | 20 ++++++-----\n",
        " .../export_models/models/vgg_train_export.py       |  6 ++--\n",
        " .../export_models/models/xception_train_export.py  |  6 ++--\n",
        " .../examples/train_lenet/model/lenet_export.py     |  4 +--\n",
        " .../lite/examples/train_lenet/model/train_utils.py |  5 +--\n",
        " .../lite/examples/train_lenet/src/net_runner.cc    | 31 +++++++++++------\n",
        " .../examples/transfer_learning/model/effnet.py     | 16 +++++++--\n",
        " .../transfer_learning/model/train_utils.py         |  4 +--\n",
        " .../model/transfer_learning_export.py              |  4 +--\n",
        " .../lite/examples/transfer_learning/src/dataset.cc | 20 +++++++----\n",
        " .../examples/transfer_learning/src/net_runner.cc   | 18 ++++++----\n",
        " mindspore/lite/include/train/accuracy_metrics.h    |  1 +\n",
        " .../train/classification_train_accuracy_monitor.h  |  5 ++-\n",
        " mindspore/lite/src/huffman_decode.cc               | 11 +++---\n",
        " mindspore/lite/src/huffman_decode.h                |  2 +-\n",
        " .../lite/src/ops/populate/arithmetic_populate.cc   |  2 ++\n",
        " .../lite/src/ops/populate/bias_grad_populate.cc    | 39 ----------------------\n",
        " mindspore/lite/src/train/accuracy_metrics.cc       |  2 --\n",
        " mindspore/lite/src/train/accuracy_monitor.cc       |  2 --\n",
        " .../train/classification_train_accuracy_monitor.cc | 34 +++++++------------\n",
        " mindspore/lite/src/train/loss_monitor.cc           |  6 ++--\n",
        " mindspore/lite/src/train/lr_scheduler.cc           |  2 --\n",
        " mindspore/lite/src/train/train_loop.cc             | 27 ++-------------\n",
        " mindspore/lite/src/train/train_loop.h              |  1 -\n",
        " .../lite/src/train/train_populate_parameter_v0.cc  |  2 --\n",
        " mindspore/lite/src/weight_decoder.cc               |  2 +-\n",
        " 40 files changed, 207 insertions(+), 208 deletions(-)\n",
        " delete mode 100644 mindspore/lite/src/ops/populate/bias_grad_populate.cc\n",
        "\n",
        "diff --git a/mindspore/lite/examples/export_models/models/densenet_train_export.py b/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "index dd46e32..a8c85a3 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/densenet_train_export.py\n",
        "@@ -17,14 +17,43 @@\n",
        " import sys\n",
        " import os\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        " from mindspore.train.serialization import export\n",
        "+<<<<<<< Updated upstream\n",
        "+<<<<<<< Updated upstream\n",
        "+<<<<<<< Updated upstream\n",
        "+<<<<<<< Updated upstream\n",
        "+from src.network.densenet import DenseNet121\n",
        "+sys.path.append(os.environ['CLOUD_MODEL_ZOO'] + 'official/cv/densenet121/')\n",
        "+#pylint: disable=wrong-import-position\n",
        "+\n",
        " \n",
        "-sys.path.append(os.environ['CLOUD_MODEL_ZOO'] + 'official/cv/densenet/')\n",
        "+=======\n",
        "+=======\n",
        "+>>>>>>> Stashed changes\n",
        "+=======\n",
        "+>>>>>>> Stashed changes\n",
        "+from official.cv.densenet121.src.network.densenet import DenseNet121\n",
        "+sys.path.append(os.environ['CLOUD_MODEL_ZOO'] + 'official/cv/densenet121/')\n",
        " #pylint: disable=wrong-import-position\n",
        "-from src.network.densenet import DenseNet121\n",
        "+\n",
        "+<<<<<<< Updated upstream\n",
        "+<<<<<<< Updated upstream\n",
        "+>>>>>>> Stashed changes\n",
        "+=======\n",
        "+from official.cv.densenet121.src.network.densenet import DenseNet121\n",
        "+sys.path.append(os.environ['CLOUD_MODEL_ZOO'] + 'official/cv/densenet121/')\n",
        "+#pylint: disable=wrong-import-position\n",
        "+\n",
        "+>>>>>>> Stashed changes\n",
        "+=======\n",
        "+>>>>>>> Stashed changes\n",
        "+=======\n",
        "+>>>>>>> Stashed changes\n",
        "+\n",
        "+\n",
        " \n",
        " \n",
        " context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\", save_graphs=False)\n",
        "@@ -33,7 +62,7 @@ n = DenseNet121(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.001, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=0.9)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -41,4 +70,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/densenet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"densenet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"densenet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/effnet.py b/mindspore/lite/examples/export_models/models/effnet.py\n",
        "index 4971757..fc49872 100755\n",
        "--- a/mindspore/lite/examples/export_models/models/effnet.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/effnet.py\n",
        "@@ -20,6 +20,7 @@ from mindspore.ops import operations as P\n",
        " from mindspore.common.initializer import TruncatedNormal\n",
        " from mindspore import Tensor\n",
        " \n",
        "+\n",
        " def weight_variable():\n",
        "     \"\"\"weight initial\"\"\"\n",
        "     return TruncatedNormal(0.02)\n",
        "@@ -40,6 +41,7 @@ def _make_value_divisible(value, factor, min_value=None):\n",
        "         new_value += factor\n",
        "     return new_value\n",
        " \n",
        "+\n",
        " class Swish(nn.Cell):\n",
        "     def __init__(self):\n",
        "         super().__init__()\n",
        "@@ -58,7 +60,8 @@ class AdaptiveAvgPool(nn.Cell):\n",
        "         self.output_size = output_size\n",
        " \n",
        "     def construct(self, x):\n",
        "-        return self.mean(x, (2, 3)) ## This is not a general case\n",
        "+        return self.mean(x, (2, 3)) # This is not a general case\n",
        "+\n",
        " \n",
        " class SELayer(nn.Cell):\n",
        "     \"\"\"SELayer\"\"\"\n",
        "@@ -74,24 +77,27 @@ class SELayer(nn.Cell):\n",
        "         self.act2 = nn.Sigmoid()\n",
        " \n",
        "     def construct(self, x):\n",
        "-        o = self.avg_pool(x) #.view(b,c)\n",
        "+        o = self.avg_pool(x) # .view(b,c)\n",
        "         o = self.conv_reduce(o)\n",
        "         o = self.act1(o)\n",
        "         o = self.conv_expand(o)\n",
        "-        o = self.act2(o) #.view(b, c, 1,1)\n",
        "+        o = self.act2(o) # .view(b, c, 1,1)\n",
        "         return x * o\n",
        " \n",
        "+\n",
        " class DepthwiseSeparableConv(nn.Cell):\n",
        "     \"\"\"DepthwiseSeparableConv\"\"\"\n",
        "     def __init__(self, in_chs, out_chs, dw_kernel_size=3, stride=1, noskip=False, se_ratio=0.0, drop_connect_rate=0.0):\n",
        "         super().__init__()\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR stride param\")\n",
        "+            return\n",
        "         self.has_residual = (stride == 1 and in_chs == out_chs) and not noskip\n",
        "         self.drop_connect_rate = drop_connect_rate\n",
        " \n",
        "         self.conv_dw = nn.Conv2d(in_channels=in_chs, out_channels=in_chs, kernel_size=dw_kernel_size, stride=stride,\n",
        "                                  pad_mode=\"pad\", padding=1, has_bias=False, group=in_chs)\n",
        "-        self.bn1 = nn.BatchNorm2d(in_chs, eps=0.001) #,momentum=0.1)\n",
        "+        self.bn1 = nn.BatchNorm2d(in_chs, eps=0.001) # momentum=0.1)\n",
        "         self.act1 = Swish()\n",
        " \n",
        "        # Squeeze-and-excitation\n",
        "@@ -101,7 +107,7 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "             print(\"ERRRRRORRRR -- not prepared for this one\\n\")\n",
        " \n",
        "         self.conv_pw = nn.Conv2d(in_channels=in_chs, out_channels=out_chs, kernel_size=1, stride=stride, has_bias=False)\n",
        "-        self.bn2 = nn.BatchNorm2d(out_chs, eps=0.001) #,momentum=0.1)\n",
        "+        self.bn2 = nn.BatchNorm2d(out_chs, eps=0.001) # momentum=0.1)\n",
        " \n",
        "     def construct(self, x):\n",
        "         \"\"\"construct\"\"\"\n",
        "@@ -120,12 +126,13 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "             x += residual\n",
        "         return x\n",
        " \n",
        "+\n",
        " def conv_3x3_bn(inp, oup, stride):\n",
        "     weight = weight_variable()\n",
        "     return nn.SequentialCell([\n",
        "         nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=3, stride=stride, padding=1, weight_init=weight,\n",
        "                   has_bias=False, pad_mode='pad'),\n",
        "-        nn.BatchNorm2d(oup, eps=0.001),  #, momentum=0.1),\n",
        "+        nn.BatchNorm2d(oup, eps=0.001),  # momentum=0.1),\n",
        "         nn.HSwish()])\n",
        " \n",
        " \n",
        "@@ -142,7 +149,9 @@ class InvertedResidual(nn.Cell):\n",
        "     \"\"\"InvertedResidual\"\"\"\n",
        "     def __init__(self, in_chs, out_chs, kernel_size, stride, padding, expansion, se_ratio):\n",
        "         super().__init__()\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR stride param\")\n",
        "+            return\n",
        "         mid_chs: int = _make_value_divisible(in_chs * expansion, 1)\n",
        "         self.has_residual = (in_chs == out_chs and stride == 1)\n",
        "         self.drop_connect_rate = 0\n",
        "@@ -210,7 +219,7 @@ class EfficientNet(nn.Cell):\n",
        " \n",
        "         self.conv_stem = nn.Conv2d(in_channels=3, out_channels=stem_size, kernel_size=3, stride=2, has_bias=False)\n",
        " \n",
        "-        self.bn1 = nn.BatchNorm2d(stem_size, eps=0.001) #momentum=0.1)\n",
        "+        self.bn1 = nn.BatchNorm2d(stem_size, eps=0.001) # momentum=0.1)\n",
        "         self.act1 = Swish()\n",
        "         in_chs = stem_size\n",
        " \n",
        "@@ -240,7 +249,7 @@ class EfficientNet(nn.Cell):\n",
        "         self.blocks = nn.SequentialCell(layers)\n",
        " \n",
        "         self.conv_head = nn.Conv2d(in_channels=320, out_channels=self.num_features_, kernel_size=1)\n",
        "-        self.bn2 = nn.BatchNorm2d(self.num_features_, eps=0.001) #,momentum=0.1)\n",
        "+        self.bn2 = nn.BatchNorm2d(self.num_features_, eps=0.001) # momentum=0.1)\n",
        "         self.act2 = Swish()\n",
        "         self.global_pool = AdaptiveAvgPool(output_size=(1, 1))\n",
        "         self.classifier = nn.Dense(self.num_features_, num_classes)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/effnet_train_export.py b/mindspore/lite/examples/export_models/models/effnet_train_export.py\n",
        "index bf341f2..3384cc2 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/effnet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/effnet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from effnet import effnet\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,11 +28,11 @@ n = effnet(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(2, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([2, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/effnet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"effnet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"effnet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py b/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py\n",
        "index 2b21ee8..3e61b44 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/effnet_tune_train_export.py\n",
        "@@ -17,7 +17,7 @@\n",
        " import sys\n",
        " from os import path\n",
        " import numpy as np\n",
        "-from train_utils import TrainWrap, SaveT\n",
        "+from train_utils import train_wrap, save_t\n",
        " from effnet import effnet\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -26,11 +26,13 @@ from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        " context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\", save_graphs=False)\n",
        " \n",
        "+\n",
        " class TransferNet(nn.Cell):\n",
        "     def __init__(self, backbone, head):\n",
        "         super().__init__(TransferNet)\n",
        "         self.backbone = backbone\n",
        "         self.head = head\n",
        "+\n",
        "     def construct(self, x):\n",
        "         x = self.backbone(x)\n",
        "         x = self.head(x)\n",
        "@@ -56,7 +58,7 @@ trainable_weights_list.extend(n.head.trainable_params())\n",
        " trainable_weights = ParameterTuple(trainable_weights_list)\n",
        " sgd = nn.SGD(trainable_weights, learning_rate=0.01, momentum=0.9,\n",
        "              dampening=0.01, weight_decay=0.0, nesterov=False, loss_scale=1.0)\n",
        "-net = TrainWrap(n, optimizer=sgd, weights=trainable_weights)\n",
        "+net = train_wrap(n, optimizer=sgd, weights=trainable_weights)\n",
        " \n",
        " BATCH_SIZE = 8\n",
        " X = Tensor(np.random.randn(BATCH_SIZE, 3, 224, 224), mstype.float32)\n",
        "@@ -66,10 +68,10 @@ export(net, X, label, file_name=\"mindir/effnet_tune_train\", file_format='MINDIR'\n",
        " if len(sys.argv) > 1:\n",
        "     name_prefix = sys.argv[1] + \"effnet_tune\"\n",
        "     x_name = name_prefix + \"_input1.bin\"\n",
        "-    SaveT(Tensor(X.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        "+    save_t(Tensor(X.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        " \n",
        "     l_name = name_prefix + \"_input2.bin\"\n",
        "-    SaveT(label, l_name)\n",
        "+    save_t(label, l_name)\n",
        " \n",
        "     #train network\n",
        "     n.head.set_train(True)\n",
        "@@ -80,4 +82,4 @@ if len(sys.argv) > 1:\n",
        "     n.set_train(False)\n",
        "     y = n(X)\n",
        "     y_name = name_prefix + \"_output1.bin\"\n",
        "-    SaveT(y, y_name)\n",
        "+    save_t(y, y_name)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/googlenet_train_export.py b/mindspore/lite/examples/export_models/models/googlenet_train_export.py\n",
        "index c2ddcc2..91a0062 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/googlenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/googlenet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.googlenet.src.googlenet import GoogleNet\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = GoogleNet(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=5e-4,\n",
        "                    nesterov=True, loss_scale=0.9)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/googlenet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"googlenet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"googlenet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/lenet_train_export.py b/mindspore/lite/examples/export_models/models/lenet_train_export.py\n",
        "index 1b7dfda..4e03aab 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/lenet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/lenet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.lenet.src.lenet import LeNet5\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,11 +28,11 @@ n = LeNet5()\n",
        " loss_fn = nn.MSELoss()\n",
        " optimizer = nn.Adam(n.trainable_params(), learning_rate=1e-2, beta1=0.5, beta2=0.7, eps=1e-2, use_locking=True,\n",
        "                     use_nesterov=False, weight_decay=0.0, loss_scale=0.3)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(32, 1, 32, 32), mstype.float32)\n",
        " label = Tensor(np.zeros([32, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/lenet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"lenet\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"lenet\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mini_alexnet.py b/mindspore/lite/examples/export_models/models/mini_alexnet.py\n",
        "index 9a8b828..e6008fa 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mini_alexnet.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mini_alexnet.py\n",
        "@@ -17,13 +17,16 @@\n",
        " import mindspore.nn as nn\n",
        " from mindspore.ops import operations as P\n",
        " \n",
        "+\n",
        " def conv(in_channels, out_channels, kernel_size, stride=1, padding=0, pad_mode=\"valid\", has_bias=True):\n",
        "     return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                      has_bias=has_bias, pad_mode=pad_mode)\n",
        " \n",
        "+\n",
        " def fc_with_initialize(input_channels, out_channels, has_bias=True):\n",
        "     return nn.Dense(input_channels, out_channels, has_bias=has_bias)\n",
        " \n",
        "+\n",
        " class AlexNet(nn.Cell):\n",
        "     \"\"\"\n",
        "     Alexnet\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py b/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py\n",
        "index 544daad..1b9a82d 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mini_alexnet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from mini_alexnet import AlexNet\n",
        " from mindspore import context, Tensor, nn\n",
        " from mindspore.train.serialization import export\n",
        "@@ -31,11 +31,11 @@ n = AlexNet(phase='test')\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Adam(n.trainable_params(), learning_rate=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, use_locking=False,\n",
        "                     use_nesterov=False, weight_decay=0.0, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.ones([batch, 1, 32, 32]).astype(np.float32) * 0.01)\n",
        " label = Tensor(np.zeros([batch, number_of_classes]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mini_alexnet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mini_alexnet\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"mini_alexnet\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py b/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py\n",
        "index f668a96..b3b26de 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mobilenetv1_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.mobilenetv1.src.mobilenet_v1 import MobileNetV1\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = MobileNetV1(10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=1e-2, momentum=0.9, dampening=0.1, weight_decay=0.0,\n",
        "                    nesterov=False, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -37,4 +37,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mobilenetv1_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mobilenetv1\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"mobilenetv1\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py b/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py\n",
        "index 8f1d543..0433063 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mobilenetv2_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.mobilenetv2.src.mobilenetV2 import MobileNetV2Backbone, MobileNetV2Head, mobilenet_v2\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -31,11 +31,11 @@ n = mobilenet_v2(backbone_net, head_net)\n",
        " \n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Momentum(n.trainable_params(), 0.01, 0.9, use_nesterov=False)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mobilenetv2_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mobilenetv2\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"mobilenetv2\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py b/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py\n",
        "index 26a6671..1f75252 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/mobilenetv3_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.mobilenetv3.src.mobilenetV3 import mobilenet_v3_small\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = mobilenet_v3_small(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')\n",
        " optimizer = nn.Adam(n.trainable_params(), learning_rate=1e-3, beta1=0.5, beta2=0.7, eps=1e-2, use_locking=True,\n",
        "                     use_nesterov=False, weight_decay=0.1, loss_scale=0.3)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/mobilenetv3_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"mobilenetv3\", x, label, n, net, sparse=False)\n",
        "+    save_inout(sys.argv[1] + \"mobilenetv3\", x, label, n, net, sparse=False)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/nin_train_export.py b/mindspore/lite/examples/export_models/models/nin_train_export.py\n",
        "index 72ccc5e..786f739 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/nin_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/nin_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from NetworkInNetwork import NiN\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = NiN(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=5e-4,\n",
        "                    nesterov=True, loss_scale=0.9)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 32, 32), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch]).astype(np.int32))\n",
        " export(net, x, label, file_name=\"mindir/nin_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"nin\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"nin\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/resnet_train_export.py b/mindspore/lite/examples/export_models/models/resnet_train_export.py\n",
        "index c0dbe90..c18bcf3 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/resnet_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/resnet_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.resnet.src.resnet import resnet50\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -29,11 +29,11 @@ n = resnet50(class_num=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/resnet_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"resnet\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"resnet\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py b/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py\n",
        "index 97aa4ec..bf76d48 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/shufflenetv2_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.shufflenetv2.src.shufflenetv2 import ShuffleNetV2\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -28,7 +28,7 @@ n = ShuffleNetV2(n_class=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Momentum(n.trainable_params(), 0.01, 0.9, use_nesterov=False)\n",
        " \n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        "@@ -36,4 +36,4 @@ label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/shufflenetv2_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"shufflenetv2\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"shufflenetv2\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/train_utils.py b/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "index e32fda1..5017b8f 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/train_utils.py\n",
        "@@ -14,11 +14,13 @@\n",
        " # ============================================================================\n",
        " \"\"\"train_utils.\"\"\"\n",
        " \n",
        "+import os\n",
        " from mindspore import nn, Tensor\n",
        " from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        "-def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "-    \"\"\"TrainWrap\"\"\"\n",
        "+\n",
        "+def train_wrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "+    \"\"\"train_wrap\"\"\"\n",
        "     if loss_fn is None:\n",
        "         loss_fn = nn.SoftmaxCrossEntropyWithLogits()\n",
        "     loss_net = nn.WithLossCell(net, loss_fn)\n",
        "@@ -32,22 +34,22 @@ def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "     return train_net\n",
        " \n",
        " \n",
        "-def SaveT(t, file):\n",
        "+def save_t(t, file):\n",
        "     x = t.asnumpy()\n",
        "     x.tofile(file)\n",
        " \n",
        " \n",
        "-def SaveInOut(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "-    \"\"\"SaveInOut\"\"\"\n",
        "+def save_inout(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "+    \"\"\"save_inout\"\"\"\n",
        "     x_name = name + \"_input1.bin\"\n",
        "     if sparse:\n",
        "         x_name = name + \"_input2.bin\"\n",
        "-    SaveT(Tensor(x.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        "+    save_t(Tensor(x.asnumpy().transpose(0, 2, 3, 1)), x_name)\n",
        " \n",
        "     l_name = name + \"_input2.bin\"\n",
        "     if sparse:\n",
        "         l_name = name + \"_input1.bin\"\n",
        "-    SaveT(l, l_name)\n",
        "+    save_t(l, l_name)\n",
        " \n",
        "     net.set_train(False)\n",
        "     y = net(x)\n",
        "@@ -62,10 +64,10 @@ def SaveInOut(name, x, l, net, net_train, sparse=False, epoch=1):\n",
        "     if isinstance(y, tuple):\n",
        "         i = 1\n",
        "         for t in y:\n",
        "-            with open(name + \"_output\" + str(i) + \".bin\", 'w') as f:\n",
        "+            with os.fdopen(name + \"_output\" + str(i) + \".bin\", 'w') as f:\n",
        "                 for j in t.asnumpy().flatten():\n",
        "                     f.write(str(j)+' ')\n",
        "             i = i + 1\n",
        "     else:\n",
        "         y_name = name + \"_output1.bin\"\n",
        "-        SaveT(y, y_name)\n",
        "+        save_t(y, y_name)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/vgg_train_export.py b/mindspore/lite/examples/export_models/models/vgg_train_export.py\n",
        "index 0078252..c18b33c 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/vgg_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/vgg_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.vgg16.src.vgg import vgg16\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -29,11 +29,11 @@ batch = 2\n",
        " n = vgg16(num_classes=10)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.Momentum(n.trainable_params(), 0.01, 0.9, use_nesterov=False)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " x = Tensor(np.random.randn(batch, 3, 224, 224), mstype.float32)\n",
        " label = Tensor(np.zeros([batch, 10]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/vgg_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"vgg\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"vgg\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/export_models/models/xception_train_export.py b/mindspore/lite/examples/export_models/models/xception_train_export.py\n",
        "index 6b82b3b..e544d7e 100644\n",
        "--- a/mindspore/lite/examples/export_models/models/xception_train_export.py\n",
        "+++ b/mindspore/lite/examples/export_models/models/xception_train_export.py\n",
        "@@ -16,7 +16,7 @@\n",
        " \n",
        " import sys\n",
        " import numpy as np\n",
        "-from train_utils import SaveInOut, TrainWrap\n",
        "+from train_utils import save_inout, train_wrap\n",
        " from official.cv.xception.src.Xception import Xception\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore import context, Tensor, nn\n",
        "@@ -31,7 +31,7 @@ n.dropout = nn.Dropout(keep_prob=1.0)\n",
        " loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False)\n",
        " optimizer = nn.SGD(n.trainable_params(), learning_rate=0.01, momentum=0.9, dampening=0.0, weight_decay=0.0,\n",
        "                    nesterov=True, loss_scale=1.0)\n",
        "-net = TrainWrap(n, loss_fn, optimizer)\n",
        "+net = train_wrap(n, loss_fn, optimizer)\n",
        " \n",
        " batch = 2\n",
        " x = Tensor(np.random.randn(batch, 3, 299, 299), mstype.float32)\n",
        "@@ -39,4 +39,4 @@ label = Tensor(np.zeros([batch, 1000]).astype(np.float32))\n",
        " export(net, x, label, file_name=\"mindir/xception_train\", file_format='MINDIR')\n",
        " \n",
        " if len(sys.argv) > 1:\n",
        "-    SaveInOut(sys.argv[1] + \"xception\", x, label, n, net)\n",
        "+    save_inout(sys.argv[1] + \"xception\", x, label, n, net)\n",
        "diff --git a/mindspore/lite/examples/train_lenet/model/lenet_export.py b/mindspore/lite/examples/train_lenet/model/lenet_export.py\n",
        "index 8a9cd7c..c774887 100644\n",
        "--- a/mindspore/lite/examples/train_lenet/model/lenet_export.py\n",
        "+++ b/mindspore/lite/examples/train_lenet/model/lenet_export.py\n",
        "@@ -19,7 +19,7 @@ from mindspore import context, Tensor\n",
        " import mindspore.common.dtype as mstype\n",
        " from mindspore.train.serialization import export\n",
        " from lenet import LeNet5\n",
        "-from train_utils import TrainWrap\n",
        "+from train_utils import train_wrap\n",
        " \n",
        " n = LeNet5()\n",
        " n.set_train()\n",
        "@@ -28,7 +28,7 @@ context.set_context(mode=context.PYNATIVE_MODE, device_target=\"CPU\", save_graphs\n",
        " BATCH_SIZE = 32\n",
        " x = Tensor(np.ones((BATCH_SIZE, 1, 32, 32)), mstype.float32)\n",
        " label = Tensor(np.zeros([BATCH_SIZE]).astype(np.int32))\n",
        "-net = TrainWrap(n)\n",
        "+net = train_wrap(n)\n",
        " export(net, x, label, file_name=\"lenet_tod\", file_format='MINDIR')\n",
        " \n",
        " print(\"finished exporting\")\n",
        "diff --git a/mindspore/lite/examples/train_lenet/model/train_utils.py b/mindspore/lite/examples/train_lenet/model/train_utils.py\n",
        "index 9e8e3fa..9e3ad76 100644\n",
        "--- a/mindspore/lite/examples/train_lenet/model/train_utils.py\n",
        "+++ b/mindspore/lite/examples/train_lenet/model/train_utils.py\n",
        "@@ -17,9 +17,10 @@\n",
        " import mindspore.nn as nn\n",
        " from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        "-def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "+\n",
        "+def train_wrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "     \"\"\"\n",
        "-    TrainWrap\n",
        "+    train_wrap\n",
        "     \"\"\"\n",
        "     if loss_fn is None:\n",
        "         loss_fn = nn.SoftmaxCrossEntropyWithLogits(reduction='mean', sparse=True)\n",
        "diff --git a/mindspore/lite/examples/train_lenet/src/net_runner.cc b/mindspore/lite/examples/train_lenet/src/net_runner.cc\n",
        "index 1de4f72..4045bdb 100644\n",
        "--- a/mindspore/lite/examples/train_lenet/src/net_runner.cc\n",
        "+++ b/mindspore/lite/examples/train_lenet/src/net_runner.cc\n",
        "@@ -44,10 +44,20 @@ using mindspore::lite::Model;\n",
        " using mindspore::session::TrainLoopCallBack;\n",
        " using mindspore::session::TrainLoopCallBackData;\n",
        " \n",
        "+constexpr int kPrintNum = 10;\n",
        "+constexpr float kScalePoint = 255.0f;\n",
        "+constexpr int kBatchSize = 2;\n",
        "+constexpr int kNCHWDims = 4;\n",
        "+constexpr int kNCHWCDim = 2;\n",
        "+constexpr int kPrintTimes = 100;\n",
        "+constexpr int kSaveSteps = 1000;\n",
        "+constexpr float kLearningRate = 0.7f;\n",
        " class Rescaler : public mindspore::session::TrainLoopCallBack {\n",
        "  public:\n",
        "   explicit Rescaler(float scale) : scale_(scale) {\n",
        "-    if (scale_ == 0) scale_ = 1.0;\n",
        "+    if (scale_ == 0) {\n",
        "+      scale_ = 1.0;\n",
        "+    }\n",
        "   }\n",
        "   ~Rescaler() override = default;\n",
        "   void StepBegin(const mindspore::session::TrainLoopCallBackData &cb_data) override {\n",
        "@@ -68,7 +78,7 @@ bool after_callback(const std::vector<mindspore::tensor::MSTensor *> &after_inpu\n",
        "   for (size_t i = 0; i < after_inputs.size(); i++) {\n",
        "     int num2p = (after_inputs.at(i)->ElementsNum());\n",
        "     printf(\"in%zu(%d): \", i, num2p);\n",
        "-    if (num2p > 10) num2p = 10;\n",
        "+    if (num2p > kPrintNum) num2p = kPrintNum;\n",
        "     if (after_inputs.at(i)->data_type() == mindspore::kNumberTypeInt32) {\n",
        "       auto d = reinterpret_cast<int *>(after_inputs.at(i)->MutableData());\n",
        "       for (int j = 0; j < num2p; j++) printf(\"%d, \", d[j]);\n",
        "@@ -101,8 +111,7 @@ void NetRunner::InitAndFigureInputs() {\n",
        "   context.thread_num_ = 2;\n",
        " \n",
        "   session_ = mindspore::session::LiteSession::CreateTrainSession(ms_file_, &context, true);\n",
        "-\n",
        "-  MS_ASSERT(nullptr != session_);\n",
        "+  MS_ASSERT(session_ != nullptr);\n",
        "   loop_ = mindspore::session::TrainLoop::CreateTrainLoop(session_);\n",
        " \n",
        "   if (verbose_) {\n",
        "@@ -115,10 +124,10 @@ void NetRunner::InitAndFigureInputs() {\n",
        "   auto inputs = session_->GetInputs();\n",
        "   MS_ASSERT(inputs.size() > 1);\n",
        "   auto nhwc_input_dims = inputs.at(0)->shape();\n",
        "-  MS_ASSERT(nhwc_input_dims.size() == 4);\n",
        "+  MS_ASSERT(nhwc_input_dims.size() == kNCHWDims);\n",
        "   batch_size_ = nhwc_input_dims.at(0);\n",
        "   h_ = nhwc_input_dims.at(1);\n",
        "-  w_ = nhwc_input_dims.at(2);\n",
        "+  w_ = nhwc_input_dims.at(kNCHWCDim);\n",
        " }\n",
        " \n",
        " float NetRunner::CalculateAccuracy(int max_tests) {\n",
        "@@ -131,7 +140,7 @@ float NetRunner::CalculateAccuracy(int max_tests) {\n",
        "   test_ds_ = test_ds_->Map({&typecast}, {\"label\"});\n",
        "   test_ds_ = test_ds_->Batch(batch_size_, true);\n",
        " \n",
        "-  Rescaler rescale(255.0);\n",
        "+  Rescaler rescale(kScalePoint);\n",
        " \n",
        "   loop_->Eval(test_ds_.get(), std::vector<TrainLoopCallBack *>{&rescale});\n",
        "   std::cout << \"Eval Accuracy is \" << acc_metrics_->Eval() << std::endl;\n",
        "@@ -162,13 +171,13 @@ int NetRunner::InitDB() {\n",
        " }\n",
        " \n",
        " int NetRunner::TrainLoop() {\n",
        "-  struct mindspore::lite::StepLRLambda step_lr_lambda(1, 0.7);\n",
        "+  struct mindspore::lite::StepLRLambda step_lr_lambda(1, kLearningRate);\n",
        "   mindspore::lite::LRScheduler step_lr_sched(mindspore::lite::StepLRLambda, static_cast<void *>(&step_lr_lambda), 1);\n",
        " \n",
        "-  mindspore::lite::LossMonitor lm(100);\n",
        "+  mindspore::lite::LossMonitor lm(kPrintTimes);\n",
        "   mindspore::lite::ClassificationTrainAccuracyMonitor am(1);\n",
        "-  mindspore::lite::CkptSaver cs(1000, std::string(\"lenet\"));\n",
        "-  Rescaler rescale(255.0);\n",
        "+  mindspore::lite::CkptSaver cs(kSaveSteps, std::string(\"lenet\"));\n",
        "+  Rescaler rescale(kScalePoint);\n",
        " \n",
        "   loop_->Train(epochs_, train_ds_.get(), std::vector<TrainLoopCallBack *>{&rescale, &lm, &cs, &am, &step_lr_sched});\n",
        "   return 0;\n",
        "diff --git a/mindspore/lite/examples/transfer_learning/model/effnet.py b/mindspore/lite/examples/transfer_learning/model/effnet.py\n",
        "index 8ed066f..eba29b5 100755\n",
        "--- a/mindspore/lite/examples/transfer_learning/model/effnet.py\n",
        "+++ b/mindspore/lite/examples/transfer_learning/model/effnet.py\n",
        "@@ -44,6 +44,7 @@ class Swish(nn.Cell):\n",
        "         m = x*s\n",
        "         return m\n",
        " \n",
        "+\n",
        " class AdaptiveAvgPool(nn.Cell):\n",
        "     def __init__(self, output_size=None):\n",
        "         super().__init__(AdaptiveAvgPool)\n",
        "@@ -53,6 +54,7 @@ class AdaptiveAvgPool(nn.Cell):\n",
        "     def construct(self, x):\n",
        "         return self.mean(x, (2, 3))\n",
        " \n",
        "+\n",
        " class SELayer(nn.Cell):\n",
        "     \"\"\"\n",
        "     SELayer\n",
        "@@ -77,6 +79,7 @@ class SELayer(nn.Cell):\n",
        "         o = self.act2(o)\n",
        "         return x * o\n",
        " \n",
        "+\n",
        " class DepthwiseSeparableConv(nn.Cell):\n",
        "     \"\"\"\n",
        "     DepthwiseSeparableConv\n",
        "@@ -84,7 +87,9 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "     def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n",
        "                  stride=1, noskip=False, se_ratio=0.0, drop_connect_rate=0.0):\n",
        "         super().__init__(DepthwiseSeparableConv)\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR\")\n",
        "+            return\n",
        "         self.has_residual = (stride == 1 and in_chs == out_chs) and not noskip\n",
        "         self.drop_connect_rate = drop_connect_rate\n",
        " \n",
        "@@ -117,6 +122,7 @@ class DepthwiseSeparableConv(nn.Cell):\n",
        "             x += residual\n",
        "         return x\n",
        " \n",
        "+\n",
        " def conv_3x3_bn(inp, oup, stride):\n",
        "     weight = weight_variable()\n",
        "     return nn.SequentialCell([\n",
        "@@ -125,6 +131,7 @@ def conv_3x3_bn(inp, oup, stride):\n",
        "         nn.BatchNorm2d(oup, eps=0.001),  # , momentum=0.1),\n",
        "         nn.HSwish()])\n",
        " \n",
        "+\n",
        " def conv_1x1_bn(inp, oup):\n",
        "     weight = weight_variable()\n",
        "     return nn.SequentialCell([\n",
        "@@ -133,13 +140,16 @@ def conv_1x1_bn(inp, oup):\n",
        "         nn.BatchNorm2d(oup, eps=0.001),\n",
        "         nn.HSwish()])\n",
        " \n",
        "+\n",
        " class InvertedResidual(nn.Cell):\n",
        "     \"\"\"\n",
        "     InvertedResidual\n",
        "     \"\"\"\n",
        "     def __init__(self, in_chs, out_chs, kernel_size, stride, padding, expansion, se_ratio):\n",
        "         super().__init__(InvertedResidual)\n",
        "-        assert stride in [1, 2]\n",
        "+        if stride not in [1, 2]:\n",
        "+            print(\"ERROR\")\n",
        "+            return\n",
        "         mid_chs: int = _make_divisible(in_chs * expansion, 1)\n",
        "         self.has_residual = (in_chs == out_chs and stride == 1)\n",
        "         self.drop_connect_rate = 0\n",
        "@@ -194,6 +204,7 @@ class InvertedResidual(nn.Cell):\n",
        "             x += residual\n",
        "         return x\n",
        " \n",
        "+\n",
        " class EfficientNet(nn.Cell):\n",
        "     \"\"\"\n",
        "     EfficientNet\n",
        "@@ -295,6 +306,7 @@ class EfficientNet(nn.Cell):\n",
        "             elif isinstance(m, nn.Dense):\n",
        "                 init_linear_weight(m)\n",
        " \n",
        "+\n",
        " def effnet(**kwargs):\n",
        "     \"\"\"\n",
        "     Constructs a EfficientNet model\n",
        "diff --git a/mindspore/lite/examples/transfer_learning/model/train_utils.py b/mindspore/lite/examples/transfer_learning/model/train_utils.py\n",
        "index 550dd34..53585c8 100644\n",
        "--- a/mindspore/lite/examples/transfer_learning/model/train_utils.py\n",
        "+++ b/mindspore/lite/examples/transfer_learning/model/train_utils.py\n",
        "@@ -17,9 +17,9 @@\n",
        " import mindspore.nn as nn\n",
        " from mindspore.common.parameter import ParameterTuple\n",
        " \n",
        "-def TrainWrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "+def train_wrap(net, loss_fn=None, optimizer=None, weights=None):\n",
        "     \"\"\"\n",
        "-    TrainWrap\n",
        "+    train_wrap\n",
        "     \"\"\"\n",
        "     if loss_fn is None:\n",
        "         loss_fn = nn.SoftmaxCrossEntropyWithLogits(reduction='mean')\n",
        "diff --git a/mindspore/lite/examples/transfer_learning/model/transfer_learning_export.py b/mindspore/lite/examples/transfer_learning/model/transfer_learning_export.py\n",
        "index a6009c7..9d10cc9 100755\n",
        "--- a/mindspore/lite/examples/transfer_learning/model/transfer_learning_export.py\n",
        "+++ b/mindspore/lite/examples/transfer_learning/model/transfer_learning_export.py\n",
        "@@ -19,7 +19,7 @@ import mindspore as M\n",
        " from mindspore.nn import Cell\n",
        " from mindspore.train.serialization import load_checkpoint, export\n",
        " from effnet import effnet\n",
        "-from train_utils import TrainWrap\n",
        "+from train_utils import train_wrap\n",
        " \n",
        " \n",
        " class TransferNet(Cell):\n",
        "@@ -51,7 +51,7 @@ HEAD.bias.set_data(M.Tensor(np.zeros(HEAD.bias.data.shape, dtype=\"float32\")))\n",
        " \n",
        " sgd = M.nn.SGD(HEAD.trainable_params(), learning_rate=0.015, momentum=0.9,\n",
        "                dampening=0.01, weight_decay=0.0, nesterov=False, loss_scale=1.0)\n",
        "-net = TrainWrap(HEAD, optimizer=sgd)\n",
        "+net = train_wrap(HEAD, optimizer=sgd)\n",
        " backbone_out = M.Tensor(np.zeros([BATCH_SIZE, 1000]).astype(np.float32))\n",
        " export(net, backbone_out, label, file_name=\"transfer_learning_tod_head\", file_format='MINDIR')\n",
        " \n",
        "diff --git a/mindspore/lite/examples/transfer_learning/src/dataset.cc b/mindspore/lite/examples/transfer_learning/src/dataset.cc\n",
        "index 7a0669b..2f8d3c2 100644\n",
        "--- a/mindspore/lite/examples/transfer_learning/src/dataset.cc\n",
        "+++ b/mindspore/lite/examples/transfer_learning/src/dataset.cc\n",
        "@@ -50,6 +50,10 @@ float CH_MEAN[3] = {0.485, 0.456, 0.406};\n",
        " float CH_STD[3] = {0.229, 0.224, 0.225};\n",
        " \n",
        " using LabelId = std::map<std::string, int>;\n",
        "+constexpr int kClassNum = 10;\n",
        "+constexpr int kBGRDim = 2;\n",
        "+constexpr float kRGBMAX = 255.0f;\n",
        "+constexpr int kRGBDims = 3;\n",
        " \n",
        " static char *ReadBitmapFile(const std::string &filename, size_t *size) {\n",
        "   MS_ASSERT(size != nullptr);\n",
        "@@ -78,7 +82,7 @@ static char *ReadBitmapFile(const std::string &filename, size_t *size) {\n",
        " \n",
        "   ifs.read(reinterpret_cast<char *>(bmp_image), bitmap_header.image_size_bytes);\n",
        " \n",
        "-  size_t buffer_size = bitmap_header.width * bitmap_header.height * 3;\n",
        "+  size_t buffer_size = bitmap_header.width * bitmap_header.height * kRGBDims;\n",
        "   float *hwc_bin_image = new (std::nothrow) float[buffer_size];\n",
        "   if (hwc_bin_image == nullptr) {\n",
        "     free(bmp_image);\n",
        "@@ -95,14 +99,16 @@ static char *ReadBitmapFile(const std::string &filename, size_t *size) {\n",
        "   for (int h = 0; h < bitmap_header.height; h++) {\n",
        "     for (int w = 0; w < bitmap_header.width; w++) {\n",
        "       hwc_bin_image[h * hStride + w * channels + 0] =\n",
        "-        (((static_cast<float>(bmp_image[(height - h - 1) * hStride + w * channels + 2])) / 255.0) - CH_MEAN[0]) /\n",
        "+        (((static_cast<float>(bmp_image[(height - h - 1) * hStride + w * channels + kBGRDim])) / kRGBMAX) -\n",
        "+         CH_MEAN[0]) /\n",
        "         CH_STD[0];\n",
        "       hwc_bin_image[h * hStride + w * channels + 1] =\n",
        "-        (((static_cast<float>(bmp_image[(height - h - 1) * hStride + w * channels + 1])) / 255.0) - CH_MEAN[1]) /\n",
        "+        (((static_cast<float>(bmp_image[(height - h - 1) * hStride + w * channels + 1])) / kRGBMAX) - CH_MEAN[1]) /\n",
        "         CH_STD[1];\n",
        "-      hwc_bin_image[h * hStride + w * channels + 2] =\n",
        "-        (((static_cast<float>(bmp_image[(height - h - 1) * hStride + w * channels + 0])) / 255.0) - CH_MEAN[2]) /\n",
        "-        CH_STD[2];\n",
        "+      hwc_bin_image[h * hStride + w * channels + kBGRDim] =\n",
        "+        (((static_cast<float>(bmp_image[(height - h - 1) * hStride + w * channels + 0])) / kRGBMAX) -\n",
        "+         CH_MEAN[kBGRDim]) /\n",
        "+        CH_STD[kBGRDim];\n",
        "     }\n",
        "   }\n",
        " \n",
        "@@ -190,7 +196,7 @@ void DataSet::InitializeBMPFoldersDatabase(std::string dpath) {\n",
        " std::vector<FileTuple> DataSet::ReadDir(const std::string dpath) {\n",
        "   std::vector<FileTuple> vec;\n",
        "   struct dirent *entry = nullptr;\n",
        "-  num_of_classes_ = 10;\n",
        "+  num_of_classes_ = kClassNum;\n",
        "   for (int class_id = 0; class_id < num_of_classes_; class_id++) {\n",
        "     std::string dirname = dpath + \"/\" + std::to_string(class_id);\n",
        "     DIR *dp = opendir(dirname.c_str());\n",
        "diff --git a/mindspore/lite/examples/transfer_learning/src/net_runner.cc b/mindspore/lite/examples/transfer_learning/src/net_runner.cc\n",
        "index d341720..449fc54 100644\n",
        "--- a/mindspore/lite/examples/transfer_learning/src/net_runner.cc\n",
        "+++ b/mindspore/lite/examples/transfer_learning/src/net_runner.cc\n",
        "@@ -15,9 +15,9 @@\n",
        "  */\n",
        " \n",
        " #include \"src/net_runner.h\"\n",
        "-#include <math.h>\n",
        " #include <getopt.h>\n",
        " #include <algorithm>\n",
        "+#include <cmath>\n",
        " #include <cstring>\n",
        " #include <fstream>\n",
        " #include <iostream>\n",
        "@@ -26,6 +26,9 @@\n",
        " #include \"src/utils.h\"\n",
        " \n",
        " static unsigned int seed = time(NULL);\n",
        "+constexpr int kBatchNum = 20;\n",
        "+constexpr int kPrintNum = 10;\n",
        "+constexpr float kThreshold = 0.9f;\n",
        " \n",
        " // Definition of callback function after forwarding operator.\n",
        " bool after_callback(const std::vector<mindspore::tensor::MSTensor *> &after_inputs,\n",
        "@@ -35,7 +38,7 @@ bool after_callback(const std::vector<mindspore::tensor::MSTensor *> &after_inpu\n",
        "   for (size_t i = 0; i < after_inputs.size(); i++) {\n",
        "     int num2p = (after_inputs.at(i)->ElementsNum());\n",
        "     std::cout << \"in\" << i << \"(\" << num2p << \"): \";\n",
        "-    if (num2p > 10) num2p = 10;\n",
        "+    if (num2p > kPrintNum) num2p = kPrintNum;\n",
        "     if (after_inputs.at(i)->data_type() == mindspore::kNumberTypeInt32) {\n",
        "       auto d = reinterpret_cast<int *>(after_inputs.at(i)->MutableData());\n",
        "       for (int j = 0; j < num2p; j++) {\n",
        "@@ -53,7 +56,7 @@ bool after_callback(const std::vector<mindspore::tensor::MSTensor *> &after_inpu\n",
        "     auto d = reinterpret_cast<float *>(after_outputs.at(i)->MutableData());\n",
        "     int num2p = (after_outputs.at(i)->ElementsNum());\n",
        "     std::cout << \"ou\" << i << \"(\" << num2p << \"): \";\n",
        "-    if (num2p > 10) num2p = 10;\n",
        "+    if (num2p > kPrintNum) num2p = kPrintNum;\n",
        "     for (int j = 0; j < num2p; j++) {\n",
        "       std::cout << d[j] << \", \";\n",
        "     }\n",
        "@@ -72,7 +75,7 @@ void NetRunner::InitAndFigureInputs() {\n",
        "   context.thread_num_ = 1;\n",
        " \n",
        "   session_ = mindspore::session::LiteSession::CreateTransferSession(ms_backbone_file_, ms_head_file_, &context);\n",
        "-  MS_ASSERT(nullptr != session_);\n",
        "+  MS_ASSERT(session_ != nullptr);\n",
        " \n",
        "   auto inputs = session_->GetInputs();\n",
        "   MS_ASSERT(inputs.size() > 1);\n",
        "@@ -108,7 +111,8 @@ std::vector<int> NetRunner::FillInputData(const std::vector<DataLabelTuple> &dat\n",
        "   std::fill(labels, labels + inputs.at(label_index_)->ElementsNum(), 0.f);\n",
        "   for (int i = 0; i < batch_size_; i++) {\n",
        "     if (serially >= 0) {\n",
        "-      idx = ++idx % total_size;\n",
        "+      auto reminder = ++idx % total_size;\n",
        "+      idx = reminder;\n",
        "     } else {\n",
        "       idx = rand_r(&seed) % total_size;\n",
        "     }\n",
        "@@ -191,13 +195,13 @@ int NetRunner::TrainLoop() {\n",
        "     }\n",
        " \n",
        "     std::cout << i + 1 << \": Loss is \" << loss << \" [min=\" << min_loss << \"]\" << std::endl;\n",
        "-    if ((i + 1) % 20 == 0) {\n",
        "+    if ((i + 1) % kBatchNum == 0) {\n",
        "       session_->Eval();\n",
        "       float acc = CalculateAccuracy(ds_.test_data(), session_);\n",
        "       session_->Train();\n",
        "       if (max_acc < acc) max_acc = acc;\n",
        "       std::cout << \"accuracy on test data = \" << acc << \" max accuracy = \" << max_acc << std::endl;\n",
        "-      if (acc > 0.9) return 0;\n",
        "+      if (acc > kThreshold) return 0;\n",
        "     }\n",
        "   }\n",
        "   return 0;\n",
        "diff --git a/mindspore/lite/include/train/accuracy_metrics.h b/mindspore/lite/include/train/accuracy_metrics.h\n",
        "index e3822fd..9dfa451 100644\n",
        "--- a/mindspore/lite/include/train/accuracy_metrics.h\n",
        "+++ b/mindspore/lite/include/train/accuracy_metrics.h\n",
        "@@ -41,6 +41,7 @@ class AccuracyMetrics : public Metrics {\n",
        "   std::vector<int> output_indexes_ = {0};\n",
        "   float total_accuracy_ = 0.0;\n",
        "   float total_steps_ = 0.0;\n",
        "+  friend class ClassificationTrainAccuracyMonitor;\n",
        " };\n",
        " \n",
        " }  // namespace lite\n",
        "diff --git a/mindspore/lite/include/train/classification_train_accuracy_monitor.h b/mindspore/lite/include/train/classification_train_accuracy_monitor.h\n",
        "index 5c85592..3df8af9 100644\n",
        "--- a/mindspore/lite/include/train/classification_train_accuracy_monitor.h\n",
        "+++ b/mindspore/lite/include/train/classification_train_accuracy_monitor.h\n",
        "@@ -16,6 +16,7 @@\n",
        " #ifndef MINDSPORE_LITE_INCLUDE_TRAIN_CLASSIFICATION_TRAIN_ACCURACY_MONITOR_H_\n",
        " #define MINDSPORE_LITE_INCLUDE_TRAIN_CLASSIFICATION_TRAIN_ACCURACY_MONITOR_H_\n",
        " #include <vector>\n",
        "+#include <memory>\n",
        " #include <string>\n",
        " #include <utility>\n",
        " #include <climits>\n",
        "@@ -44,9 +45,7 @@ class ClassificationTrainAccuracyMonitor : public session::TrainLoopCallBack {\n",
        " \n",
        "  private:\n",
        "   std::vector<GraphPoint> accuracies_;\n",
        "-  int accuracy_metrics_ = METRICS_CLASSIFICATION;\n",
        "-  std::vector<int> input_indexes_ = {1};\n",
        "-  std::vector<int> output_indexes_ = {0};\n",
        "+  std::shared_ptr<AccuracyMetrics> accuracy_metrics_;\n",
        "   int print_every_n_ = 0;\n",
        " };\n",
        " \n",
        "diff --git a/mindspore/lite/src/huffman_decode.cc b/mindspore/lite/src/huffman_decode.cc\n",
        "index 44a0dc3..92efb73 100644\n",
        "--- a/mindspore/lite/src/huffman_decode.cc\n",
        "+++ b/mindspore/lite/src/huffman_decode.cc\n",
        "@@ -19,7 +19,7 @@\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-STATUS HuffmanDecode::DoHuffmanDecode(const std::string &input_str, void *decoded_data) {\n",
        "+STATUS HuffmanDecode::DoHuffmanDecode(const std::string &input_str, void *decoded_data, size_t data_len) {\n",
        "   if (decoded_data == nullptr) {\n",
        "     MS_LOG(ERROR) << \"decoded_data is nullptr.\";\n",
        "     return RET_ERROR;\n",
        "@@ -57,8 +57,12 @@ STATUS HuffmanDecode::DoHuffmanDecode(const std::string &input_str, void *decode\n",
        "   }\n",
        " \n",
        "   size_t len = huffman_decoded_str.length();\n",
        "-  memcpy(decoded_data, huffman_decoded_str.c_str(), len);\n",
        "-\n",
        "+  if (data_len >= len) {\n",
        "+    memcpy(decoded_data, huffman_decoded_str.c_str(), len);\n",
        "+  } else {\n",
        "+    FreeHuffmanNodeTree(root);\n",
        "+    return RET_ERROR;\n",
        "+  }\n",
        "   FreeHuffmanNodeTree(root);\n",
        "   return RET_OK;\n",
        " }\n",
        "@@ -173,6 +177,5 @@ void HuffmanDecode::FreeHuffmanNodeTree(HuffmanNodePtr root) {\n",
        "     delete (cur_node);\n",
        "   }\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/huffman_decode.h b/mindspore/lite/src/huffman_decode.h\n",
        "index 0495fd0..587af17 100644\n",
        "--- a/mindspore/lite/src/huffman_decode.h\n",
        "+++ b/mindspore/lite/src/huffman_decode.h\n",
        "@@ -42,7 +42,7 @@ class HuffmanDecode {\n",
        "  public:\n",
        "   virtual ~HuffmanDecode() = default;\n",
        " \n",
        "-  static STATUS DoHuffmanDecode(const std::string &input_str, void *decoded_data);\n",
        "+  static STATUS DoHuffmanDecode(const std::string &input_str, void *decoded_data, size_t data_len);\n",
        " \n",
        "  private:\n",
        "   HuffmanDecode() = default;\n",
        "diff --git a/mindspore/lite/src/ops/populate/arithmetic_populate.cc b/mindspore/lite/src/ops/populate/arithmetic_populate.cc\n",
        "index f32f601..fe7ce3d 100644\n",
        "--- a/mindspore/lite/src/ops/populate/arithmetic_populate.cc\n",
        "+++ b/mindspore/lite/src/ops/populate/arithmetic_populate.cc\n",
        "@@ -32,6 +32,7 @@ using mindspore::schema::PrimitiveType_Mod;\n",
        " using mindspore::schema::PrimitiveType_NotEqual;\n",
        " using mindspore::schema::PrimitiveType_RealDiv;\n",
        " using mindspore::schema::PrimitiveType_SquaredDifference;\n",
        "+using mindspore::schema::PrimitiveType_BiasAddGrad;\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "@@ -79,5 +80,6 @@ REG_POPULATE(PrimitiveType_FloorDiv, PopulateArithmetic, SCHEMA_CUR)\n",
        " REG_POPULATE(PrimitiveType_FloorMod, PopulateArithmetic, SCHEMA_CUR)\n",
        " REG_POPULATE(PrimitiveType_Mod, PopulateArithmetic, SCHEMA_CUR)\n",
        " REG_POPULATE(PrimitiveType_SquaredDifference, PopulateArithmetic, SCHEMA_CUR)\n",
        "+REG_POPULATE(PrimitiveType_BiasAddGrad, PopulateArithmetic, SCHEMA_CUR)\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/ops/populate/bias_grad_populate.cc b/mindspore/lite/src/ops/populate/bias_grad_populate.cc\n",
        "deleted file mode 100644\n",
        "index edcfec3..0000000\n",
        "--- a/mindspore/lite/src/ops/populate/bias_grad_populate.cc\n",
        "+++ /dev/null\n",
        "@@ -1,39 +0,0 @@\n",
        "-/**\n",
        "- * Copyright 2019-2021 Huawei Technologies Co., Ltd\n",
        "- *\n",
        "- * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "- * you may not use this file except in compliance with the License.\n",
        "- * You may obtain a copy of the License at\n",
        "- *\n",
        "- * http://www.apache.org/licenses/LICENSE-2.0\n",
        "- *\n",
        "- * Unless required by applicable law or agreed to in writing, software\n",
        "- * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "- * See the License for the specific language governing permissions and\n",
        "- * limitations under the License.\n",
        "- */\n",
        "-#include \"src/ops/populate/populate_register.h\"\n",
        "-#include \"nnacl/arithmetic.h\"\n",
        "-using mindspore::schema::PrimitiveType_BiasAddGrad;\n",
        "-\n",
        "-namespace mindspore {\n",
        "-namespace lite {\n",
        "-OpParameter *PopulateBiasAddGradParameter(const void *prim) {\n",
        "-  auto primitive = static_cast<const schema::Primitive *>(prim);\n",
        "-  MS_ASSERT(primitive != nullptr);\n",
        "-\n",
        "-  auto *param = reinterpret_cast<ArithmeticParameter *>(malloc(sizeof(ArithmeticParameter)));\n",
        "-  if (param == nullptr) {\n",
        "-    MS_LOG(ERROR) << \"malloc ArithmeticParameter failed.\";\n",
        "-    return nullptr;\n",
        "-  }\n",
        "-  memset(param, 0, sizeof(ArithmeticParameter));\n",
        "-\n",
        "-  param->op_parameter_.type_ = primitive->value_type();\n",
        "-  return reinterpret_cast<OpParameter *>(param);\n",
        "-}\n",
        "-\n",
        "-REG_POPULATE(PrimitiveType_BiasAddGrad, PopulateBiasAddGradParameter, SCHEMA_CUR);\n",
        "-}  // namespace lite\n",
        "-}  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/accuracy_metrics.cc b/mindspore/lite/src/train/accuracy_metrics.cc\n",
        "index 6b79088..6d6a5c5 100644\n",
        "--- a/mindspore/lite/src/train/accuracy_metrics.cc\n",
        "+++ b/mindspore/lite/src/train/accuracy_metrics.cc\n",
        "@@ -22,7 +22,6 @@\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-\n",
        " AccuracyMetrics::AccuracyMetrics(int accuracy_metrics, const std::vector<int> &input_indexes,\n",
        "                                  const std::vector<int> &output_indexes)\n",
        "     : Metrics() {\n",
        "@@ -66,6 +65,5 @@ float AccuracyMetrics::Eval() {\n",
        " \n",
        "   return (total_accuracy_ / total_steps_);\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/accuracy_monitor.cc b/mindspore/lite/src/train/accuracy_monitor.cc\n",
        "index a9aabdc..0cb3b5d 100644\n",
        "--- a/mindspore/lite/src/train/accuracy_monitor.cc\n",
        "+++ b/mindspore/lite/src/train/accuracy_monitor.cc\n",
        "@@ -29,7 +29,6 @@\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-\n",
        " void AccuracyMonitor::Begin(const session::TrainLoopCallBackData &cb_data) {\n",
        "   if (cb_data.epoch_ == 0) accuracies_.clear();\n",
        " }\n",
        "@@ -40,6 +39,5 @@ int AccuracyMonitor::EpochEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "   accuracies_.push_back(std::make_pair(cb_data.epoch_, 0.0));\n",
        "   return mindspore::session::RET_CONTINUE;\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/classification_train_accuracy_monitor.cc b/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "index d398429..d806902 100644\n",
        "--- a/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "+++ b/mindspore/lite/src/train/classification_train_accuracy_monitor.cc\n",
        "@@ -26,21 +26,10 @@ using mindspore::WARNING;\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-\n",
        " ClassificationTrainAccuracyMonitor::ClassificationTrainAccuracyMonitor(int print_every_n, int accuracy_metrics,\n",
        "                                                                        const std::vector<int> &input_indexes,\n",
        "                                                                        const std::vector<int> &output_indexes) {\n",
        "-  if (input_indexes.size() == output_indexes.size()) {\n",
        "-    input_indexes_ = input_indexes;\n",
        "-    output_indexes_ = output_indexes;\n",
        "-  } else {\n",
        "-    MS_LOG(WARNING) << \"input to output mapping vectors sizes do not match\";\n",
        "-  }\n",
        "-  if (accuracy_metrics != METRICS_CLASSIFICATION) {\n",
        "-    MS_LOG(WARNING) << \"Only classification metrics is supported\";\n",
        "-  } else {\n",
        "-    accuracy_metrics_ = accuracy_metrics;\n",
        "-  }\n",
        "+  accuracy_metrics_ = std::make_shared<AccuracyMetrics>(accuracy_metrics, input_indexes, output_indexes);\n",
        "   print_every_n_ = print_every_n;\n",
        " }\n",
        " \n",
        "@@ -59,8 +48,8 @@ void ClassificationTrainAccuracyMonitor::EpochBegin(const session::TrainLoopCall\n",
        " int ClassificationTrainAccuracyMonitor::EpochEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "   if (cb_data.step_ > 0) accuracies_.at(cb_data.epoch_).second /= static_cast<float>(cb_data.step_ + 1);\n",
        "   if ((cb_data.epoch_ + 1) % print_every_n_ == 0) {\n",
        "-    std::cout << \"Epoch (\" << cb_data.epoch_ + 1 << \"):\\tTraining Accuracy is \" << accuracies_.at(cb_data.epoch_).second\n",
        "-              << std::endl;\n",
        "+    std::cout << \"Epoch (\" << (cb_data.epoch_ + 1) << \"):\\tTraining Accuracy is \"\n",
        "+              << accuracies_.at(cb_data.epoch_).second << std::endl;\n",
        "   }\n",
        "   return mindspore::session::RET_CONTINUE;\n",
        " }\n",
        "@@ -70,21 +59,22 @@ void ClassificationTrainAccuracyMonitor::StepEnd(const session::TrainLoopCallBac\n",
        "   auto outputs = cb_data.session_->GetPredictions();\n",
        " \n",
        "   float accuracy = 0.0;\n",
        "-  for (unsigned int i = 0; i < input_indexes_.size(); i++) {\n",
        "-    if ((inputs.size() <= static_cast<unsigned int>(input_indexes_[i])) ||\n",
        "-        (outputs.size() <= static_cast<unsigned int>(output_indexes_[i]))) {\n",
        "-      MS_LOG(WARNING) << \"indices \" << input_indexes_[i] << \"/\" << output_indexes_[i]\n",
        "+  auto input_indexes = accuracy_metrics_->input_indexes_;\n",
        "+  auto output_indexes = accuracy_metrics_->output_indexes_;\n",
        "+  for (unsigned int i = 0; i < input_indexes.size(); i++) {\n",
        "+    if ((inputs.size() <= static_cast<unsigned int>(input_indexes[i])) ||\n",
        "+        (outputs.size() <= static_cast<unsigned int>(output_indexes[i]))) {\n",
        "+      MS_LOG(WARNING) << \"indices \" << input_indexes[i] << \"/\" << output_indexes[i]\n",
        "                       << \" is outside of input/output range\";\n",
        "       return;\n",
        "     }\n",
        "-    if (inputs.at(input_indexes_[i])->data_type() == kNumberTypeInt32) {\n",
        "-      accuracy += CalculateSparseClassification(inputs.at(input_indexes_[i]), outputs.at(output_indexes_[i]));\n",
        "+    if (inputs.at(input_indexes[i])->data_type() == kNumberTypeInt32) {\n",
        "+      accuracy += CalculateSparseClassification(inputs.at(input_indexes[i]), outputs.at(output_indexes[i]));\n",
        "     } else {\n",
        "-      accuracy += CalculateOneHotClassification(inputs.at(input_indexes_[i]), outputs.at(output_indexes_[i]));\n",
        "+      accuracy += CalculateOneHotClassification(inputs.at(input_indexes[i]), outputs.at(output_indexes[i]));\n",
        "     }\n",
        "   }\n",
        "   accuracies_.at(cb_data.epoch_).second += accuracy;\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/loss_monitor.cc b/mindspore/lite/src/train/loss_monitor.cc\n",
        "index bd3e529..60637f9 100644\n",
        "--- a/mindspore/lite/src/train/loss_monitor.cc\n",
        "+++ b/mindspore/lite/src/train/loss_monitor.cc\n",
        "@@ -26,7 +26,6 @@\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-\n",
        " void LossMonitor::Begin(const session::TrainLoopCallBackData &cb_data) {\n",
        "   if (cb_data.epoch_ == 0) losses_.clear();\n",
        " }\n",
        "@@ -42,7 +41,7 @@ void LossMonitor::EpochBegin(const session::TrainLoopCallBackData &cb_data) {\n",
        " int LossMonitor::EpochEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "   if (cb_data.step_ > 0) losses_.at(cb_data.epoch_).second /= static_cast<float>(cb_data.step_ + 1);\n",
        "   if (print_every_n_ > 0) {\n",
        "-    std::cout << \"Epoch (\" << cb_data.epoch_ + 1 << \"):\\tLoss is \" << losses_.at(cb_data.epoch_).second << std::endl;\n",
        "+    std::cout << \"Epoch (\" << (cb_data.epoch_ + 1) << \"):\\tLoss is \" << losses_.at(cb_data.epoch_).second << std::endl;\n",
        "   }\n",
        "   return mindspore::session::RET_CONTINUE;\n",
        " }\n",
        "@@ -54,12 +53,11 @@ void LossMonitor::StepEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "       auto loss = reinterpret_cast<float *>(it->second->MutableData());\n",
        "       losses_.at(cb_data.epoch_).second += loss[0];\n",
        "       if ((cb_data.step_ + 1) % print_every_n_ == 0)\n",
        "-        std::cout << cb_data.epoch_ + 1 << \".\" << cb_data.step_ + 1 << \":\\tLoss is \" << loss[0] << std::endl;\n",
        "+        std::cout << (cb_data.epoch_ + 1) << \".\" << (cb_data.step_ + 1) << \":\\tLoss is \" << loss[0] << std::endl;\n",
        "       return;\n",
        "     }\n",
        "   }\n",
        "   MS_LOG(WARNING) << \"Model does not have a loss output tensor of size 1\";\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/lr_scheduler.cc b/mindspore/lite/src/train/lr_scheduler.cc\n",
        "index ea9d074..d9713c3 100644\n",
        "--- a/mindspore/lite/src/train/lr_scheduler.cc\n",
        "+++ b/mindspore/lite/src/train/lr_scheduler.cc\n",
        "@@ -29,7 +29,6 @@\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-\n",
        " int MultiplicativeLRLambda(float *lr, int epoch, void *lr_cb_data) {\n",
        "   if ((lr == nullptr) || (lr_cb_data == nullptr)) {\n",
        "     MS_LOG(ERROR) << \"nullptr passed as input to MultiplicativeLRLambda\";\n",
        "@@ -70,6 +69,5 @@ int LRScheduler::EpochEnd(const session::TrainLoopCallBackData &cb_data) {\n",
        "   }\n",
        "   return mindspore::session::RET_CONTINUE;\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/train_loop.cc b/mindspore/lite/src/train/train_loop.cc\n",
        "index 93ab442..4f3f904 100644\n",
        "--- a/mindspore/lite/src/train/train_loop.cc\n",
        "+++ b/mindspore/lite/src/train/train_loop.cc\n",
        "@@ -25,7 +25,6 @@\n",
        " \n",
        " namespace mindspore {\n",
        " namespace lite {\n",
        "-\n",
        " using dataset::Dataset;\n",
        " using dataset::Iterator;\n",
        " using dataset::MSTensorVec;\n",
        "@@ -132,28 +131,8 @@ int TrainLoop::LoadData(std::vector<tensor::MSTensor *> inputs, dataset::MSTenso\n",
        "   }\n",
        " \n",
        "   for (unsigned int i = 0; i < num_of_inputs; i++) {\n",
        "-    unsigned char *input_data = reinterpret_cast<unsigned char *>(inputs.at(i)->MutableData());\n",
        "-    const unsigned char *row_data = reinterpret_cast<const unsigned char *>(row_vec->at(i).MutableData());\n",
        "-    auto data_size = row_vec->at(i).DataSize();\n",
        "-    if (data_size != inputs.at(i)->Size()) {\n",
        "-      MS_LOG(WARNING) << \"Model Input tensor \" << i << \" size (\" << inputs.at(i)->Size()\n",
        "-                      << \") does not match dataset size (\" << data_size << \")\\n\";\n",
        "-      return RET_STOP_TRAINING;\n",
        "-    }\n",
        "-    std::copy(row_data, row_data + data_size, input_data);\n",
        "-  }\n",
        "-  return RET_OK;\n",
        "-}\n",
        "-\n",
        "-int TrainLoop::LoadPartialData(std::vector<tensor::MSTensor *> inputs, dataset::MSTensorVec *row_vec) {\n",
        "-  auto num_of_inputs = inputs.size();\n",
        "-  if ((num_of_inputs == 0) || (row_vec == nullptr) || (num_of_inputs < row_vec->size())) {\n",
        "-    return RET_STOP_TRAINING;\n",
        "-  }\n",
        "-\n",
        "-  for (unsigned int i = 0; i < row_vec->size(); i++) {\n",
        "-    unsigned char *input_data = reinterpret_cast<unsigned char *>(inputs.at(i)->MutableData());\n",
        "-    const unsigned char *row_data = reinterpret_cast<const unsigned char *>(row_vec->at(i).MutableData());\n",
        "+    auto *input_data = reinterpret_cast<unsigned char *>(inputs.at(i)->MutableData());\n",
        "+    const auto *row_data = reinterpret_cast<const unsigned char *>(row_vec->at(i).MutableData());\n",
        "     auto data_size = row_vec->at(i).DataSize();\n",
        "     if (data_size != inputs.at(i)->Size()) {\n",
        "       MS_LOG(WARNING) << \"Model Input tensor \" << i << \" size (\" << inputs.at(i)->Size()\n",
        "@@ -164,12 +143,10 @@ int TrainLoop::LoadPartialData(std::vector<tensor::MSTensor *> inputs, dataset::\n",
        "   }\n",
        "   return RET_OK;\n",
        " }\n",
        "-\n",
        " }  // namespace lite\n",
        " \n",
        " session::TrainLoop *session::TrainLoop::CreateTrainLoop(session::LiteSession *train_session) {\n",
        "   auto loop = new (std::nothrow) lite::TrainLoop(train_session);\n",
        "   return loop;\n",
        " }\n",
        "-\n",
        " }  // namespace mindspore\n",
        "diff --git a/mindspore/lite/src/train/train_loop.h b/mindspore/lite/src/train/train_loop.h\n",
        "index e35a71a..40a63da 100644\n",
        "--- a/mindspore/lite/src/train/train_loop.h\n",
        "+++ b/mindspore/lite/src/train/train_loop.h\n",
        "@@ -63,7 +63,6 @@ class TrainLoop : virtual public session::TrainLoop {\n",
        " \n",
        "  protected:\n",
        "   static int LoadData(std::vector<tensor::MSTensor *> inputs, dataset::MSTensorVec *dataset_vec);\n",
        "-  static int LoadPartialData(std::vector<tensor::MSTensor *> inputs, dataset::MSTensorVec *dataset_vec);\n",
        " \n",
        "   session::LiteSession *train_session_ = nullptr;\n",
        "   unsigned int epoch_ = 0;\n",
        "diff --git a/mindspore/lite/src/train/train_populate_parameter_v0.cc b/mindspore/lite/src/train/train_populate_parameter_v0.cc\n",
        "index 072dfb0..80c907e 100644\n",
        "--- a/mindspore/lite/src/train/train_populate_parameter_v0.cc\n",
        "+++ b/mindspore/lite/src/train/train_populate_parameter_v0.cc\n",
        "@@ -592,7 +592,6 @@ OpParameter *PopulateArithmeticGradParameter(const void *primitive) {\n",
        "   }\n",
        "   return reinterpret_cast<OpParameter *>(arithmetic_param);\n",
        " }\n",
        "-\n",
        " }  // namespace\n",
        " \n",
        " void PopulateTrainV0Parameters() {\n",
        "@@ -665,5 +664,4 @@ void PopulateTrainV0Parameters() {\n",
        "   lite::Registry g_sigmoidCrossEntropyWithLogitsGradRegistry(\n",
        "     schema::v0::PrimitiveType_SigmoidCrossEntropyWithLogitsGrad, DefaultPopulateParameter, mindspore::lite::SCHEMA_V0);\n",
        " }\n",
        "-\n",
        " }  // namespace mindspore::kernel\n",
        "diff --git a/mindspore/lite/src/weight_decoder.cc b/mindspore/lite/src/weight_decoder.cc\n",
        "index 86492d7..ece9582 100644\n",
        "--- a/mindspore/lite/src/weight_decoder.cc\n",
        "+++ b/mindspore/lite/src/weight_decoder.cc\n",
        "@@ -255,7 +255,7 @@ int WeightDecoder::DecodeHuffmanCode(const schema::Tensor &src_tensor, lite::Ten\n",
        "   }\n",
        "   auto dst_data = dst_tensor->data_c();\n",
        "   MS_ASSERT(dst_data != nullptr);\n",
        "-  ret = HuffmanDecode::DoHuffmanDecode(encode_str, dst_data);\n",
        "+  ret = HuffmanDecode::DoHuffmanDecode(encode_str, dst_data, dst_tensor->Size());\n",
        "   if (ret != RET_OK) {\n",
        "     MS_LOG(ERROR) << \"DoHuffmanDecode failed.\";\n",
        "     return ret;\n",
        "-- \n",
        "2.7.4\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}