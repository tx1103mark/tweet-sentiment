{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TensorFlow_with_GPU6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# pytorch with TPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sh85Zg9c3vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/**\n",
        " * Copyright 2020 Huawei Technologies Co., Ltd\n",
        " *\n",
        " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " * you may not use this file except in compliance with the License.\n",
        " * You may obtain a copy of the License at\n",
        " *\n",
        " * http://www.apache.org/licenses/LICENSE-2.0\n",
        " *conv_activation_fusion.h\n",
        " * Unless required by applicable law or agreed to in writing, software\n",
        " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        " * See the License for the specific language governing permissions and\n",
        " * limitations under the License.\n",
        " */\n",
        "\n",
        "#include \"mindspore/lite/src/gllo/fusion/conv_activation_fusion.h\"\n",
        "#include \"mindspore/lite/schema/inner/model_generated.h\"\n",
        "#include \"mindspore/lite/src/ir/primitive_t_value.h\"\n",
        "#include <memory>\n",
        "#include \"mindspore/ccsrc/utils/utils.h\"\n",
        "#include \"mindspore/lite/src/gllo/common/utils.h\"\n",
        "\n",
        "namespace mindspore::opt {\n",
        "namespace {\n",
        "constexpr size_t kActivationInputsLength = 2;\n",
        "}\n",
        "const BaseRef ConvActivationFusion::DefinePattern() const {\n",
        "  auto conv_var = std::make_shared<CondVar>(IsConvNode);\n",
        "  auto prim = new schema::PrimitiveT();\n",
        "  prim->value.type = primitive_type;\n",
        "  auto prim_value = std::make_shared<lite::PrimitiveTValue>(prim);\n",
        "\n",
        "  return VectorRef({prim_value, conv_var});\n",
        "}\n",
        "\n",
        "const AnfNodePtr ConvActivationFusion::Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,\n",
        "                                               const EquivPtr &) const {\n",
        "  MS_LOG(DEBUG) << \"conv activation pass process:\" << schema::EnumNamesPrimitiveType()[primitive_type];\n",
        "  CheckIfFuncGraphIsNull(func_graph);\n",
        "\n",
        "  CheckIfAnfNodeIsNull(node);\n",
        "  auto act_node = node->cast<CNodePtr>();\n",
        "  CheckIfCNodeIsNull(act_node);\n",
        "  CheckInputSize(act_node, kActivationInputsLength);\n",
        "\n",
        "  auto act_primitive = GetValueNode<std::shared_ptr<lite::PrimitiveTValue>>(act_node->input(0));\n",
        "  if (act_primitive->GetPrimitiveT()->value.AsActivation()->type != activation_type) {\n",
        "    return node;\n",
        "  }\n",
        "  AnfNodePtr pre_node = act_node->input(1);\n",
        "  CheckIfAnfNodeIsNull(pre_node);\n",
        "  if (pre_node != nullptr && pre_node->isa<CNode>()) {\n",
        "    auto conv_node = pre_node->cast<CNodePtr>();\n",
        "    auto node_type = GetCNodeType(conv_node);\n",
        "    auto primitiveT_value = GetValueNode<std::shared_ptr<lite::PrimitiveTValue>>(conv_node->input(0));\n",
        "    MS_ASSERT(primitiveT_value);\n",
        "    if (node_type == schema::PrimitiveType_Conv2D) {\n",
        "      primitiveT_value->GetPrimitiveT()->value.AsConv2D()->activationType = activation_type;\n",
        "      return pre_node;\n",
        "    } else if (node_type == schema::PrimitiveType_DepthwiseConv2D) {\n",
        "      primitiveT_value->GetPrimitiveT()->value.AsDepthwiseConv2D()->activationType = activation_type;\n",
        "      return pre_node;\n",
        "    } else {\n",
        "      MS_LOG(EXCEPTION) << \"conv activation pass match only conv2d or depthwise_conv2d \";\n",
        "    }\n",
        "  }\n",
        "  return node;\n",
        "}\n",
        "}  // namespace mindspore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f1fcf63-0282-4b0e-d707-d32fa191f9b2"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4MQZYFVY5em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "5bd22a75-adfb-476f-e303-2ca5f445969a"
      },
      "source": [
        "!git clone https://github.com/tx1103mark/tweet-sentiment.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=147589fa04efb3f9caf9d59853156beb76c0fc51d859da8d80acb10970b86fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htU6cfJsJd-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dacc0407-b5ef-41c1-c370-6cdd40774019"
      },
      "source": [
        "!export XLA_USE_BF16=1\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKFrOdEZhwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0tjHVJLWiu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.Inf\n",
        "        else:\n",
        "            self.val_score = -np.Inf\n",
        "\n",
        "    def __call__(self, epoch_score, model, model_path):\n",
        "\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        self.val_score = epoch_score\n",
        "\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZii14QqRPRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals) / len(vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPemFrvdRRGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ./input/roberta-base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPGTmCBaRTyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = './input/roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "config = RobertaConfig.from_pretrained('roberta-base')\n",
        "tokenizer.save_vocabulary(save_path)\n",
        "model.save_pretrained(save_path)\n",
        "config.save_pretrained(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0aXDt1bSAbW",
        "colab_type": "text"
      },
      "source": [
        "#config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYLQJVsWRVro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class config:\n",
        "    LEARNING_RATE = 4e-5\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    VALID_BATCH_SIZE = 32\n",
        "    EPOCHS = 3\n",
        "    TRAINING_FILE = \"./tweet-sentiment/train_8folds.csv\"\n",
        "    ROBERTA_PATH = \"./input/roberta-base\"\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
        "        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
        "        lowercase=True,\n",
        "        add_prefix_space=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TM1V8MRXZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "    \n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "    \n",
        "    targets_start = target_idx[0]\n",
        "    targets_end = target_idx[-1]\n",
        "\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "533cqoc8RYLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item], \n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxH2zzU_R7Rr",
        "colab_type": "text"
      },
      "source": [
        "#TweetModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPEb3HPjRbWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n",
        "        self.drop_out = nn.Dropout(0.1)\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "        out = self.drop_out(out)\n",
        "        logits = self.l0(out)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMZTa4VvRcdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvGvBvYSR42T",
        "colab_type": "text"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoSeSyC1Rf7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n",
        "    model.train()\n",
        "    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n",
        "    for bi, d in enumerate(tk0):\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        sentiment = d[\"sentiment\"]\n",
        "        orig_selected = d[\"orig_selected\"]\n",
        "        orig_tweet = d[\"orig_tweet\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        offsets = d[\"offsets\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets_start = targets_start.to(device, dtype=torch.long)\n",
        "        targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs_start, outputs_end = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer, barrier=True)\n",
        "        scheduler.step()\n",
        "        tk0.set_postfix(loss=loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l801oRiR16g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate_jaccard_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfPApEVQRh7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    jaccards = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Validating\")\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            sentiment = d[\"sentiment\"]\n",
        "            orig_selected = d[\"orig_selected\"]\n",
        "            orig_tweet = d[\"orig_tweet\"]\n",
        "            targets_start = d[\"targets_start\"]\n",
        "            targets_end = d[\"targets_end\"]\n",
        "            offsets = d[\"offsets\"].cpu().numpy()\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets_start = targets_start.to(device, dtype=torch.long)\n",
        "            targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                tweet_sentiment = sentiment[px]\n",
        "                jaccard_score, _ = calculate_jaccard_score(\n",
        "                    original_tweet=tweet,\n",
        "                    target_string=selected_tweet,\n",
        "                    sentiment_val=tweet_sentiment,\n",
        "                    idx_start=np.argmax(outputs_start[px, :]),\n",
        "                    idx_end=np.argmax(outputs_end[px, :]),\n",
        "                    offsets=offsets[px]\n",
        "                )\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
        "            losses.update(loss.item(), ids.size(0))\n",
        "            tk0.set_postfix(loss=loss.item())\n",
        "\n",
        "    return jaccards.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rab2AgntRk3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfx = pd.read_csv(config.TRAINING_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__u6TEtURyXQ",
        "colab_type": "text"
      },
      "source": [
        "#run fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McDosqmsRley",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(fold):\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n",
        "    model_config.output_hidden_states = True\n",
        "    MX = TweetModel(conf=model_config)\n",
        "    \n",
        "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    device = xm.xla_device(fold + 1)\n",
        "    model = MX.to(device)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\n",
        "        \"bias\",\n",
        "        \"LayerNorm.bias\",\n",
        "        \"LayerNorm.weight\"\n",
        "    ]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if not any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "         'weight_decay': 0.001\n",
        "        },\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "            'weight_decay': 0.0\n",
        "        },\n",
        "    ]\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        "    )\n",
        "    optimizer = AdamW(\n",
        "        optimizer_parameters, \n",
        "        lr=config.LEARNING_RATE\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_jac = 0\n",
        "    es = EarlyStopping(patience=2, mode=\"max\")\n",
        "    num_batches = int(len(df_train) / config.TRAIN_BATCH_SIZE)\n",
        "    \n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_fn(\n",
        "            train_data_loader, \n",
        "            model, \n",
        "            optimizer, \n",
        "            device,\n",
        "            num_batches,\n",
        "            scheduler\n",
        "        )\n",
        "\n",
        "        jac = eval_fn(\n",
        "            valid_data_loader, \n",
        "            model, \n",
        "            device\n",
        "        )\n",
        "        print(f'Epoch={epoch}, Fold={fold}, Jaccard={jac}')\n",
        "        if jac > best_jac:\n",
        "            xm.save(model.state_dict(), f\"model_{fold}.bin\")\n",
        "            best_jac = jac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP2bUXHURpc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from joblib import Parallel,delayed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hB75xirRrXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Parallel(n_jobs=8, backend=\"threading\")(delayed(run)(i) for i in range(8))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}