{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/tweet-sentiment/blob/master/TensorFlow_with_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "outputId": "7f1fcf63-0282-4b0e-d707-d32fa191f9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4MQZYFVY5em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "5bd22a75-adfb-476f-e303-2ca5f445969a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=147589fa04efb3f9caf9d59853156beb76c0fc51d859da8d80acb10970b86fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htU6cfJsJd-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dacc0407-b5ef-41c1-c370-6cdd40774019"
      },
      "source": [
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version',tf.__version__)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKFrOdEZhwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p input/roberta-base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0tjHVJLWiu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = './input/roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "config = RobertaConfig.from_pretrained('roberta-base')\n",
        "tokenizer.save_vocabulary(save_path)\n",
        "model.save_pretrained(save_path)\n",
        "config.save_pretrained(save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tx7pln-ZWzK",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mDgF5dOYRzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "5500edef-dcdd-487d-c285-f9f27e5f16e3"
      },
      "source": [
        "!git clone https://github.com/tx1103mark/tweet-sentiment.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tweet-sentiment'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects:   8% (1/12)\u001b[K\rremote: Counting objects:  16% (2/12)\u001b[K\rremote: Counting objects:  25% (3/12)\u001b[K\rremote: Counting objects:  33% (4/12)\u001b[K\rremote: Counting objects:  41% (5/12)\u001b[K\rremote: Counting objects:  50% (6/12)\u001b[K\rremote: Counting objects:  58% (7/12)\u001b[K\rremote: Counting objects:  66% (8/12)\u001b[K\rremote: Counting objects:  75% (9/12)\u001b[K\rremote: Counting objects:  83% (10/12)\u001b[K\rremote: Counting objects:  91% (11/12)\u001b[K\rremote: Counting objects: 100% (12/12)\u001b[K\rremote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects:  10% (1/10)\u001b[K\rremote: Compressing objects:  20% (2/10)\u001b[K\rremote: Compressing objects:  30% (3/10)\u001b[K\rremote: Compressing objects:  40% (4/10)\u001b[K\rremote: Compressing objects:  50% (5/10)\u001b[K\rremote: Compressing objects:  60% (6/10)\u001b[K\rremote: Compressing objects:  70% (7/10)\u001b[K\rremote: Compressing objects:  80% (8/10)\u001b[K\rremote: Compressing objects:  90% (9/10)\u001b[K\rremote: Compressing objects: 100% (10/10)\u001b[K\rremote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "Unpacking objects:   8% (1/12)   \rUnpacking objects:  16% (2/12)   \rUnpacking objects:  25% (3/12)   \rUnpacking objects:  33% (4/12)   \rUnpacking objects:  41% (5/12)   \rUnpacking objects:  50% (6/12)   \rUnpacking objects:  58% (7/12)   \rUnpacking objects:  66% (8/12)   \rUnpacking objects:  75% (9/12)   \rremote: Total 12 (delta 0), reused 9 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  83% (10/12)   \rUnpacking objects:  91% (11/12)   \rUnpacking objects: 100% (12/12)   \rUnpacking objects: 100% (12/12), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sMRZ6PCYxT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_train():\n",
        "    train=pd.read_csv('./tweet-sentiment/original/train.csv')\n",
        "    train['text']=train['text'].astype(str)\n",
        "    train['selected_text']=train['selected_text'].astype(str)\n",
        "    return train\n",
        "\n",
        "def read_test():\n",
        "    test=pd.read_csv('./tweet-sentiment/original/test.csv')\n",
        "    test['text']=test['text'].astype(str)\n",
        "    return test\n",
        "\n",
        "def read_submission():\n",
        "    test=pd.read_csv('./tweet-sentiment/original/sample_submission.csv')\n",
        "    return test\n",
        "    \n",
        "train_df = read_train()\n",
        "test_df = read_test()\n",
        "submission_df = read_submission()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXZGxUjbYbeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8UJwUq7JZ8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 96\n",
        "PATH = './input/roberta-base/'\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=PATH+'vocab.json', \n",
        "    merges_file=PATH+'merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyYI-fhxJPy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train_df.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train_df.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WNwJ9LQZBZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch):\n",
        "    return 3e-5 * 0.2**epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv2tTOOCDZKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'tf_model.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXU1F5ptDhbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_splits = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQoe97iFKTj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be1a32ac-6928-4d8a-c59c-726ce3f4c9fc"
      },
      "source": [
        "jac = []; VER='v4'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=5, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 0.0332 - activation_loss: 0.0169 - activation_1_loss: 0.0163\n",
            "Epoch 00001: val_loss improved from inf to 0.02911, saving model to v4-roberta-0.h5\n",
            "2748/2748 [==============================] - 740s 269ms/step - loss: 0.0332 - activation_loss: 0.0169 - activation_1_loss: 0.0163 - val_loss: 0.0291 - val_activation_loss: 0.0147 - val_activation_1_loss: 0.0144 - lr: 3.0000e-05\n",
            "Epoch 2/5\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 0.0259 - activation_loss: 0.0132 - activation_1_loss: 0.0127\n",
            "Epoch 00002: val_loss improved from 0.02911 to 0.02724, saving model to v4-roberta-0.h5\n",
            "2748/2748 [==============================] - 738s 269ms/step - loss: 0.0259 - activation_loss: 0.0132 - activation_1_loss: 0.0127 - val_loss: 0.0272 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0133 - lr: 6.0000e-06\n",
            "Epoch 3/5\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 0.0240 - activation_loss: 0.0123 - activation_1_loss: 0.0117\n",
            "Epoch 00003: val_loss did not improve from 0.02724\n",
            "2748/2748 [==============================] - 737s 268ms/step - loss: 0.0240 - activation_loss: 0.0123 - activation_1_loss: 0.0117 - val_loss: 0.0276 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0135 - lr: 1.2000e-06\n",
            "Epoch 4/5\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 0.0235 - activation_loss: 0.0121 - activation_1_loss: 0.0115\n",
            "Epoch 00004: val_loss did not improve from 0.02724\n",
            "2748/2748 [==============================] - 739s 269ms/step - loss: 0.0235 - activation_loss: 0.0121 - activation_1_loss: 0.0115 - val_loss: 0.0276 - val_activation_loss: 0.0141 - val_activation_1_loss: 0.0135 - lr: 2.4000e-07\n",
            "Epoch 5/5\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 0.0234 - activation_loss: 0.0120 - activation_1_loss: 0.0114\n",
            "Epoch 00005: val_loss did not improve from 0.02724\n",
            "2748/2748 [==============================] - 739s 269ms/step - loss: 0.0234 - activation_loss: 0.0120 - activation_1_loss: 0.0114 - val_loss: 0.0276 - val_activation_loss: 0.0141 - val_activation_1_loss: 0.0135 - lr: 4.8000e-08\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 45s 260ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7030651075023784\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0318 - activation_loss: 0.0161 - activation_1_loss: 0.0157\n",
            "Epoch 00001: val_loss improved from inf to 0.02733, saving model to v4-roberta-1.h5\n",
            "2749/2749 [==============================] - 784s 285ms/step - loss: 0.0318 - activation_loss: 0.0161 - activation_1_loss: 0.0157 - val_loss: 0.0273 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0135 - lr: 3.0000e-05\n",
            "Epoch 2/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0253 - activation_loss: 0.0129 - activation_1_loss: 0.0124\n",
            "Epoch 00002: val_loss improved from 0.02733 to 0.02681, saving model to v4-roberta-1.h5\n",
            "2749/2749 [==============================] - 780s 284ms/step - loss: 0.0253 - activation_loss: 0.0129 - activation_1_loss: 0.0124 - val_loss: 0.0268 - val_activation_loss: 0.0137 - val_activation_1_loss: 0.0131 - lr: 6.0000e-06\n",
            "Epoch 3/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0233 - activation_loss: 0.0120 - activation_1_loss: 0.0113\n",
            "Epoch 00003: val_loss did not improve from 0.02681\n",
            "2749/2749 [==============================] - 779s 283ms/step - loss: 0.0233 - activation_loss: 0.0120 - activation_1_loss: 0.0113 - val_loss: 0.0270 - val_activation_loss: 0.0137 - val_activation_1_loss: 0.0132 - lr: 1.2000e-06\n",
            "Epoch 4/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0227 - activation_loss: 0.0117 - activation_1_loss: 0.0110\n",
            "Epoch 00004: val_loss did not improve from 0.02681\n",
            "2749/2749 [==============================] - 780s 284ms/step - loss: 0.0227 - activation_loss: 0.0117 - activation_1_loss: 0.0110 - val_loss: 0.0270 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0132 - lr: 2.4000e-07\n",
            "Epoch 5/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0227 - activation_loss: 0.0117 - activation_1_loss: 0.0110\n",
            "Epoch 00005: val_loss did not improve from 0.02681\n",
            "2749/2749 [==============================] - 780s 284ms/step - loss: 0.0227 - activation_loss: 0.0117 - activation_1_loss: 0.0110 - val_loss: 0.0270 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0132 - lr: 4.8000e-08\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 45s 260ms/step\n",
            ">>>> FOLD 2 Jaccard = 0.7112633692867966\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0323 - activation_loss: 0.0162 - activation_1_loss: 0.0160\n",
            "Epoch 00001: val_loss improved from inf to 0.02793, saving model to v4-roberta-2.h5\n",
            "2749/2749 [==============================] - 783s 285ms/step - loss: 0.0323 - activation_loss: 0.0162 - activation_1_loss: 0.0160 - val_loss: 0.0279 - val_activation_loss: 0.0142 - val_activation_1_loss: 0.0138 - lr: 3.0000e-05\n",
            "Epoch 2/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0254 - activation_loss: 0.0130 - activation_1_loss: 0.0124\n",
            "Epoch 00002: val_loss improved from 0.02793 to 0.02740, saving model to v4-roberta-2.h5\n",
            "2749/2749 [==============================] - 782s 284ms/step - loss: 0.0254 - activation_loss: 0.0130 - activation_1_loss: 0.0124 - val_loss: 0.0274 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0134 - lr: 6.0000e-06\n",
            "Epoch 3/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0235 - activation_loss: 0.0120 - activation_1_loss: 0.0114\n",
            "Epoch 00003: val_loss improved from 0.02740 to 0.02710, saving model to v4-roberta-2.h5\n",
            "2749/2749 [==============================] - 783s 285ms/step - loss: 0.0235 - activation_loss: 0.0120 - activation_1_loss: 0.0114 - val_loss: 0.0271 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0133 - lr: 1.2000e-06\n",
            "Epoch 4/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0229 - activation_loss: 0.0118 - activation_1_loss: 0.0111\n",
            "Epoch 00004: val_loss did not improve from 0.02710\n",
            "2749/2749 [==============================] - 782s 284ms/step - loss: 0.0229 - activation_loss: 0.0118 - activation_1_loss: 0.0111 - val_loss: 0.0274 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0135 - lr: 2.4000e-07\n",
            "Epoch 5/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0228 - activation_loss: 0.0117 - activation_1_loss: 0.0111\n",
            "Epoch 00005: val_loss did not improve from 0.02710\n",
            "2749/2749 [==============================] - 784s 285ms/step - loss: 0.0228 - activation_loss: 0.0117 - activation_1_loss: 0.0111 - val_loss: 0.0274 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0135 - lr: 4.8000e-08\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 45s 260ms/step\n",
            ">>>> FOLD 3 Jaccard = 0.7038921837959938\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0323 - activation_loss: 0.0164 - activation_1_loss: 0.0159\n",
            "Epoch 00001: val_loss improved from inf to 0.02901, saving model to v4-roberta-3.h5\n",
            "2749/2749 [==============================] - 782s 284ms/step - loss: 0.0323 - activation_loss: 0.0164 - activation_1_loss: 0.0159 - val_loss: 0.0290 - val_activation_loss: 0.0142 - val_activation_1_loss: 0.0148 - lr: 3.0000e-05\n",
            "Epoch 2/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0252 - activation_loss: 0.0129 - activation_1_loss: 0.0124\n",
            "Epoch 00002: val_loss improved from 0.02901 to 0.02660, saving model to v4-roberta-3.h5\n",
            "2749/2749 [==============================] - 780s 284ms/step - loss: 0.0252 - activation_loss: 0.0129 - activation_1_loss: 0.0124 - val_loss: 0.0266 - val_activation_loss: 0.0133 - val_activation_1_loss: 0.0133 - lr: 6.0000e-06\n",
            "Epoch 3/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0231 - activation_loss: 0.0119 - activation_1_loss: 0.0112\n",
            "Epoch 00003: val_loss did not improve from 0.02660\n",
            "2749/2749 [==============================] - 779s 283ms/step - loss: 0.0231 - activation_loss: 0.0119 - activation_1_loss: 0.0112 - val_loss: 0.0267 - val_activation_loss: 0.0133 - val_activation_1_loss: 0.0134 - lr: 1.2000e-06\n",
            "Epoch 4/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0225 - activation_loss: 0.0116 - activation_1_loss: 0.0109\n",
            "Epoch 00004: val_loss did not improve from 0.02660\n",
            "2749/2749 [==============================] - 779s 284ms/step - loss: 0.0225 - activation_loss: 0.0116 - activation_1_loss: 0.0109 - val_loss: 0.0267 - val_activation_loss: 0.0134 - val_activation_1_loss: 0.0133 - lr: 2.4000e-07\n",
            "Epoch 5/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0224 - activation_loss: 0.0116 - activation_1_loss: 0.0109\n",
            "Epoch 00005: val_loss did not improve from 0.02660\n",
            "2749/2749 [==============================] - 779s 283ms/step - loss: 0.0224 - activation_loss: 0.0116 - activation_1_loss: 0.0109 - val_loss: 0.0268 - val_activation_loss: 0.0134 - val_activation_1_loss: 0.0134 - lr: 4.8000e-08\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 45s 260ms/step\n",
            ">>>> FOLD 4 Jaccard = 0.7095165438165816\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0327 - activation_loss: 0.0164 - activation_1_loss: 0.0163\n",
            "Epoch 00001: val_loss improved from inf to 0.02738, saving model to v4-roberta-4.h5\n",
            "2749/2749 [==============================] - 783s 285ms/step - loss: 0.0327 - activation_loss: 0.0164 - activation_1_loss: 0.0163 - val_loss: 0.0274 - val_activation_loss: 0.0142 - val_activation_1_loss: 0.0132 - lr: 3.0000e-05\n",
            "Epoch 2/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0256 - activation_loss: 0.0131 - activation_1_loss: 0.0125\n",
            "Epoch 00002: val_loss improved from 0.02738 to 0.02668, saving model to v4-roberta-4.h5\n",
            "2749/2749 [==============================] - 782s 284ms/step - loss: 0.0256 - activation_loss: 0.0131 - activation_1_loss: 0.0125 - val_loss: 0.0267 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0129 - lr: 6.0000e-06\n",
            "Epoch 3/5\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 0.0237 - activation_loss: 0.0121 - activation_1_loss: 0.0116"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzrEChoRKr5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TweetModel, self).__init__()\n",
        "        \n",
        "        config = RobertaConfig.from_pretrained(\n",
        "            './tweet-sentiment/config.json', output_hidden_states=True)    \n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            './tweet-sentiment/pytorch_model.bin', config=config)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(config.hidden_size, 2)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
        "         \n",
        "        x = torch.stack([hs[-1], hs[-2], hs[-3]])\n",
        "        x = torch.mean(x, 0)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        start_logits, end_logits = x.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "                \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADOFzWEkG4r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TweetModel()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8R_ttEhNc9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyKejAHtGWZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_selected_text(text, start_idx, end_idx, offsets):\n",
        "    selected_text = \"\"\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            selected_text += \" \"\n",
        "    return selected_text\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
        "    start_pred = np.argmax(start_logits)\n",
        "    end_pred = np.argmax(end_logits)\n",
        "    if start_pred > end_pred:\n",
        "        pred = text\n",
        "    else:\n",
        "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
        "        \n",
        "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
        "    \n",
        "    return jaccard(true, pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ecm7ufCGr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
        "    model.cuda()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            epoch_jaccard = 0.0\n",
        "            \n",
        "            for data in (dataloaders_dict[phase]):\n",
        "                ids = data['ids'].cuda()\n",
        "                masks = data['masks'].cuda()\n",
        "                tweet = data['tweet']\n",
        "                offsets = data['offsets'].numpy()\n",
        "                start_idx = data['start_idx'].cuda()\n",
        "                end_idx = data['end_idx'].cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    start_logits, end_logits = model(ids, masks)\n",
        "\n",
        "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    epoch_loss += loss.item() * len(ids)\n",
        "                    \n",
        "                    start_idx = start_idx.cpu().detach().numpy()\n",
        "                    end_idx = end_idx.cpu().detach().numpy()\n",
        "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
        "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
        "                    \n",
        "                    for i in range(len(ids)):                        \n",
        "                        jaccard_score = compute_jaccard_score(\n",
        "                            tweet[i],\n",
        "                            start_idx[i],\n",
        "                            end_idx[i],\n",
        "                            start_logits[i], \n",
        "                            end_logits[i], \n",
        "                            offsets[i])\n",
        "                        epoch_jaccard += jaccard_score\n",
        "                    \n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
        "            \n",
        "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
        "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
        "    \n",
        "    torch.save(model.state_dict(), filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2x6kPcWOQd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7vZtDZvPNM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(fold):\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n",
        "    model_config.output_hidden_states = True\n",
        "    MX = TweetModel(conf=model_config)\n",
        "    \n",
        "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    model = MX.to(device)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\n",
        "        \"bias\",\n",
        "        \"LayerNorm.bias\",\n",
        "        \"LayerNorm.weight\"\n",
        "    ]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if not any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "         'weight_decay': 0.001\n",
        "        },\n",
        "        {\n",
        "            'params': [\n",
        "                p for n, p in param_optimizer if any(\n",
        "                    nd in n for nd in no_decay\n",
        "                )\n",
        "            ], \n",
        "            'weight_decay': 0.0\n",
        "        },\n",
        "    ]\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        "    )\n",
        "    optimizer = AdamW(\n",
        "        optimizer_parameters, \n",
        "        lr=config.LEARNING_RATE\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_jac = 0\n",
        "    es = utils.EarlyStopping(patience=2, mode=\"max\")\n",
        "    num_batches = int(len(df_train) / config.TRAIN_BATCH_SIZE)\n",
        "    \n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_fn(\n",
        "            train_data_loader, \n",
        "            model, \n",
        "            optimizer, \n",
        "            device,\n",
        "            num_batches,\n",
        "            scheduler\n",
        "        )\n",
        "\n",
        "        jac = eval_fn(\n",
        "            valid_data_loader, \n",
        "            model, \n",
        "            device\n",
        "        )\n",
        "        print(f'Epoch={epoch}, Fold={fold}, Jaccard={jac}')\n",
        "        if jac > best_jac:\n",
        "            xm.save(model.state_dict(), f\"model_{fold}.bin\")\n",
        "            best_jac = jac"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}